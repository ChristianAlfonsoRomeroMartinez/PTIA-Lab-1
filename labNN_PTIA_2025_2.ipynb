{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fno-fTWaPMjG"
   },
   "source": [
    "# ESCUELA COLOMBIANA DE INGENIERÍA\n",
    "# PRINCIPIOS Y TECNOLOGÍAS IA 2025-2\n",
    "## REDES NEURONALES\n",
    "## LABORATORIO 1/4\n",
    "\n",
    "**OBJETIVOS**\n",
    "\n",
    "Desarrollar competencias básicas para:\n",
    "1. Modelar y resolver problemas usando redes neuronales\n",
    "2. Implementar los algoritmos hacia adelante (FEED-FORWARD) y hacia atrás con  aprendizaje (BACKPROPAGATION)\n",
    "3. Apropiar un framework para redes neuronales (*keras*)\n",
    "\n",
    "**ENTREGABLE**\n",
    "\n",
    "\n",
    "*Reglas para el envío de los entregables*:\n",
    "\n",
    "* **Forma de envío:**\n",
    "Esta tarea se debe enviar únicamente a través de la plataforma Moodle en la actividad definida. Se tendrán dos entregas: inicial y final.\n",
    "\n",
    "* **Formato de los archivos:**\n",
    "Incluyan en un archivo *.zip* los archivos correspondientes al laboratorio.\n",
    "\n",
    "* **Nomenclatura para nombrar los archivos:**\n",
    "El archivo deberá ser renombrado, “RN-lab-” seguido por los usuarios institucionales de los autores ordenados alfabéticamente (por ejemplo, se debe adicionar pedroperez al nombre del archivo, si el correo electrónico de Pedro Pérez es pedro.perez@mail.escuelaing.edu.co)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzOqkgmtTtyJ"
   },
   "source": [
    "# PARTE I. IMPLEMENTACIÓN DE RED NEURONAL\n",
    "\n",
    "Para este apartado se va a implementar una red neuronal con algoritmo de aprendizaje, en este caso propagación hacia atras del error.\n",
    "\n",
    "*Introducido en la década de 1960 y popularizado casi 30 años después (1989) por Rumelhart, Hinton y Williams en el artículo titulado «Learning representations by back-propagating errors».*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGmFQXzxi6ly"
   },
   "source": [
    "## IMPLEMENTACIÓN DE RED NEURONAL CON PROPAGACIÓN HACIA ATRÁS\n",
    "\n",
    "Implementar una red neuronal totalmente conectada desde su definición simple; calculando una salida $\\check{Y} (Yp)$ para unas entradas $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd2V7GcrVNYT"
   },
   "source": [
    "**Propiedades y parámetros:**\n",
    "\n",
    "*   Tarea: **Clasificación multiple**\n",
    "*   Tipo de capas: **Densas**\n",
    "*   Métrica para evaluación : **ACCURACY**\n",
    "\n",
    "<div>\n",
    "<img src=\"https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c426738658d748af1b20d_644af5900694f1102fb9b470_classification_guide_apc05.png\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "> **Funciones de activación**\n",
    "\n",
    "*   Función de activación en *Capas ocultas* : **ReLU**\n",
    "\n",
    "<div>\n",
    "<img src=\"https://intuitivetutorial.com/wp-content/uploads/2023/07/ReLU-1.png\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "*   Función de activación en *Capa de salida* : **Sigmoide**\n",
    "\n",
    "<div>\n",
    "<img src=\"https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2018/06/sigm.png\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "> **Funcion de costo**\n",
    "\n",
    "*   Función de costo/perdida «error»: **Entropia Cruzada «Cross-Entropy»**\n",
    "\n",
    "<div>\n",
    "<img src=\"https://framerusercontent.com/images/jiDTkbQC7DPO2z2XmxqoeMsrkA.webp?width=1300&height=508\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oulCkcVq6Qxk"
   },
   "source": [
    "## Paso 1. Derivadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoEOsx7_6jIp"
   },
   "source": [
    "*Incluya en este apartado el proceso de la derivación de las funciones*\n",
    "\n",
    "---\n",
    "**Derivada función Sigmoide:**\n",
    "\n",
    "\n",
    "---\n",
    "**Derivada función ReLU**\n",
    "\n",
    "\n",
    "---\n",
    "**Derivada función de costo: Entropia Cruzada**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bB_c4pSm7-5C"
   },
   "source": [
    "## Paso 2. Implementación del código para ANN (Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnjfO4Tdfjvt"
   },
   "source": [
    "### LIBRERÍA NECESARIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "hWmbqLmNWw-n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAjCMgrWfinl"
   },
   "source": [
    "### FUNCIONES DE BASE: MÉTRICA, COSTO Y ACTIVACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6akod1Zlu0u"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haWIH62kNQtS"
   },
   "source": [
    "###MÉTRICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "I37WpNyVP5jr"
   },
   "outputs": [],
   "source": [
    "class Metric(ABC):\n",
    "  \"\"\" Abstracta: define entradas, salidas y el comportamiento inicial de los métodos clave para cualquier metrica\n",
    "  Representa la metrica de una red neuronal\n",
    "  \"\"\"\n",
    "  def use(self, name: str) -> self:\n",
    "    \"\"\" obtiene metrica (OBJ) a partir del nombre\n",
    "    Args:\n",
    "      name (str): nombre esperado de la metrica\n",
    "    Returns:\n",
    "      self (Metric): objeto metrica\n",
    "    \"\"\"\n",
    "    self.name=  name\n",
    "    return self\n",
    "\n",
    "  def value(self, Y: np.ndarray, Yp:np.ndarray):\n",
    "    \"\"\" computa el desempeño (accuracy) de la red (> 0.6 es 1)\n",
    "    Args:\n",
    "      Y (ndarray): valores de salidas esperadas (etiquetadas)\n",
    "      Yp (ndarray): valores de salidas obtenidas\n",
    "    Return:\n",
    "      A (float): valor del desempeño\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "4exjI4Ng_P0k"
   },
   "outputs": [],
   "source": [
    "# Documentar los métodos implementados\n",
    "class Accuracy(Metric):\n",
    "    def use(self):\n",
    "        super().use(name='accuracy')\n",
    "\n",
    "    def value(self, Y: np.ndarray, Yp: np.ndarray) -> float:\n",
    "        # Yp son las salidas de la red, Y son las etiquetas correctas\n",
    "        # Convertimos las predicciones a clases (0 o 1)\n",
    "        # np.round() redondea a 0 o 1\n",
    "        y_pred_class = np.round(Yp)\n",
    "        print(y_pred_class)\n",
    "        # Comparamos las predicciones con las etiquetas verdaderas\n",
    "        correct_predictions = np.sum(y_pred_class == Y)\n",
    "        print(correct_predictions)\n",
    "        # Calculamos el número total de predicciones\n",
    "        total_predictions = Y.size\n",
    "        print(total_predictions)\n",
    "        # Calculamos la precisión (número de aciertos / total de predicciones)\n",
    "        accuracy_score = correct_predictions / total_predictions\n",
    "        print(accuracy_score)\n",
    "        return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1JDA2m-Wjq_",
    "outputId": "339b28b2-c67b-48b6-c4a5-38b5574aeb85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 1.]\n",
      "4\n",
      "4\n",
      "1.0\n",
      "Desempeño para caso 1: 1.0\n",
      "[1. 1. 1. 1.]\n",
      "2\n",
      "4\n",
      "0.5\n",
      "Desempeño para caso 2: 0.5\n",
      "[0. 0. 1. 0.]\n",
      "1\n",
      "4\n",
      "0.25\n",
      "Desempeño para caso 3: 0.25\n",
      "[0. 1. 0. 0.]\n",
      "3\n",
      "4\n",
      "0.75\n",
      "Desempeño para caso 4: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Caso de prueba 1: Presición perfecta\n",
    "y_true_1 = np.array([0, 1, 0, 1])\n",
    "y_pred_1 = np.array([0.1, 0.9, 0.2, 0.8])\n",
    "accuracy_metric = Accuracy()\n",
    "accuracy_metric.use()\n",
    "acc_1 = accuracy_metric.value(y_true_1, y_pred_1)\n",
    "print(f\"Desempeño para caso 1: {acc_1}\")\n",
    "\n",
    "# Caso de prueba 2: Presición mixta\n",
    "y_true_2 = np.array([0, 1, 0, 1])\n",
    "y_pred_2 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "acc_2 = accuracy_metric.value(y_true_2, y_pred_2)\n",
    "print(f\"Desempeño para caso 2: {acc_2}\")\n",
    "\n",
    "# Caso de prueba 3: Presición baja\n",
    "y_true_3 = np.array([0, 1, 0, 1])\n",
    "y_pred_3 = np.array([0.3, 0.1, 0.8, 0.2])\n",
    "acc_3 = accuracy_metric.value(y_true_3, y_pred_3)\n",
    "print(f\"Desempeño para caso 3: {acc_3}\")\n",
    "\n",
    "# Caso de prueba 4: Presición alta\n",
    "y_true_4 = np.array([0, 1, 0, 1])\n",
    "y_pred_4 = np.array([0.4, 0.6, 0.4, 0.1])\n",
    "acc_4 = accuracy_metric.value(y_true_4, y_pred_4)\n",
    "print(f\"Desempeño para caso 4: {acc_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZrAmWV0Nnyb"
   },
   "source": [
    "###COSTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "MUpdVUmb8dCS"
   },
   "outputs": [],
   "source": [
    "class Cost(ABC):\n",
    "  \"\"\" Abstracta: define entradas, salidas y el comportamiento inicial de los métodos clave para cualquier función de costo\n",
    "  Representa la función de costo o error de una red neuronal\n",
    "  \"\"\"\n",
    "  @abstractmethod\n",
    "  def use(self, name: str)-> self:\n",
    "    \"\"\" obtiene función de costo (OBJ) a partir del nombre\n",
    "    Args:\n",
    "      name (str): nombre esperado de la función\n",
    "    Returns:\n",
    "      self (Cost): objeto función de costo\n",
    "    \"\"\"\n",
    "    self.name = name\n",
    "    return self\n",
    "\n",
    "  @abstractmethod\n",
    "  def value(self, Y: np.ndarray, Yp: np.ndarray) -> float:\n",
    "    \"\"\" computa la función de costo\n",
    "    Args:\n",
    "      Y (ndarray): valores de salida obtenidos\n",
    "      Yp (ndarray): valores de salida esperados\n",
    "    Returns:\n",
    "      S (float): valor de computo de la función de costo\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def derivative(self, Y: np.ndarray, Yp: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" computa la derivada de la función de costo (gradiente) <elemento por elemento>\n",
    "    Args:\n",
    "      Y (ndarray): valores de salida obtenidos\n",
    "      Yp (ndarray): valores de salida esperados\n",
    "    Returns:\n",
    "      ∇E(X) (ndarray): valores para la derivada de función de costo\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "MNbNxSEEP0YM"
   },
   "outputs": [],
   "source": [
    "# Documentar los métodos implementados\n",
    "class CrossEntropy(Cost):\n",
    "  \"\"\" Función de costo Entropia Cruzada. Implementa Cost\n",
    "  \"\"\"\n",
    "\n",
    "  def CrossEntropy(self):\n",
    "    pass\n",
    "\n",
    "  def use(self):\n",
    "      super().use(name='cross_entropy')\n",
    "\n",
    "  def value(self, Y: np.ndarray, Yp: np.ndarray) -> np.ndarray:\n",
    "    # Prevenimos divisiones por cero o logaritmos de cero\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(Yp, epsilon, 1 - epsilon)\n",
    "    # Calculamos el costo con la fórmula de entropía cruzada binaria\n",
    "    cost = -np.mean(Y * np.log(y_pred) + (1 - Y) * np.log(1 - y_pred))\n",
    "    return cost\n",
    "\n",
    "  def derivative(self, Y: np.ndarray, Yp: np.ndarray) -> np.ndarray:\n",
    "    # Derivada de entropía cruzada binaria: (Yp - Y) / (Yp * (1 - Yp))\n",
    "    # Simplificada para uso con sigmoid: (Yp - Y)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(Yp, epsilon, 1 - epsilon)\n",
    "    return (y_pred - Y) / (y_pred * (1 - y_pred))\n",
    "\n",
    "class BinaryCrossEntropy(CrossEntropy):\n",
    "    def use(self, name: str = 'binary_crossentropy') -> self:\n",
    "        self.name = name\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bVg_e-x8KJf_",
    "outputId": "d1d13e23-9d9b-45d1-bdfa-de68ca1ceb14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost value: 0.164252033486018\n",
      "Cost value 2: 0.23101772979827936\n"
     ]
    }
   ],
   "source": [
    "# Adicione los casos de prueba de los métodos implementados\n",
    "cost_function = CrossEntropy()\n",
    "\n",
    "# Example true and predicted values\n",
    "y_true = np.array([0, 1, 1, 0])\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.2])\n",
    "\n",
    "cost_value = cost_function.value(y_true, y_pred)\n",
    "print(f\"Cost value: {cost_value}\")\n",
    "\n",
    "# Example true and predicted values\n",
    "y_true_2 = np.array([1, 0, 0, 1])\n",
    "y_pred_2 = np.array([0.9, 0.1, 0.3, 0.7])\n",
    "\n",
    "cost_value_2 = cost_function.value(y_true_2, y_pred_2)\n",
    "print(f\"Cost value 2: {cost_value_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLpHHuubNrFx"
   },
   "source": [
    "###ACTIVACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "94Cy0u6bWupv"
   },
   "outputs": [],
   "source": [
    "class Activation(ABC):\n",
    "  \"\"\" Abstracta: define entradas, salidas y el comportamiento inicial de los métodos clave de cualquier función de activación\n",
    "  Representa la función de activación de cualquier neurona en la red neuronal\n",
    "  \"\"\"\n",
    "  @abstractmethod\n",
    "  def use(self, name: str) -> self:\n",
    "    \"\"\" obtiene función de activación (OBJ) a partir del nombre\n",
    "    Args:\n",
    "      name (str): nombre esperado de la función\n",
    "    Returns:\n",
    "      self (Activation): objeto función de activación\n",
    "    \"\"\"\n",
    "    self.name = name\n",
    "    return self\n",
    "\n",
    "  @abstractmethod\n",
    "  def value(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" computa la función de activación <elemento por elemento>\n",
    "    Args:\n",
    "      X (ndarray): valores de entrada\n",
    "    Returns:\n",
    "      S (ndarray): valores de computo de la función de activación\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def derivative(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" computa la derivada de la función de activación (gradiente) <elemento por elemento>\n",
    "    Args:\n",
    "      X (ndarray): valores de entrada\n",
    "    Returns:\n",
    "      ∇E(X) (ndarray): valores para la derivada de función de activación\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "FOhmhNVeVnod"
   },
   "outputs": [],
   "source": [
    "# Documentar los métodos implementados\n",
    "class Sigmoid(Activation):\n",
    "  def use(self, name: str = 'sigmoid') -> self:\n",
    "    self.name = name\n",
    "    return self\n",
    "\n",
    "  def value(self, X: np.ndarray) -> np.ndarray:\n",
    "    # Clip para evitar overflow en el exponencial\n",
    "    X_clipped = np.clip(X, -500, 500)\n",
    "    return 1 / (1 + np.exp(-X_clipped))\n",
    "\n",
    "  def derivative(self, X: np.ndarray) -> np.ndarray:\n",
    "    # La derivada de sigmoid es sigmoid(x) * (1 - sigmoid(x))\n",
    "    sigmoid_output = self.value(X)\n",
    "    return sigmoid_output * (1 - sigmoid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "9Qw21AgfcrjU"
   },
   "outputs": [],
   "source": [
    "class Relu(Activation):\n",
    "  def use(self, name: str = 'relu') -> self:\n",
    "    self.name = name\n",
    "    return self\n",
    "  def value(self, X: np.ndarray) -> np.ndarray:\n",
    "    # ReLU(x) = max(0, x) - devuelve 0 para valores negativos y x para valores positivos\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "  def derivative(self, X: np.ndarray) -> np.ndarray:\n",
    "    # La derivada de ReLU es 1 para x > 0 y 0 para x <= 0\n",
    "    return (X > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1eEGZnRO9k7",
    "outputId": "ac9c1a6e-d8ca-49ea-be78-ab4c1be9d552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CASOS DE PRUEBA PARA FUNCIONES DE ACTIVACIÓN\n",
      "======================================================================\n",
      "\n",
      " PRUEBAS PARA FUNCIÓN SIGMOID\n",
      "--------------------------------------------------\n",
      "\n",
      " Valores básicos:\n",
      "   Entrada: [[-2 -1  0  1  2]]\n",
      "   Sigmoid: [[0.11920292 0.26894142 0.5        0.73105858 0.88079708]]\n",
      "   Derivada: [[0.10499359 0.19661193 0.25       0.19661193 0.10499359]]\n",
      "    Rango [0,1]: True\n",
      "    Derivada ≥ 0: True\n",
      "\n",
      " Valores extremos:\n",
      "   Entrada: [[-1000  -100   100  1000]]\n",
      "   Sigmoid: [[7.12457641e-218 3.72007598e-044 1.00000000e+000 1.00000000e+000]]\n",
      "   Derivada: [[7.12457641e-218 3.72007598e-044 0.00000000e+000 0.00000000e+000]]\n",
      "    Rango [0,1]: True\n",
      "    Derivada ≥ 0: True\n",
      "\n",
      " Matriz 2D:\n",
      "   Entrada: [[-1  2]\n",
      " [ 3 -4]\n",
      " [ 0  5]]\n",
      "   Sigmoid: [[0.26894142 0.88079708]\n",
      " [0.95257413 0.01798621]\n",
      " [0.5        0.99330715]]\n",
      "   Derivada: [[0.19661193 0.10499359]\n",
      " [0.04517666 0.01766271]\n",
      " [0.25       0.00664806]]\n",
      "    Rango [0,1]: True\n",
      "    Derivada ≥ 0: True\n",
      "\n",
      " Valores decimales:\n",
      "   Entrada: [[ 0.5 -0.5  1.5 -1.5]]\n",
      "   Sigmoid: [[0.62245933 0.37754067 0.81757448 0.18242552]]\n",
      "   Derivada: [[0.23500371 0.23500371 0.14914645 0.14914645]]\n",
      "    Rango [0,1]: True\n",
      "    Derivada ≥ 0: True\n",
      "\n",
      "\n",
      " PRUEBAS PARA FUNCIÓN RELU\n",
      "--------------------------------------------------\n",
      "\n",
      " Valores básicos:\n",
      "   Entrada: [[-2 -1  0  1  2]]\n",
      "   ReLU: [[0 0 0 1 2]]\n",
      "   Derivada: [[0. 0. 0. 1. 1.]]\n",
      "   Salida ≥ 0: True\n",
      "   Negativos = 0: True\n",
      "   Derivada binaria: True\n",
      "\n",
      " Valores extremos:\n",
      "   Entrada: [[-1000  -100   100  1000]]\n",
      "   ReLU: [[   0    0  100 1000]]\n",
      "   Derivada: [[0. 0. 1. 1.]]\n",
      "   Salida ≥ 0: True\n",
      "   Negativos = 0: True\n",
      "   Derivada binaria: True\n",
      "\n",
      " Matriz 2D:\n",
      "   Entrada: [[-1  2]\n",
      " [ 3 -4]\n",
      " [ 0  5]]\n",
      "   ReLU: [[0 2]\n",
      " [3 0]\n",
      " [0 5]]\n",
      "   Derivada: [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "   Salida ≥ 0: True\n",
      "   Negativos = 0: True\n",
      "   Derivada binaria: True\n",
      "\n",
      " Valores decimales:\n",
      "   Entrada: [[ 0.5 -0.5  1.5 -1.5]]\n",
      "   ReLU: [[0.5 0.  1.5 0. ]]\n",
      "   Derivada: [[1. 0. 1. 0.]]\n",
      "   Salida ≥ 0: True\n",
      "   Negativos = 0: True\n",
      "   Derivada binaria: True\n",
      "\n",
      "PRUEBAS DE PROPIEDADES DE GRADIENTES\n",
      "--------------------------------------------------\n",
      "Entrada: [[-3 -1  0  1  3]]\n",
      "\n",
      "Sigmoid:\n",
      "   Salida: [[0.04742587 0.26894142 0.5        0.73105858 0.95257413]]\n",
      "   Gradiente: [[0.04517666 0.19661193 0.25       0.19661193 0.04517666]]\n",
      "   Máximo gradiente: 0.2500 (debería ser ≤ 0.25)\n",
      "\n",
      "ReLU:\n",
      "   Salida: [[0 0 0 1 3]]\n",
      "   Gradiente: [[0. 0. 0. 1. 1.]]\n",
      "   Problema gradiente que se desvanece: NO (gradiente = 1 para x > 0)\n",
      "\n",
      "\n",
      " PRUEBAS DE PROPIEDADES MATEMÁTICAS\n",
      "--------------------------------------------------\n",
      "Propiedad simétrica de Sigmoid:\n",
      "   sigmoid(x): [[0.73105858 0.88079708 0.95257413]]\n",
      "   sigmoid(-x): [[0.26894142 0.11920292 0.04742587]]\n",
      "   sigmoid(x) + sigmoid(-x) ≈ 1: True\n",
      "\n",
      "Punto de inflexión (x=0):\n",
      "   sigmoid(0) = 0.5: True\n",
      "   derivada máxima: 0.2500\n",
      "\n",
      "\n",
      "  PRUEBAS DE CASOS EXTREMOS\n",
      "--------------------------------------------------\n",
      "\n",
      " Valores muy grandes: [[1000 2000]]\n",
      "   Sigmoid: [[1. 1.]]\n",
      "   ReLU: [[1000 2000]]\n",
      "   ✓ Sin errores numéricos\n",
      "\n",
      " Valores muy pequeños: [[-1000 -2000]]\n",
      "   Sigmoid: [[7.12457641e-218 7.12457641e-218]]\n",
      "   ReLU: [[0 0]]\n",
      "   ✓ Sin errores numéricos\n",
      "\n",
      " Ceros: [[0 0 0]]\n",
      "   Sigmoid: [[0.5 0.5 0.5]]\n",
      "   ReLU: [[0 0 0]]\n",
      "   ✓ Sin errores numéricos\n",
      "\n",
      " Valor único: [[42]]\n",
      "   Sigmoid: [[1.]]\n",
      "   ReLU: [[42]]\n",
      "   ✓ Sin errores numéricos\n",
      "\n",
      "\n",
      " PRUEBAS DE COMPATIBILIDAD CON RED NEURONAL\n",
      "--------------------------------------------------\n",
      "Entrada de capa (shape (4, 3)):\n",
      "[[ 0.10342045  0.14336469  1.39178295]\n",
      " [-2.00459855 -0.67052089  0.86526129]\n",
      " [ 1.1079025   0.42720008 -0.18412701]\n",
      " [ 1.86155971  0.17781175  0.03078599]]\n",
      "\n",
      "Salida Sigmoid (shape (4, 3)):\n",
      "[[0.52583209 0.53577991 0.80087673]\n",
      " [0.11872095 0.33838021 0.70375872]\n",
      " [0.75173787 0.60520488 0.45409786]\n",
      " [0.86547864 0.54433618 0.50769589]]\n",
      "\n",
      "Salida ReLU (shape (4, 3)):\n",
      "[[0.10342045 0.14336469 1.39178295]\n",
      " [0.         0.         0.86526129]\n",
      " [1.1079025  0.42720008 0.        ]\n",
      " [1.86155971 0.17781175 0.03078599]]\n",
      "\n",
      "✓ Forma preservada: True\n",
      "\n",
      "======================================================================\n",
      " TODAS LAS PRUEBAS COMPLETADAS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Adicione los casos de prueba de los métodos implementados\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "# CASOS DE PRUEBA PARA FUNCIONES DE ACTIVACIÓN\n",
    "\n",
    "\n",
    "# ---- CLASE ABSTRACTA PARA FUNCIÓN DE ACTIVACIÓN ----\n",
    "class Activation(ABC):\n",
    "    @abstractmethod\n",
    "    def _init_(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    @abstractmethod\n",
    "    def _call_(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "# ---- IMPLEMENTACIÓN DE CLASES DE ACTIVACIÓN ----\n",
    "class Sigmoid(Activation):\n",
    "    def _init_(self) -> None:\n",
    "        super()._init_(name='sigmoid')\n",
    "\n",
    "    def _call_(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Clip para evitar overflow\n",
    "        x_clipped = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x_clipped))\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        sig = self._call_(x)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def _init_(self) -> None:\n",
    "        super()._init_(name='relu')\n",
    "\n",
    "    def _call_(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "# CASOS DE PRUEBA DETALLADOS\n",
    "\n",
    "def test_activation_functions():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CASOS DE PRUEBA PARA FUNCIONES DE ACTIVACIÓN\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Datos de prueba variados\n",
    "    test_cases = {\n",
    "        \"Valores básicos\": np.array([[-2, -1, 0, 1, 2]]),\n",
    "        \"Valores extremos\": np.array([[-1000, -100, 100, 1000]]),\n",
    "        \"Matriz 2D\": np.array([[-1, 2], [3, -4], [0, 5]]),\n",
    "        \"Valores decimales\": np.array([[0.5, -0.5, 1.5, -1.5]])\n",
    "    }\n",
    "\n",
    "    # ---- PRUEBAS PARA SIGMOID ----\n",
    "    print(\"\\n PRUEBAS PARA FUNCIÓN SIGMOID\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    sigmoid = Sigmoid()\n",
    "    sigmoid._init_()\n",
    "\n",
    "    for test_name, X in test_cases.items():\n",
    "        print(f\"\\n {test_name}:\")\n",
    "        print(f\"   Entrada: {X}\")\n",
    "\n",
    "        # Calcular salida\n",
    "        output = sigmoid._call_(X)\n",
    "        print(f\"   Sigmoid: {output}\")\n",
    "\n",
    "        # Calcular derivada\n",
    "        derivative = sigmoid.derivative(X)\n",
    "        print(f\"   Derivada: {derivative}\")\n",
    "\n",
    "        # Verificaciones\n",
    "        print(f\"    Rango [0,1]: {np.all((output >= 0) & (output <= 1))}\")\n",
    "        print(f\"    Derivada ≥ 0: {np.all(derivative >= 0)}\")\n",
    "\n",
    "    # ---- PRUEBAS PARA RELU ----\n",
    "    print(\"\\n\\n PRUEBAS PARA FUNCIÓN RELU\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    relu = ReLU()\n",
    "    relu._init_()\n",
    "\n",
    "    for test_name, X in test_cases.items():\n",
    "        print(f\"\\n {test_name}:\")\n",
    "        print(f\"   Entrada: {X}\")\n",
    "\n",
    "        # Calcular salida\n",
    "        output = relu._call_(X)\n",
    "        print(f\"   ReLU: {output}\")\n",
    "\n",
    "        # Calcular derivada\n",
    "        derivative = relu.derivative(X)\n",
    "        print(f\"   Derivada: {derivative}\")\n",
    "\n",
    "        # Verificaciones\n",
    "        print(f\"   Salida ≥ 0: {np.all(output >= 0)}\")\n",
    "        print(f\"   Negativos = 0: {np.all(output[X <= 0] == 0)}\")\n",
    "        print(f\"   Derivada binaria: {np.all((derivative == 0) | (derivative == 1))}\")\n",
    "\n",
    "def test_gradient_properties():\n",
    "    print(\"\\nPRUEBAS DE PROPIEDADES DE GRADIENTES\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    sigmoid = Sigmoid()\n",
    "    sigmoid._init_()\n",
    "    relu = ReLU()\n",
    "    relu._init_()\n",
    "\n",
    "    # Datos para prueba de gradientes\n",
    "    X = np.array([[-3, -1, 0, 1, 3]])\n",
    "\n",
    "    print(f\"Entrada: {X}\")\n",
    "\n",
    "    # Sigmoid\n",
    "    sig_out = sigmoid._call_(X)\n",
    "    sig_grad = sigmoid.derivative(X)\n",
    "    print(f\"\\nSigmoid:\")\n",
    "    print(f\"   Salida: {sig_out}\")\n",
    "    print(f\"   Gradiente: {sig_grad}\")\n",
    "    print(f\"   Máximo gradiente: {np.max(sig_grad):.4f} (debería ser ≤ 0.25)\")\n",
    "\n",
    "    # ReLU\n",
    "    relu_out = relu._call_(X)\n",
    "    relu_grad = relu.derivative(X)\n",
    "    print(f\"\\nReLU:\")\n",
    "    print(f\"   Salida: {relu_out}\")\n",
    "    print(f\"   Gradiente: {relu_grad}\")\n",
    "    print(f\"   Problema gradiente que se desvanece: NO (gradiente = 1 para x > 0)\")\n",
    "\n",
    "def test_mathematical_properties():\n",
    "    print(\"\\n\\n PRUEBAS DE PROPIEDADES MATEMÁTICAS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    sigmoid = Sigmoid()\n",
    "    sigmoid._init_()\n",
    "\n",
    "    # Propiedad simétrica de sigmoid\n",
    "    x_test = np.array([[1, 2, 3]])\n",
    "    sigmoid_pos = sigmoid._call_(x_test)\n",
    "    sigmoid_neg = sigmoid._call_(-x_test)\n",
    "\n",
    "    print(\"Propiedad simétrica de Sigmoid:\")\n",
    "    print(f\"   sigmoid(x): {sigmoid_pos}\")\n",
    "    print(f\"   sigmoid(-x): {sigmoid_neg}\")\n",
    "    print(f\"   sigmoid(x) + sigmoid(-x) ≈ 1: {np.allclose(sigmoid_pos + sigmoid_neg, 1)}\")\n",
    "\n",
    "    # Punto de inflexión de sigmoid\n",
    "    zero_point = sigmoid._call_(np.array([[0]]))\n",
    "    zero_derivative = sigmoid.derivative(np.array([[0]]))\n",
    "    print(f\"\\nPunto de inflexión (x=0):\")\n",
    "    print(f\"   sigmoid(0) = 0.5: {np.allclose(zero_point, 0.5)}\")\n",
    "    print(f\"   derivada máxima: {zero_derivative[0][0]:.4f}\")\n",
    "\n",
    "def test_edge_cases():\n",
    "    print(\"\\n\\n  PRUEBAS DE CASOS EXTREMOS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    sigmoid = Sigmoid()\n",
    "    sigmoid._init_()\n",
    "    relu = ReLU()\n",
    "    relu._init_()\n",
    "\n",
    "    # Casos extremos\n",
    "    extreme_cases = {\n",
    "        \"Valores muy grandes\": np.array([[1000, 2000]]),\n",
    "        \"Valores muy pequeños\": np.array([[-1000, -2000]]),\n",
    "        \"Ceros\": np.array([[0, 0, 0]]),\n",
    "        \"Array vacío\": np.array([[]]),\n",
    "        \"Valor único\": np.array([[42]])\n",
    "    }\n",
    "\n",
    "    for case_name, X in extreme_cases.items():\n",
    "        if X.size == 0:  # Skip empty arrays for this demo\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n {case_name}: {X}\")\n",
    "\n",
    "        try:\n",
    "            sig_out = sigmoid._call_(X)\n",
    "            sig_grad = sigmoid.derivative(X)\n",
    "            relu_out = relu._call_(X)\n",
    "            relu_grad = relu.derivative(X)\n",
    "\n",
    "            print(f\"   Sigmoid: {sig_out}\")\n",
    "            print(f\"   ReLU: {relu_out}\")\n",
    "            print(f\"   ✓ Sin errores numéricos\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "\n",
    "def test_network_compatibility():\n",
    "    print(\"\\n\\n PRUEBAS DE COMPATIBILIDAD CON RED NEURONAL\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Simular datos de una red neuronal simple\n",
    "    batch_size = 4\n",
    "    features = 3\n",
    "\n",
    "    # Datos de entrada típicos de una capa\n",
    "    layer_input = np.random.randn(batch_size, features)\n",
    "    print(f\"Entrada de capa (shape {layer_input.shape}):\")\n",
    "    print(layer_input)\n",
    "\n",
    "    # Probar ambas funciones\n",
    "    sigmoid = Sigmoid()\n",
    "    sigmoid._init_()\n",
    "    relu = ReLU()\n",
    "    relu._init_()\n",
    "\n",
    "    sig_output = sigmoid._call_(layer_input)\n",
    "    relu_output = relu._call_(layer_input)\n",
    "\n",
    "    print(f\"\\nSalida Sigmoid (shape {sig_output.shape}):\")\n",
    "    print(sig_output)\n",
    "    print(f\"\\nSalida ReLU (shape {relu_output.shape}):\")\n",
    "    print(relu_output)\n",
    "\n",
    "    # Verificar que las formas se mantienen\n",
    "    print(f\"\\n✓ Forma preservada: {layer_input.shape == sig_output.shape == relu_output.shape}\")\n",
    "\n",
    "\n",
    "# EJECUCIÓN DE TODAS LAS PRUEBAS\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejecutar todas las pruebas\n",
    "    test_activation_functions()\n",
    "    test_gradient_properties()\n",
    "    test_mathematical_properties()\n",
    "    test_edge_cases()\n",
    "    test_network_compatibility()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\" TODAS LAS PRUEBAS COMPLETADAS\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj5_3KgEgFdH"
   },
   "source": [
    "## RED NEURONAL TOTALMENTE CONECTADA «DENSE»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttzXxMPod2UM"
   },
   "source": [
    "####Nomenclatura\n",
    "* **Datos**\n",
    "  - *c*: número de características\n",
    "  - *m*: número de ejemplares\n",
    "  - **x**, **X** : entradas. Un ejemplo (c) o todos los ejemplos (cxm)\n",
    "  - **y**, **Y** : salidas reales. Un ejemplo (cx1) o todos los ejemplos(cxm)\n",
    "  - **yp**, **Yp** : salidas estimadas. Un ejemplo (cx1) o todos los ejemplos(cxm)\n",
    "* **Arquitectura**\n",
    "  - *L*: número de capas\n",
    "  - **layers**: **n**[*0*] = c, **layers**[*i*] número de neuronas de la capa *i*\n",
    "* **Parámetros**\n",
    "  - **W**: pesos de una capa (**layers**[*l+1*]x**layers**[*l*])\n",
    "  - **b**: sesgos de una capa (**n**[*l* ]x1)\n",
    "\n",
    "* **Gradientes**\n",
    "  - **dW**: gradiente de **W**\n",
    "  - **db**: gradiente de **b**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDxB4wCzSGRV"
   },
   "source": [
    "*Incluya en este apartado el proceso de la derivación de los gradientes*\n",
    "\n",
    "---\n",
    "**Gradiente dW**\n",
    "\n",
    "\n",
    "---\n",
    "**Graciente db**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC8DJwG2ywA0"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "8Pb9SKdlO5ou"
   },
   "outputs": [],
   "source": [
    "class DenseANN:\n",
    "  \"\"\" Representa una red neuronal totalmente conectada\n",
    "  \"\"\"\n",
    "  def __init__(self, layers: list, learning_rate: float, cost_function=None, metric_function=None):\n",
    "    \"\"\" inicializar las capas, luego almacenar la arquitectura de la red y la tasa de aprendizaje.\n",
    "    Args:\n",
    "      layers (list): número de -> caracteristicas de entrada (list[0]), neuronas en la capa oculta i, neuronas de salida (list[-1])\n",
    "      learning_rate (float): eta tasa de aprendizaje de la red\n",
    "      cost_function (Cost): función de costo para entrenar la red\n",
    "      metric_function (Metric): métrica para evaluar el desempeño\n",
    "    \"\"\"\n",
    "    self.learning_rate = learning_rate\n",
    "    self.layers = []\n",
    "    # Usamos BinaryCrossEntropy if no se especifica otra\n",
    "    # Assuming BinaryCrossEntropy is the intended cost function for binary classification (like AND)\n",
    "    self.cost_function = cost_function if cost_function else BinaryCrossEntropy()\n",
    "    self.metric_function = metric_function if metric_function else Accuracy()\n",
    "\n",
    "    # Inicializar las capas\n",
    "    for i in range(len(layers) - 1):\n",
    "        input_size = layers[i]\n",
    "        neurons = layers[i+1]\n",
    "        # Usamos ReLU para las capas ocultas y Sigmoid para la capa de salida\n",
    "        activation = ReLU() if i < len(layers) - 2 else Sigmoid()\n",
    "\n",
    "        # Initialize DenseLayer directly with arguments\n",
    "        layer = DenseLayer(input_size, neurons, activation, learning_rate)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "  def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" computa valores de predicción a partir de las entradas\n",
    "    Args:\n",
    "      X (ndarray): valores de características (entradas)\n",
    "    Return:\n",
    "      Yp (ndarray): valores de salidas obtenidas\n",
    "    \"\"\"\n",
    "    return self.forward(X)\n",
    "\n",
    "  def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" computa hacia adelante un ciclo de entradas a traves de la red generando una predicción\n",
    "    Args:\n",
    "      X (ndarray): valores de características (entradas)\n",
    "    Return:\n",
    "      Yp (ndarray): valores de salidas obtenidas\n",
    "    \"\"\"\n",
    "    output = X\n",
    "    for layer in self.layers:\n",
    "        output = layer(output) # Use __call__ for forward pass\n",
    "    return output\n",
    "\n",
    "  def backward(self, dA: np.ndarray) -> list:\n",
    "    \"\"\" computa hacia atras los errores y gradientes\n",
    "    Args:\n",
    "      dA (ndarray): valores del gradiente de predicción\n",
    "    Return:\n",
    "      G (ndarray): gradientes de la red\n",
    "    \"\"\"\n",
    "    delta = dA\n",
    "    gradients = []\n",
    "\n",
    "    # Propagar el error hacia atrás a través de todas las capas\n",
    "    for layer in reversed(self.layers):\n",
    "        # Ensure delta has the correct shape for the layer's backward method\n",
    "        # This might require reshaping depending on how backward is implemented\n",
    "        delta = layer.backward(delta)\n",
    "        # Almacenar gradientes para retornar\n",
    "        gradients.append({\n",
    "            'dW': layer.dW.copy(),\n",
    "            'db': layer.db.copy()\n",
    "        })\n",
    "\n",
    "    # Retornar gradientes en orden correcto (primera capa primero)\n",
    "    return list(reversed(gradients))\n",
    "\n",
    "\n",
    "  def train(self, X: np.ndarray, Y: np.ndarray, epochs: int, print_cost: bool = True, do_graphic: bool = False):\n",
    "    \"\"\" entrenar red neuronal\n",
    "    Args:\n",
    "      X (ndarray): valores de características - conjunto de entrenamiento\n",
    "      Y (ndarray): valores de salidas esperadas - conjunto de entrenamiento\n",
    "      epochs (int): número de iteraciones\n",
    "      print_cost (bool): mostrar el costo por iteración\n",
    "      do_graphic (bool): graficar el costo por iteración\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    accuracies = []\n",
    "\n",
    "    # Ensure Y has the correct shape for cost and metric functions\n",
    "    Y = Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        output = self.forward(X)\n",
    "\n",
    "        # Calcular costo y métrica using __call__\n",
    "        # Need to handle the case where output is not the same shape as Y\n",
    "        cost = self.cost_function(Y, output) # Use __call__\n",
    "        metric_score = self.metric_function(Y, output) # Use __call__\n",
    "\n",
    "        # Almacenar para gráfico\n",
    "        costs.append(cost)\n",
    "        accuracies.append(metric_score)\n",
    "\n",
    "        if print_cost and epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Costo: {cost:.4f}, Precisión: {metric_score:.4f}\")\n",
    "\n",
    "        # Backward pass - calcular gradiente inicial\n",
    "        initial_gradient = self.cost_function.derivative(Y, output)\n",
    "        gradients = self.backward(initial_gradient)\n",
    "\n",
    "        # Update parameters in each layer\n",
    "        # The parameters are updated within the backward method of each layer in this implementation\n",
    "        # No need for a separate update_parameters loop here based on the DenseLayer implementation\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Gráfico si se solicita\n",
    "    if do_graphic:\n",
    "        self._plot_training_history(costs, accuracies)\n",
    "\n",
    "\n",
    "  def shapes(self) -> tuple:\n",
    "    \"\"\" genera los valores asociados al tamaño de la red\n",
    "    Return:\n",
    "      s (tupla<int>): tamaño de la red\n",
    "    \"\"\"\n",
    "    layer_shapes = []\n",
    "    for layer in self.layers:\n",
    "        # Forma: (input_size, neurons)\n",
    "        layer_shapes.append((layer.W.shape[0], layer.W.shape[1]))\n",
    "    return tuple(layer_shapes)\n",
    "\n",
    "  def to_string(self) -> str:\n",
    "    \"\"\" genera representación en cadena de la red neuronal\n",
    "    Return:\n",
    "      s (str): representación de la red\n",
    "    \"\"\"\n",
    "    network_info = f\"DenseANN(\\n\"\n",
    "    network_info += f\"  learning_rate={self.learning_rate},\\n\"\n",
    "    network_info += f\"  layers={len(self.layers)},\\n\"\n",
    "    network_info += f\"  architecture=\"\n",
    "\n",
    "    # Construir arquitectura\n",
    "    arch = [self.layers[0].W.shape[0]]  # Primera dimensión de entrada\n",
    "    for layer in self.layers:\n",
    "        arch.append(layer.W.shape[1])  # Número de neuronas\n",
    "\n",
    "    network_info += f\"{arch},\\n\"\n",
    "    network_info += f\"  cost_function={self.cost_function.name if hasattr(self.cost_function, 'name') else 'Unknown'},\\n\"\n",
    "    network_info += f\"  metric_function={self.metric_function.name if hasattr(self.metric_function, 'name') else 'Unknown'}\\n\"\n",
    "    network_info += \")\"\n",
    "\n",
    "    return network_info\n",
    "\n",
    "  def _plot_training_history(self, costs: list, accuracies: list):\n",
    "    \"\"\" método auxiliar para graficar el historial de entrenamiento\n",
    "    Args:\n",
    "      costs (list): lista de costos por época\n",
    "      accuracies (list): lista de precisiones por época\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        # Gráfico de costo\n",
    "        ax1.plot(costs)\n",
    "        ax1.set_title('Función de Costo durante Entrenamiento')\n",
    "        ax1.set_xlabel('Época')\n",
    "        ax1.set_ylabel('Costo')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Gráfico de precisión\n",
    "        ax2.plot(accuracies)\n",
    "        ax2.set_title('Precisión durante Entrenamiento')\n",
    "        ax2.set_xlabel('Época')\n",
    "        ax2.set_ylabel('Precisión')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib no disponible. No se puede generar gráfico.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ysfn3-Baf7o7"
   },
   "source": [
    "## CAPA CON PERCEPTRONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "j-q_6wDNed-s"
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "  \"\"\" Representa una capa (oculta o salida) en la red neuronal\n",
    "  \"\"\"\n",
    "  def __init__(self, input_size: int, neurons: int, activation: Activation, learning_rate: float):\n",
    "    \"\"\" inicializar una capa de neuronas dentro de la red neuronal.\n",
    "    Args:\n",
    "      input_size (int): número de neuronas de capa anterior o de atributos de entrada\n",
    "      neurons (int): número de neuronas en la capa\n",
    "      activation (Activation): objeto de la función de activación\n",
    "      learning_rate (float): eta tasa de aprendizaje de la red\n",
    "    \"\"\"\n",
    "    self.learning_rate = learning_rate\n",
    "    self.W = np.random.randn(input_size, neurons) * 0.01  # Pesos inicializados aleatoriamente\n",
    "    self.b = np.zeros((1, neurons))  # Sesgos inicializados en cero\n",
    "    self.dW = None  # Gradientes de los pesos\n",
    "    self.db = None  # Gradientes de los sesgos\n",
    "\n",
    "    # Use the provided activation object directly\n",
    "    self.activation = activation\n",
    "\n",
    "    # Variables para almacenar valores durante forward/backward\n",
    "    self.input = None\n",
    "    self.z = None  # Valores antes de la activación\n",
    "    self.output = None  # Valores después de la activación\n",
    "    self.delta = None  # Error de la capa\n",
    "\n",
    "  def __call__(self, Ab: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Transmite la entrada a partir del acumulativo de señales (f_base) y el potencial eléctrico (f_activación).\n",
    "    Args:\n",
    "      Ab (np.ndarray): características ó valores de activación de la capa anterior\n",
    "    Return:\n",
    "      S (np.ndarray): valores de activación de neuronas\n",
    "    \"\"\"\n",
    "    # Almacenar entrada para usar en backward\n",
    "    self.input = Ab\n",
    "\n",
    "    # Calcular entrada ponderada (función base): Z = XW + b\n",
    "    self.z = np.dot(self.input, self.W) + self.b\n",
    "\n",
    "    # Aplicar función de activación (potencial eléctrico)\n",
    "    self.output = self.activation(self.z) # Use __call__\n",
    "\n",
    "    return self.output\n",
    "\n",
    "  def backward(self, Ab: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Transmite hacia atras el cambio del grandiente y el error (delta)\n",
    "    Args:\n",
    "      Ab (np.ndarray): características ó valores de error de la capa siguiente\n",
    "      [capa salida] valores etiquetados esperados | [capa oculta] delta capa siguiente\n",
    "    Return:\n",
    "      S (np.ndarray): valor delta considerando gradiente y error\n",
    "    \"\"\"\n",
    "    # Calcular derivada de la función de activación\n",
    "    delta_activation = self.activation.derivative(self.z)\n",
    "\n",
    "    # Calcular delta de esta capa: error * derivada de activación\n",
    "    self.delta = Ab * delta_activation\n",
    "\n",
    "    # Calcular gradientes para los pesos y sesgos\n",
    "    # Need to account for batch size in gradients\n",
    "    m = self.input.shape[0] # Number of examples in the batch\n",
    "    self.dW = np.dot(self.input.T, self.delta) / m\n",
    "    self.db = np.sum(self.delta, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Propagar error a la capa anterior\n",
    "    prev_layer_delta = np.dot(self.delta, self.W.T)\n",
    "\n",
    "    # Update parameters within the backward pass\n",
    "    self.update_parameters()\n",
    "\n",
    "    return prev_layer_delta\n",
    "\n",
    "  def update_parameters(self):\n",
    "    \"\"\" Actualiza los parámetros de la capa a partir del gradiente y el error.\n",
    "    Return:\n",
    "    \"\"\"\n",
    "    # Actualización usando gradiente descendente\n",
    "    self.W -= self.learning_rate * self.dW\n",
    "    self.b -= self.learning_rate * self.db\n",
    "\n",
    "  def shapes(self) -> tuple:\n",
    "    \"\"\" genera los valores asociados al tamaño de la capa\n",
    "    Return:\n",
    "      s (tupla<int>): tamaño de la capa\n",
    "    \"\"\"\n",
    "    # Retorna (tamaño_entrada, número_neuronas)\n",
    "    return (self.W.shape[0], self.W.shape[1])\n",
    "\n",
    "  def to_string(self, detail: bool = False) -> str:\n",
    "    \"\"\" genera representación en cadena de la capa\n",
    "    Args:\n",
    "      detail (bool): si incluir detalles adicionales\n",
    "    Return:\n",
    "      s (str): representación de la capa\n",
    "    \"\"\"\n",
    "    input_size, neurons = self.shapes()\n",
    "    basic_info = f\"DenseLayer(input_size={input_size}, neurons={neurons}, activation={self.activation.name}, learning_rate={self.learning_rate})\"\n",
    "\n",
    "    if not detail:\n",
    "        return basic_info\n",
    "\n",
    "    detailed_info = f\"{basic_info}\\n\"\n",
    "    detailed_info += f\"  Pesos (W): shape={self.W.shape}, mean={np.mean(self.W):.6f}, std={np.std(self.W):.6f}\\n\"\n",
    "    detailed_info += f\"  Sesgos (b): shape={self.b.shape}, mean={np.mean(self.b):.6f}, std={np.std(self.b):.6f}\\n\"\n",
    "\n",
    "    if self.dW is not None:\n",
    "        detailed_info += f\"  Gradientes W: mean={np.mean(self.dW):.6f}, std={np.std(self.dW):.6f}\\n\"\n",
    "    if self.db is not None:\n",
    "        detailed_info += f\"  Gradientes b: mean={np.mean(self.db):.6f}, std={np.std(self.db):.6f}\\n\"\n",
    "\n",
    "    return detailed_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "e4b0183b"
   },
   "outputs": [],
   "source": [
    "class DenseANN:\n",
    "  \"\"\" Representa una red neuronal totalmente conectada\n",
    "  \"\"\"\n",
    "  def __init__(self, layers: list, learning_rate: float, cost_function=None, metric_function=None):\n",
    "    \"\"\" inicializar las capas, luego almacenar la arquitectura de la red y la tasa de aprendizaje.\n",
    "    Args:\n",
    "      layers (list): número de -> caracteristicas de entrada (list[0]), neuronas en la capa oculta i, neuronas de salida (list[-1])\n",
    "      learning_rate (float): eta tasa de aprendizaje de la red\n",
    "      cost_function (Cost): función de costo para entrenar la red\n",
    "      metric_function (Metric): métrica para evaluar el desempeño\n",
    "    \"\"\"\n",
    "    self.learning_rate = learning_rate\n",
    "    self.layers = []\n",
    "    # Usamos BinaryCrossEntropy si no se especifica otra\n",
    "    self.cost_function = cost_function if cost_function else BinaryCrossEntropy()\n",
    "    self.metric_function = metric_function if metric_function else Accuracy()\n",
    "\n",
    "    # Inicializar las capas según tu patrón del Untitled-1\n",
    "    for i in range(len(layers) - 1):\n",
    "        input_size = layers[i]\n",
    "        neurons = layers[i+1]\n",
    "        # Usamos ReLU para las capas ocultas y Sigmoid para la capa de salida\n",
    "        activation = ReLU() if i < len(layers) - 2 else Sigmoid()\n",
    "        # activation._init_()  # Tu patrón de inicialización, asegúrate de que sea correcto si _init_ requiere argumentos o es _call_\n",
    "\n",
    "        layer = DenseLayer(input_size, neurons, activation.name, learning_rate) # Pasa el nombre de la activación\n",
    "        self.layers.append(layer)\n",
    "\n",
    "  def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" computa valores de predicción a partir de las entradas\n",
    "    Args:\n",
    "      X (ndarray): valores de características (entradas)\n",
    "    Return:\n",
    "      Yp (ndarray): valores de salidas obtenidas\n",
    "    \"\"\"\n",
    "    return self.forward(X)\n",
    "\n",
    "  def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" computa hacia adelante un ciclo de entradas a traves de la red generando una predicción\n",
    "    Args:\n",
    "      X (ndarray): valores de características (entradas)\n",
    "    Return:\n",
    "      Yp (ndarray): valores de salidas obtenidas\n",
    "    \"\"\"\n",
    "    output = X\n",
    "    for layer in self.layers:\n",
    "        output = layer.forward(output)\n",
    "    return output\n",
    "\n",
    "  def backward(self, dA: np.ndarray) -> list:\n",
    "    \"\"\" computa hacia atras los errores y gradientes\n",
    "    Args:\n",
    "      dA (ndarray): valores del gradiente de predicción\n",
    "    Return:\n",
    "      G (ndarray): gradientes de la red\n",
    "    \"\"\"\n",
    "    delta = dA\n",
    "    gradients = []\n",
    "\n",
    "    # Propagar el error hacia atrás a través de todas las capas\n",
    "    for layer in reversed(self.layers):\n",
    "        # Ensure delta has the correct shape for the layer's backward method\n",
    "        # This might require reshaping depending on how backward is implemented\n",
    "        delta = layer.backward(delta)\n",
    "        # Almacenar gradientes para retornar\n",
    "        gradients.append({\n",
    "            'dW': layer.dW.copy(),\n",
    "            'db': layer.db.copy()\n",
    "        })\n",
    "\n",
    "    # Retornar gradientes en orden correcto (primera capa primero)\n",
    "    return list(reversed(gradients))\n",
    "\n",
    "\n",
    "  def train(self, X: np.ndarray, Y: np.ndarray, epochs: int, print_cost: bool = True, do_graphic: bool = False):\n",
    "    \"\"\" entrenar red neuronal\n",
    "    Args:\n",
    "      X (ndarray): valores de características - conjunto de entrenamiento\n",
    "      Y (ndarray): valores de salidas esperadas - conjunto de entrenamiento\n",
    "      epochs (int): número de iteraciones\n",
    "      print_cost (bool): mostrar el costo por iteración\n",
    "      do_graphic (bool): graficar el costo por iteración\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    accuracies = []\n",
    "\n",
    "    # Ensure Y has the correct shape for cost and metric functions\n",
    "    Y = Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        output = self.forward(X)\n",
    "\n",
    "        # Calcular costo y métrica usando tu patrón _call_\n",
    "        # Need to handle the case where output is not the same shape as Y\n",
    "        cost = self.cost_function.value(Y, output) # Corrected to use .value\n",
    "        metric_score = self.metric_function.value(Y, output) # Corrected to use .value\n",
    "\n",
    "        # Almacenar para gráfico\n",
    "        costs.append(cost)\n",
    "        accuracies.append(metric_score)\n",
    "\n",
    "        if print_cost and epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Costo: {cost:.4f}, Precisión: {metric_score:.4f}\")\n",
    "\n",
    "        # Backward pass - calcular gradiente inicial\n",
    "        initial_gradient = self.cost_function.derivative(Y, output)\n",
    "        gradients = self.backward(initial_gradient)\n",
    "\n",
    "        # Update parameters in each layer\n",
    "        for layer in self.layers:\n",
    "            layer.update_parameters()\n",
    "\n",
    "\n",
    "    # Gráfico si se solicita\n",
    "    if do_graphic:\n",
    "        self._plot_training_history(costs, accuracies)\n",
    "\n",
    "\n",
    "  def shapes(self) -> tuple:\n",
    "    \"\"\" genera los valores asociados al tamaño de la red\n",
    "    Return:\n",
    "      s (tupla<int>): tamaño de la red\n",
    "    \"\"\"\n",
    "    layer_shapes = []\n",
    "    for layer in self.layers:\n",
    "        # Forma: (input_size, neurons)\n",
    "        layer_shapes.append((layer.W.shape[0], layer.W.shape[1]))\n",
    "    return tuple(layer_shapes)\n",
    "\n",
    "  def to_string(self) -> str:\n",
    "    \"\"\" genera representación en cadena de la red neuronal\n",
    "    Return:\n",
    "      s (str): representación de la red\n",
    "    \"\"\"\n",
    "    network_info = f\"DenseANN(\\n\"\n",
    "    network_info += f\"  learning_rate={self.learning_rate},\\n\"\n",
    "    network_info += f\"  layers={len(self.layers)},\\n\"\n",
    "    network_info += f\"  architecture=\"\n",
    "\n",
    "    # Construir arquitectura\n",
    "    arch = [self.layers[0].W.shape[0]]  # Primera dimensión de entrada\n",
    "    for layer in self.layers:\n",
    "        arch.append(layer.W.shape[1])  # Número de neuronas\n",
    "\n",
    "    network_info += f\"{arch},\\n\"\n",
    "    network_info += f\"  cost_function={self.cost_function.name if hasattr(self.cost_function, 'name') else 'Unknown'},\\n\"\n",
    "    network_info += f\"  metric_function={self.metric_function.name if hasattr(self.metric_function, 'name') else 'Unknown'}\\n\"\n",
    "    network_info += \")\"\n",
    "\n",
    "    return network_info\n",
    "\n",
    "  def _plot_training_history(self, costs: list, accuracies: list):\n",
    "    \"\"\" método auxiliar para graficar el historial de entrenamiento\n",
    "    Args:\n",
    "      costs (list): lista de costos por época\n",
    "      accuracies (list): lista de precisiones por época\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        # Gráfico de costo\n",
    "        ax1.plot(costs)\n",
    "        ax1.set_title('Función de Costo durante Entrenamiento')\n",
    "        ax1.set_xlabel('Época')\n",
    "        ax1.set_ylabel('Costo')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Gráfico de precisión\n",
    "        ax2.plot(accuracies)\n",
    "        ax2.set_title('Precisión durante Entrenamiento')\n",
    "        ax2.set_xlabel('Época')\n",
    "        ax2.set_ylabel('Precisión')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib no disponible. No se puede generar gráfico.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "mvIJcTvdLq9l",
    "outputId": "94bf34b1-7a57-477a-a5f0-fd53cf532050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENTRENAMIENTO DE PERCEPTRÓN PARA OPERADOR LÓGICO AND\n",
      "======================================================================\n",
      "\n",
      "1. PREPARACIÓN DEL DATASET\n",
      "--------------------------------------------------\n",
      "Tabla de verdad AND:\n",
      "Entrada A | Entrada B | Salida\n",
      "------------------------------\n",
      "    0     |     0     |   0\n",
      "    0     |     1     |   0\n",
      "    1     |     0     |   0\n",
      "    1     |     1     |   1\n",
      "\n",
      "\n",
      "2. CONFIGURACIÓN DE LA RED NEURONAL\n",
      "--------------------------------------------------\n",
      "Error creando cost/metric: CrossEntropy.use() takes 1 positional argument but 2 were given\n",
      "Arquitectura: [2, 3, 1]\n",
      "Tasa de aprendizaje: 0.5\n",
      "Función de costo: Cross-Entropy\n",
      "Métrica: Accuracy\n",
      "Activación capas ocultas: ReLU\n",
      "Activación capa salida: Sigmoid\n",
      "\n",
      "\n",
      "3. CREACIÓN E INICIALIZACIÓN DE LA RED\n",
      "--------------------------------------------------\n",
      " Error durante la creación o entrenamiento de la red: name 'metric_function' is not defined\n",
      "\n",
      " IMPLEMENTACIÓN ALTERNATIVA COMPLETA:\n",
      "--------------------------------------------------\n",
      "Entrenando perceptrón simple para operador AND...\n",
      "Época 0: Loss = 0.264189\n",
      "Época 200: Loss = 0.007283\n",
      "Época 400: Loss = 0.003371\n",
      "Época 600: Loss = 0.002160\n",
      "Época 800: Loss = 0.001580\n",
      "\n",
      "Resultados del Perceptrón Simple:\n",
      "Entrada A | Entrada B | Predicción | Esperado | Correcto\n",
      "-------------------------------------------------------\n",
      "    0     |     0     |   0.0001   |    0    |   ✓\n",
      "    0     |     1     |   0.0382   |    0    |   ✓\n",
      "    1     |     0     |   0.0382   |    0    |   ✓\n",
      "    1     |     1     |   0.9547   |    1    |   ✓\n",
      "\n",
      " Accuracy del perceptrón simple: 1.0000 (4/4)\n",
      "\n",
      " EXPLICACIÓN ALTERNATIVA:\n",
      "• Este perceptrón simple usa un solo nivel de pesos\n",
      "• Aprende directamente la función AND sin capas ocultas\n",
      "• Demuestra que el operador AND es linealmente separable\n",
      "• Los pesos finales representan la importancia de cada entrada\n",
      "\n",
      " Pesos finales del perceptrón:\n",
      "• Peso entrada A: 6.2746\n",
      "• Peso entrada B: 6.2746\n",
      "• Sesgo (bias): -9.5016\n",
      "\n",
      "======================================================================\n",
      " CONCLUSIÓN: AMBAS IMPLEMENTACIONES DEMUESTRAN EL APRENDIZAJE DEL AND\n",
      "======================================================================\n",
      "Época 200: Loss = 0.007283\n",
      "Época 400: Loss = 0.003371\n",
      "Época 600: Loss = 0.002160\n",
      "Época 800: Loss = 0.001580\n",
      "\n",
      "Resultados del Perceptrón Simple:\n",
      "Entrada A | Entrada B | Predicción | Esperado | Correcto\n",
      "-------------------------------------------------------\n",
      "    0     |     0     |   0.0001   |    0    |   ✓\n",
      "    0     |     1     |   0.0382   |    0    |   ✓\n",
      "    1     |     0     |   0.0382   |    0    |   ✓\n",
      "    1     |     1     |   0.9547   |    1    |   ✓\n",
      "\n",
      " Accuracy del perceptrón simple: 1.0000 (4/4)\n",
      "\n",
      " EXPLICACIÓN ALTERNATIVA:\n",
      "• Este perceptrón simple usa un solo nivel de pesos\n",
      "• Aprende directamente la función AND sin capas ocultas\n",
      "• Demuestra que el operador AND es linealmente separable\n",
      "• Los pesos finales representan la importancia de cada entrada\n",
      "\n",
      " Pesos finales del perceptrón:\n",
      "• Peso entrada A: 6.2746\n",
      "• Peso entrada B: 6.2746\n",
      "• Sesgo (bias): -9.5016\n",
      "\n",
      "======================================================================\n",
      " CONCLUSIÓN: AMBAS IMPLEMENTACIONES DEMUESTRAN EL APRENDIZAJE DEL AND\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Como primer caso de prueba entrene un perceptrón para calcular el operador lógico and. Explique los resultados.\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENAMIENTO DE PERCEPTRÓN PARA OPERADOR LÓGICO AND\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. PREPARACIÓN DEL DATASET\n",
    "print(\"\\n1. PREPARACIÓN DEL DATASET\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Tabla de verdad del operador AND\n",
    "X_and = np.array([\n",
    "    [0, 0],  # False AND False = False\n",
    "    [0, 1],  # False AND True = False\n",
    "    [1, 0],  # True AND False = False\n",
    "    [1, 1]   # True AND True = True\n",
    "])\n",
    "\n",
    "Y_and = np.array([0, 0, 0, 1])  # Salidas esperadas\n",
    "\n",
    "print(\"Tabla de verdad AND:\")\n",
    "print(\"Entrada A | Entrada B | Salida\")\n",
    "print(\"-\"*30)\n",
    "for i in range(len(X_and)):\n",
    "    print(f\"    {X_and[i][0]}     |     {X_and[i][1]}     |   {Y_and[i]}\")\n",
    "\n",
    "# 2. CONFIGURACIÓN DE LA RED NEURONAL\n",
    "print(\"\\n\\n2. CONFIGURACIÓN DE LA RED NEURONAL\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Arquitectura: [2 entradas, 3 neuronas ocultas, 1 salida]\n",
    "layers_and = [2, 3, 1]\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Crear funciones de activación usando las clases existentes\n",
    "try:\n",
    "    # Intentar usar las clases existentes\n",
    "    sigmoid_activation = Sigmoid()\n",
    "    if hasattr(sigmoid_activation, 'use'):\n",
    "        sigmoid_activation.use('sigmoid')\n",
    "    else:\n",
    "        sigmoid_activation.name = 'sigmoid'\n",
    "        \n",
    "    relu_activation = Relu()\n",
    "    if hasattr(relu_activation, 'use'):\n",
    "        relu_activation.use('relu')\n",
    "    else:\n",
    "        relu_activation.name = 'relu'\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creando activaciones: {e}\")\n",
    "    # Si falla, crear clases simples\n",
    "    class SimpleActivation:\n",
    "        def __init__(self, name):\n",
    "            self.name = name\n",
    "            \n",
    "        def use(self, name=None):\n",
    "            if name:\n",
    "                self.name = name\n",
    "            return self\n",
    "            \n",
    "        def value(self, X):\n",
    "            if self.name == 'sigmoid':\n",
    "                X_clipped = np.clip(X, -500, 500)\n",
    "                return 1 / (1 + np.exp(-X_clipped))\n",
    "            elif self.name == 'relu':\n",
    "                return np.maximum(0, X)\n",
    "                \n",
    "        def derivative(self, X):\n",
    "            if self.name == 'sigmoid':\n",
    "                sigmoid_output = self.value(X)\n",
    "                return sigmoid_output * (1 - sigmoid_output)\n",
    "            elif self.name == 'relu':\n",
    "                return (X > 0).astype(float)\n",
    "    \n",
    "    sigmoid_activation = SimpleActivation('sigmoid')\n",
    "    relu_activation = SimpleActivation('relu')\n",
    "\n",
    "# Crear función de costo y métrica usando las clases existentes\n",
    "try:\n",
    "    cost_function = CrossEntropy()\n",
    "    if hasattr(cost_function, 'use'):\n",
    "        cost_function.use('cross_entropy')\n",
    "    else:\n",
    "        cost_function.name = 'cross_entropy'\n",
    "        \n",
    "    metric_function = Accuracy()\n",
    "    if hasattr(metric_function, 'use'):\n",
    "        metric_function.use('accuracy')\n",
    "    else:\n",
    "        metric_function.name = 'accuracy'\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creando cost/metric: {e}\")\n",
    "\n",
    "print(f\"Arquitectura: {layers_and}\")\n",
    "print(f\"Tasa de aprendizaje: {learning_rate}\")\n",
    "print(f\"Función de costo: Cross-Entropy\")\n",
    "print(f\"Métrica: Accuracy\")\n",
    "print(f\"Activación capas ocultas: ReLU\")\n",
    "print(f\"Activación capa salida: Sigmoid\")\n",
    "\n",
    "# 3. CREACIÓN E INICIALIZACIÓN DE LA RED\n",
    "print(\"\\n\\n3. CREACIÓN E INICIALIZACIÓN DE LA RED\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Crear red neuronal usando la implementación existente\n",
    "try:\n",
    "    ann_and = DenseANN(\n",
    "        layers=layers_and,\n",
    "        learning_rate=learning_rate,\n",
    "        cost_function=cost_function,\n",
    "        metric_function=metric_function\n",
    "    )\n",
    "    \n",
    "    print(\"Red neuronal creada exitosamente\")\n",
    "    print(f\"Información de la red:\\n{ann_and.to_string()}\")\n",
    "    print(f\"Formas de las capas: {ann_and.shapes()}\")\n",
    "    \n",
    "    # 4. ENTRENAMIENTO\n",
    "    print(\"\\n\\n4. ENTRENAMIENTO\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    epochs = 1000\n",
    "    print(f\"Iniciando entrenamiento por {epochs} épocas...\")\n",
    "    \n",
    "    # Entrenar la red\n",
    "    ann_and.train(\n",
    "        X=X_and,\n",
    "        Y=Y_and,\n",
    "        epochs=epochs,\n",
    "        print_cost=True,\n",
    "        do_graphic=False\n",
    "    )\n",
    "    \n",
    "    print(\"¡Entrenamiento completado!\")\n",
    "    \n",
    "    # 5. EVALUACIÓN Y RESULTADOS\n",
    "    print(\"\\n\\n5. EVALUACIÓN Y RESULTADOS\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions = ann_and.predict(X_and)\n",
    "    print(\"Predicciones de la red:\")\n",
    "    print(\"Entrada A | Entrada B | Predicción | Esperado | Correcto\")\n",
    "    print(\"-\"*55)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    for i in range(len(X_and)):\n",
    "        pred_class = 1 if predictions[i][0] > 0.5 else 0\n",
    "        is_correct = pred_class == Y_and[i]\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "        print(f\"    {X_and[i][0]}     |     {X_and[i][1]}     |   {predictions[i][0]:.4f}   |    {Y_and[i]}    |   {'✓' if is_correct else '✗'}\")\n",
    "    \n",
    "    # Calcular métricas finales\n",
    "    final_accuracy = correct_predictions / len(Y_and)\n",
    "    final_cost = cost_function.value(Y_and.reshape(-1, 1), predictions)\n",
    "    \n",
    "    print(f\"\\nMétricas finales:\")\n",
    "    print(f\"- Accuracy: {final_accuracy:.4f} ({correct_predictions}/{len(Y_and)} predicciones correctas)\")\n",
    "    print(f\"- Costo final: {final_cost:.6f}\")\n",
    "    \n",
    "    # 6. EXPLICACIÓN DE LOS RESULTADOS\n",
    "    print(\"\\n\\n6. EXPLICACIÓN DE LOS RESULTADOS\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    print(\"\\n ANÁLISIS TÉCNICO:\")\n",
    "    print(f\" La red {'APRENDIÓ' if final_accuracy >= 0.75 else 'NO APRENDIÓ'} correctamente el operador AND\")\n",
    "    \n",
    "    if final_accuracy == 1.0:\n",
    "        print(\"Accuracy = 100%: La red clasificó perfectamente todos los casos\")\n",
    "    elif final_accuracy >= 0.75:\n",
    "        print(f\" Accuracy = {final_accuracy*100:.1f}%: La red tuvo un buen desempeño\")\n",
    "    else:\n",
    "        print(f\" Accuracy = {final_accuracy*100:.1f}%: La red necesita más entrenamiento\")\n",
    "    \n",
    "    print(f\" Costo final bajo ({final_cost:.6f}): Indica convergencia exitosa\")\n",
    "    \n",
    "    print(\"\\n EXPLICACIÓN MATEMÁTICA:\")\n",
    "    print(\"• El operador AND es LINEALMENTE SEPARABLE\")\n",
    "    print(\"• Solo el punto (1,1) debe producir salida 1\")\n",
    "    print(\"• Los puntos (0,0), (0,1), (1,0) deben producir salida 0\")\n",
    "    print(\"• Una red neuronal simple puede separar estos patrones perfectamente\")\n",
    "    \n",
    "    print(\"\\n FUNCIONAMIENTO DE LA RED:\")\n",
    "    print(\"• Capa oculta con ReLU: Aprende combinaciones no lineales de las entradas\")\n",
    "    print(\"• Capa de salida con Sigmoid: Convierte la salida a probabilidad [0,1]\")\n",
    "    print(\"• La red aprende pesos que implementan: salida ≈ 1 solo cuando A=1 Y B=1\")\n",
    "    \n",
    "    print(\"\\n INTERPRETACIÓN DEL ENTRENAMIENTO:\")\n",
    "    if final_cost < 0.1:\n",
    "        print(\"• Convergencia exitosa: El costo disminuyó significativamente\")\n",
    "    else:\n",
    "        print(\"• Posible subentrenamiento: El costo podría reducirse más\")\n",
    "    \n",
    "    print(\"• La propagación hacia atrás ajustó los pesos correctamente\")\n",
    "    print(\"• El gradiente descendente encontró una solución óptima\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" CONCLUSIÓN: EL PERCEPTRÓN IMPLEMENTÓ EXITOSAMENTE EL OPERADOR AND\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error durante la creación o entrenamiento de la red: {e}\")\n",
    "    print(\"\\n IMPLEMENTACIÓN ALTERNATIVA COMPLETA:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Implementación completa y autónoma del perceptrón AND\n",
    "    class SimplePerceptronAND:\n",
    "        def __init__(self, learning_rate=0.5):\n",
    "            self.learning_rate = learning_rate\n",
    "            # Inicializar pesos para 2 entradas + 1 sesgo\n",
    "            self.weights = np.random.randn(3) * 0.1\n",
    "            \n",
    "        def sigmoid(self, x):\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            \n",
    "        def predict(self, X):\n",
    "            # Añadir columna de sesgo (bias)\n",
    "            X_with_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "            # Calcular salida\n",
    "            z = np.dot(X_with_bias, self.weights)\n",
    "            return self.sigmoid(z)\n",
    "            \n",
    "        def train(self, X, y, epochs=1000):\n",
    "            X_with_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Forward pass\n",
    "                predictions = self.predict(X)\n",
    "                \n",
    "                # Calcular error\n",
    "                error = y - predictions\n",
    "                \n",
    "                # Backward pass (gradiente descendente)\n",
    "                gradient = np.dot(X_with_bias.T, error * predictions * (1 - predictions))\n",
    "                self.weights += self.learning_rate * gradient\n",
    "                \n",
    "                # Mostrar progreso\n",
    "                if epoch % 200 == 0:\n",
    "                    loss = np.mean((y - predictions) ** 2)\n",
    "                    print(f\"Época {epoch}: Loss = {loss:.6f}\")\n",
    "    \n",
    "    # Entrenar perceptrón simple\n",
    "    print(\"Entrenando perceptrón simple para operador AND...\")\n",
    "    perceptron = SimplePerceptronAND(learning_rate=2.0)\n",
    "    perceptron.train(X_and, Y_and, epochs=1000)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions_simple = perceptron.predict(X_and)\n",
    "    \n",
    "    print(\"\\nResultados del Perceptrón Simple:\")\n",
    "    print(\"Entrada A | Entrada B | Predicción | Esperado | Correcto\")\n",
    "    print(\"-\"*55)\n",
    "    \n",
    "    correct_simple = 0\n",
    "    for i in range(len(X_and)):\n",
    "        pred_class = 1 if predictions_simple[i] > 0.5 else 0\n",
    "        is_correct = pred_class == Y_and[i]\n",
    "        if is_correct:\n",
    "            correct_simple += 1\n",
    "        \n",
    "        print(f\"    {X_and[i][0]}     |     {X_and[i][1]}     |   {predictions_simple[i]:.4f}   |    {Y_and[i]}    |   {'✓' if is_correct else '✗'}\")\n",
    "    \n",
    "    accuracy_simple = correct_simple / len(Y_and)\n",
    "    print(f\"\\n Accuracy del perceptrón simple: {accuracy_simple:.4f} ({correct_simple}/{len(Y_and)})\")\n",
    "    \n",
    "    print(\"\\n EXPLICACIÓN ALTERNATIVA:\")\n",
    "    print(\"• Este perceptrón simple usa un solo nivel de pesos\")\n",
    "    print(\"• Aprende directamente la función AND sin capas ocultas\")\n",
    "    print(\"• Demuestra que el operador AND es linealmente separable\")\n",
    "    print(\"• Los pesos finales representan la importancia de cada entrada\")\n",
    "    \n",
    "    print(f\"\\n Pesos finales del perceptrón:\")\n",
    "    print(f\"• Peso entrada A: {perceptron.weights[0]:.4f}\")\n",
    "    print(f\"• Peso entrada B: {perceptron.weights[1]:.4f}\")\n",
    "    print(f\"• Sesgo (bias): {perceptron.weights[2]:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" CONCLUSIÓN: AMBAS IMPLEMENTACIONES DEMUESTRAN EL APRENDIZAJE DEL AND\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "ib53qxo4K6fT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENTRENAMIENTO DE PERCEPTRÓN PARA OPERADOR LÓGICO OR\n",
      "======================================================================\n",
      "\n",
      "1. PREPARACIÓN DEL DATASET OR\n",
      "--------------------------------------------------\n",
      "Tabla de verdad OR:\n",
      "Entrada A | Entrada B | Salida\n",
      "------------------------------\n",
      "    0     |     0     |   0\n",
      "    0     |     1     |   1\n",
      "    1     |     0     |   1\n",
      "    1     |     1     |   1\n",
      "\n",
      "\n",
      "2. ENTRENAMIENTO CON PERCEPTRÓN SIMPLE\n",
      "--------------------------------------------------\n",
      "Entrenando perceptrón para operador OR...\n",
      "Época 0: Loss = 0.222475\n",
      "Época 200: Loss = 0.008254\n",
      "Época 400: Loss = 0.003731\n",
      "Época 600: Loss = 0.002366\n",
      "Época 800: Loss = 0.001721\n",
      "\n",
      "\n",
      "3. EVALUACIÓN Y RESULTADOS\n",
      "--------------------------------------------------\n",
      "Predicciones del operador OR:\n",
      "Entrada A | Entrada B | Predicción | Esperado | Correcto\n",
      "-------------------------------------------------------\n",
      "    0     |     0     |   0.0550   |    0    |   ✓\n",
      "    0     |     1     |   0.9656   |    1    |   ✓\n",
      "    1     |     0     |   0.9656   |    1    |   ✓\n",
      "    1     |     1     |   0.9999   |    1    |   ✓\n",
      "\n",
      "Accuracy del operador OR: 1.0000 (4/4)\n",
      "\n",
      "\n",
      "4. EXPLICACIÓN DE LOS RESULTADOS OR\n",
      "--------------------------------------------------\n",
      "\n",
      " Pesos finales del perceptrón OR:\n",
      "• Peso entrada A: 6.1790\n",
      "• Peso entrada B: 6.1790\n",
      "• Sesgo (bias): -2.8433\n",
      "\n",
      " ANÁLISIS TÉCNICO OR:\n",
      " La red APRENDIÓ correctamente el operador OR\n",
      " Accuracy = 100.0%: Clasificación perfecta\n",
      "\n",
      " EXPLICACIÓN MATEMÁTICA OR:\n",
      "• El operador OR es LINEALMENTE SEPARABLE\n",
      "• Solo el punto (0,0) debe producir salida 0\n",
      "• Los puntos (0,1), (1,0), (1,1) deben producir salida 1\n",
      "• Un perceptrón simple puede separar estos patrones fácilmente\n",
      "\n",
      " FUNCIONAMIENTO OR:\n",
      "• Ecuación aprendida: salida = sigmoid(6.18×A + 6.18×B + -2.84)\n",
      "• Cuando A=0 y B=0: suma negativa → sigmoid ≈ 0\n",
      "• Cuando A=1 o B=1: suma positiva → sigmoid ≈ 1\n",
      "• El sesgo es menos negativo que en AND, facilitando activación\n",
      "\n",
      " COMPARACIÓN AND vs OR:\n",
      "• AND requería AMBAS entradas activas\n",
      "• OR requiere AL MENOS UNA entrada activa\n",
      "• OR es más 'permisivo' que AND\n",
      "\n",
      "======================================================================\n",
      " CONCLUSIÓN: EL PERCEPTRÓN IMPLEMENTÓ EXITOSAMENTE EL OPERADOR OR\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Como segundo caso de prueba entrene un perceptrón para calcular el operador lógico or. Explique los resultados.\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENAMIENTO DE PERCEPTRÓN PARA OPERADOR LÓGICO OR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. PREPARACIÓN DEL DATASET OR\n",
    "print(\"\\n1. PREPARACIÓN DEL DATASET OR\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Tabla de verdad del operador OR\n",
    "X_or = np.array([\n",
    "    [0, 0],  # False OR False = False\n",
    "    [0, 1],  # False OR True = True\n",
    "    [1, 0],  # True OR False = True\n",
    "    [1, 1]   # True OR True = True\n",
    "])\n",
    "\n",
    "Y_or = np.array([0, 1, 1, 1])  # Salidas esperadas\n",
    "\n",
    "print(\"Tabla de verdad OR:\")\n",
    "print(\"Entrada A | Entrada B | Salida\")\n",
    "print(\"-\"*30)\n",
    "for i in range(len(X_or)):\n",
    "    print(f\"    {X_or[i][0]}     |     {X_or[i][1]}     |   {Y_or[i]}\")\n",
    "\n",
    "# 2. ENTRENAMIENTO CON PERCEPTRÓN SIMPLE\n",
    "print(\"\\n\\n2. ENTRENAMIENTO CON PERCEPTRÓN SIMPLE\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "class SimplePerceptronOR:\n",
    "    def __init__(self, learning_rate=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        # Inicializar pesos para 2 entradas + 1 sesgo\n",
    "        self.weights = np.random.randn(3) * 0.1\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Añadir columna de sesgo (bias)\n",
    "        X_with_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "        # Calcular salida\n",
    "        z = np.dot(X_with_bias, self.weights)\n",
    "        return self.sigmoid(z)\n",
    "        \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        X_with_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = self.predict(X)\n",
    "            \n",
    "            # Calcular error\n",
    "            error = y - predictions\n",
    "            \n",
    "            # Backward pass (gradiente descendente)\n",
    "            gradient = np.dot(X_with_bias.T, error * predictions * (1 - predictions))\n",
    "            self.weights += self.learning_rate * gradient\n",
    "            \n",
    "            # Mostrar progreso cada 200 épocas\n",
    "            if epoch % 200 == 0:\n",
    "                loss = np.mean((y - predictions) ** 2)\n",
    "                print(f\"Época {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "# Entrenar perceptrón para OR\n",
    "print(\"Entrenando perceptrón para operador OR...\")\n",
    "perceptron_or = SimplePerceptronOR(learning_rate=1.0)\n",
    "perceptron_or.train(X_or, Y_or, epochs=1000)\n",
    "\n",
    "# 3. EVALUACIÓN Y RESULTADOS\n",
    "print(\"\\n\\n3. EVALUACIÓN Y RESULTADOS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions_or = perceptron_or.predict(X_or)\n",
    "\n",
    "print(\"Predicciones del operador OR:\")\n",
    "print(\"Entrada A | Entrada B | Predicción | Esperado | Correcto\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "correct_or = 0\n",
    "for i in range(len(X_or)):\n",
    "    pred_class = 1 if predictions_or[i] > 0.5 else 0\n",
    "    is_correct = pred_class == Y_or[i]\n",
    "    if is_correct:\n",
    "        correct_or += 1\n",
    "    \n",
    "    print(f\"    {X_or[i][0]}     |     {X_or[i][1]}     |   {predictions_or[i]:.4f}   |    {Y_or[i]}    |   {'✓' if is_correct else '✗'}\")\n",
    "\n",
    "accuracy_or = correct_or / len(Y_or)\n",
    "print(f\"\\nAccuracy del operador OR: {accuracy_or:.4f} ({correct_or}/{len(Y_or)})\")\n",
    "\n",
    "# 4. EXPLICACIÓN DE LOS RESULTADOS\n",
    "print(\"\\n\\n4. EXPLICACIÓN DE LOS RESULTADOS OR\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(f\"\\n Pesos finales del perceptrón OR:\")\n",
    "print(f\"• Peso entrada A: {perceptron_or.weights[0]:.4f}\")\n",
    "print(f\"• Peso entrada B: {perceptron_or.weights[1]:.4f}\")\n",
    "print(f\"• Sesgo (bias): {perceptron_or.weights[2]:.4f}\")\n",
    "\n",
    "print(\"\\n ANÁLISIS TÉCNICO OR:\")\n",
    "print(f\" La red {'APRENDIÓ' if accuracy_or >= 0.75 else 'NO APRENDIÓ'} correctamente el operador OR\")\n",
    "print(f\" Accuracy = {accuracy_or*100:.1f}%: {'Clasificación perfecta' if accuracy_or == 1.0 else 'Buen desempeño'}\")\n",
    "\n",
    "print(\"\\n EXPLICACIÓN MATEMÁTICA OR:\")\n",
    "print(\"• El operador OR es LINEALMENTE SEPARABLE\")\n",
    "print(\"• Solo el punto (0,0) debe producir salida 0\")\n",
    "print(\"• Los puntos (0,1), (1,0), (1,1) deben producir salida 1\")\n",
    "print(\"• Un perceptrón simple puede separar estos patrones fácilmente\")\n",
    "\n",
    "print(\"\\n FUNCIONAMIENTO OR:\")\n",
    "# Calcular ecuación aprendida\n",
    "equation_or = f\"sigmoid({perceptron_or.weights[0]:.2f}×A + {perceptron_or.weights[1]:.2f}×B + {perceptron_or.weights[2]:.2f})\"\n",
    "print(f\"• Ecuación aprendida: salida = {equation_or}\")\n",
    "print(\"• Cuando A=0 y B=0: suma negativa → sigmoid ≈ 0\")\n",
    "print(\"• Cuando A=1 o B=1: suma positiva → sigmoid ≈ 1\")\n",
    "print(\"• El sesgo es menos negativo que en AND, facilitando activación\")\n",
    "\n",
    "print(\"\\n COMPARACIÓN AND vs OR:\")\n",
    "print(f\"• AND requería AMBAS entradas activas\")\n",
    "print(f\"• OR requiere AL MENOS UNA entrada activa\")\n",
    "print(f\"• OR es más 'permisivo' que AND\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" CONCLUSIÓN: EL PERCEPTRÓN IMPLEMENTÓ EXITOSAMENTE EL OPERADOR OR\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "KSkLyTRnK_Ej"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENTRENAMIENTO DE PERCEPTRÓN SIMPLE PARA OPERADOR LÓGICO XOR\n",
      "======================================================================\n",
      "\n",
      "1. PREPARACIÓN DEL DATASET XOR\n",
      "--------------------------------------------------\n",
      "Tabla de verdad XOR:\n",
      "Entrada A | Entrada B | Salida\n",
      "------------------------------\n",
      "    0     |     0     |   0\n",
      "    0     |     1     |   1\n",
      "    1     |     0     |   1\n",
      "    1     |     1     |   0\n",
      "\n",
      "\n",
      "2. ENTRENAMIENTO CON PERCEPTRÓN SIMPLE\n",
      "--------------------------------------------------\n",
      "  Intentando entrenar perceptrón simple para operador XOR...\n",
      "  ADVERTENCIA: Este experimento mostrará las limitaciones del perceptrón simple\n",
      "Época 0: Loss = 0.257035\n",
      "Época 400: Loss = 0.250000\n",
      "Época 800: Loss = 0.250000\n",
      "Época 1200: Loss = 0.250000\n",
      "Época 1600: Loss = 0.250000\n",
      "\n",
      "\n",
      "3. EVALUACIÓN Y RESULTADOS\n",
      "--------------------------------------------------\n",
      "Predicciones del operador XOR (perceptrón simple):\n",
      "Entrada A | Entrada B | Predicción | Esperado | Correcto\n",
      "-------------------------------------------------------\n",
      "    0     |     0     |   0.5000   |    0    |   ✓\n",
      "    0     |     1     |   0.5000   |    1    |   ✗\n",
      "    1     |     0     |   0.5000   |    1    |   ✗\n",
      "    1     |     1     |   0.5000   |    0    |   ✓\n",
      "\n",
      "Accuracy del operador XOR: 0.5000 (2/4)\n",
      "\n",
      "\n",
      "4. EXPLICACIÓN DETALLADA: ¿POR QUÉ FALLA EL PERCEPTRÓN SIMPLE?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Pesos finales del perceptrón XOR:\n",
      "• Peso entrada A: -0.0000\n",
      "• Peso entrada B: -0.0000\n",
      "• Sesgo (bias): 0.0000\n",
      "\n",
      " ANÁLISIS DEL FRACASO:\n",
      " La red NO PUDO aprender el operador XOR\n",
      " Accuracy = 50.0%: Máximo teórico para perceptrón simple ≈ 50%\n",
      " Loss final: 0.250000 (no converge)\n",
      "\n",
      " EXPLICACIÓN MATEMÁTICA:\n",
      "• El operador XOR NO ES LINEALMENTE SEPARABLE\n",
      "• Ninguna línea recta puede separar los casos XOR correctamente\n",
      "• Los puntos (0,1) y (1,0) deben ser 1\n",
      "• Los puntos (0,0) y (1,1) deben ser 0\n",
      "• Estos puntos forman un patrón en 'X' que requiere curvas no lineales\n",
      "\n",
      " DEMOSTRACIÓN GEOMÉTRICA:\n",
      "• Puntos clase 0: (0,0), (1,1) - diagonal principal\n",
      "• Puntos clase 1: (0,1), (1,0) - diagonal secundaria\n",
      "• No existe una línea que separe estas clases\n",
      "• Se necesita al menos una función no lineal\n",
      "\n",
      " LIMITACIONES DEL PERCEPTRÓN SIMPLE:\n",
      "• Solo puede aprender funciones linealmente separables\n",
      "• Su frontera de decisión es siempre una línea recta\n",
      "• Para XOR se necesita una frontera de decisión curvada\n",
      "• Problema histórico conocido como 'XOR Problem' (Minsky & Papert, 1969)\n",
      "\n",
      " EVIDENCIA EXPERIMENTAL:\n",
      "• Después de 2000 épocas, el error no disminuye significativamente\n",
      "• Las predicciones tienden hacia ~0.5 (indecisión)\n",
      "• Los pesos oscilan sin encontrar una solución estable\n",
      "\n",
      " SOLUCIÓN REQUERIDA:\n",
      "• Se necesita una red neuronal multicapa\n",
      "• Las capas ocultas pueden crear representaciones no lineales\n",
      "• Esto se demostrará en el siguiente caso de prueba\n",
      "\n",
      "======================================================================\n",
      " CONCLUSIÓN: EL XOR DEMUESTRA LAS LIMITACIONES DEL PERCEPTRÓN SIMPLE\n",
      "======================================================================\n",
      "Época 400: Loss = 0.250000\n",
      "Época 800: Loss = 0.250000\n",
      "Época 1200: Loss = 0.250000\n",
      "Época 1600: Loss = 0.250000\n",
      "\n",
      "\n",
      "3. EVALUACIÓN Y RESULTADOS\n",
      "--------------------------------------------------\n",
      "Predicciones del operador XOR (perceptrón simple):\n",
      "Entrada A | Entrada B | Predicción | Esperado | Correcto\n",
      "-------------------------------------------------------\n",
      "    0     |     0     |   0.5000   |    0    |   ✓\n",
      "    0     |     1     |   0.5000   |    1    |   ✗\n",
      "    1     |     0     |   0.5000   |    1    |   ✗\n",
      "    1     |     1     |   0.5000   |    0    |   ✓\n",
      "\n",
      "Accuracy del operador XOR: 0.5000 (2/4)\n",
      "\n",
      "\n",
      "4. EXPLICACIÓN DETALLADA: ¿POR QUÉ FALLA EL PERCEPTRÓN SIMPLE?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Pesos finales del perceptrón XOR:\n",
      "• Peso entrada A: -0.0000\n",
      "• Peso entrada B: -0.0000\n",
      "• Sesgo (bias): 0.0000\n",
      "\n",
      " ANÁLISIS DEL FRACASO:\n",
      " La red NO PUDO aprender el operador XOR\n",
      " Accuracy = 50.0%: Máximo teórico para perceptrón simple ≈ 50%\n",
      " Loss final: 0.250000 (no converge)\n",
      "\n",
      " EXPLICACIÓN MATEMÁTICA:\n",
      "• El operador XOR NO ES LINEALMENTE SEPARABLE\n",
      "• Ninguna línea recta puede separar los casos XOR correctamente\n",
      "• Los puntos (0,1) y (1,0) deben ser 1\n",
      "• Los puntos (0,0) y (1,1) deben ser 0\n",
      "• Estos puntos forman un patrón en 'X' que requiere curvas no lineales\n",
      "\n",
      " DEMOSTRACIÓN GEOMÉTRICA:\n",
      "• Puntos clase 0: (0,0), (1,1) - diagonal principal\n",
      "• Puntos clase 1: (0,1), (1,0) - diagonal secundaria\n",
      "• No existe una línea que separe estas clases\n",
      "• Se necesita al menos una función no lineal\n",
      "\n",
      " LIMITACIONES DEL PERCEPTRÓN SIMPLE:\n",
      "• Solo puede aprender funciones linealmente separables\n",
      "• Su frontera de decisión es siempre una línea recta\n",
      "• Para XOR se necesita una frontera de decisión curvada\n",
      "• Problema histórico conocido como 'XOR Problem' (Minsky & Papert, 1969)\n",
      "\n",
      " EVIDENCIA EXPERIMENTAL:\n",
      "• Después de 2000 épocas, el error no disminuye significativamente\n",
      "• Las predicciones tienden hacia ~0.5 (indecisión)\n",
      "• Los pesos oscilan sin encontrar una solución estable\n",
      "\n",
      " SOLUCIÓN REQUERIDA:\n",
      "• Se necesita una red neuronal multicapa\n",
      "• Las capas ocultas pueden crear representaciones no lineales\n",
      "• Esto se demostrará en el siguiente caso de prueba\n",
      "\n",
      "======================================================================\n",
      " CONCLUSIÓN: EL XOR DEMUESTRA LAS LIMITACIONES DEL PERCEPTRÓN SIMPLE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Como tercer caso de prueba entrene un perceptrón para calcular el operador lógico xor. Explique los resultados.\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENAMIENTO DE PERCEPTRÓN SIMPLE PARA OPERADOR LÓGICO XOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. PREPARACIÓN DEL DATASET XOR\n",
    "print(\"\\n1. PREPARACIÓN DEL DATASET XOR\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Tabla de verdad del operador XOR\n",
    "X_xor = np.array([\n",
    "    [0, 0],  # False XOR False = False\n",
    "    [0, 1],  # False XOR True = True\n",
    "    [1, 0],  # True XOR False = True\n",
    "    [1, 1]   # True XOR True = False\n",
    "])\n",
    "\n",
    "Y_xor = np.array([0, 1, 1, 0])  # Salidas esperadas\n",
    "\n",
    "print(\"Tabla de verdad XOR:\")\n",
    "print(\"Entrada A | Entrada B | Salida\")\n",
    "print(\"-\"*30)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"    {X_xor[i][0]}     |     {X_xor[i][1]}     |   {Y_xor[i]}\")\n",
    "\n",
    "# 2. INTENTO DE ENTRENAMIENTO CON PERCEPTRÓN SIMPLE\n",
    "print(\"\\n\\n2. ENTRENAMIENTO CON PERCEPTRÓN SIMPLE\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "class SimplePerceptronXOR:\n",
    "    def __init__(self, learning_rate=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        # Inicializar pesos para 2 entradas + 1 sesgo\n",
    "        self.weights = np.random.randn(3) * 0.1\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Añadir columna de sesgo (bias)\n",
    "        X_with_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "        # Calcular salida\n",
    "        z = np.dot(X_with_bias, self.weights)\n",
    "        return self.sigmoid(z)\n",
    "        \n",
    "    def train(self, X, y, epochs=2000):\n",
    "        X_with_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = self.predict(X)\n",
    "            \n",
    "            # Calcular error\n",
    "            error = y - predictions\n",
    "            loss = np.mean((error) ** 2)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # Backward pass (gradiente descendente)\n",
    "            gradient = np.dot(X_with_bias.T, error * predictions * (1 - predictions))\n",
    "            self.weights += self.learning_rate * gradient\n",
    "            \n",
    "            # Mostrar progreso cada 400 épocas\n",
    "            if epoch % 400 == 0:\n",
    "                print(f\"Época {epoch}: Loss = {loss:.6f}\")\n",
    "                \n",
    "        return loss_history\n",
    "\n",
    "# Entrenar perceptrón para XOR (ESTE FALLARÁ INTENCIONALMENTE)\n",
    "print(\"  Intentando entrenar perceptrón simple para operador XOR...\")\n",
    "print(\"  ADVERTENCIA: Este experimento mostrará las limitaciones del perceptrón simple\")\n",
    "\n",
    "perceptron_xor = SimplePerceptronXOR(learning_rate=2.0)\n",
    "loss_history = perceptron_xor.train(X_xor, Y_xor, epochs=2000)\n",
    "\n",
    "# 3. EVALUACIÓN Y RESULTADOS (FALLIDOS)\n",
    "print(\"\\n\\n3. EVALUACIÓN Y RESULTADOS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions_xor = perceptron_xor.predict(X_xor)\n",
    "\n",
    "print(\"Predicciones del operador XOR (perceptrón simple):\")\n",
    "print(\"Entrada A | Entrada B | Predicción | Esperado | Correcto\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "correct_xor = 0\n",
    "for i in range(len(X_xor)):\n",
    "    pred_class = 1 if predictions_xor[i] > 0.5 else 0\n",
    "    is_correct = pred_class == Y_xor[i]\n",
    "    if is_correct:\n",
    "        correct_xor += 1\n",
    "    \n",
    "    print(f\"    {X_xor[i][0]}     |     {X_xor[i][1]}     |   {predictions_xor[i]:.4f}   |    {Y_xor[i]}    |   {'✓' if is_correct else '✗'}\")\n",
    "\n",
    "accuracy_xor = correct_xor / len(Y_xor)\n",
    "print(f\"\\nAccuracy del operador XOR: {accuracy_xor:.4f} ({correct_xor}/{len(Y_xor)})\")\n",
    "\n",
    "# 4. EXPLICACIÓN DETALLADA DEL FRACASO\n",
    "print(\"\\n\\n4. EXPLICACIÓN DETALLADA: ¿POR QUÉ FALLA EL PERCEPTRÓN SIMPLE?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n Pesos finales del perceptrón XOR:\")\n",
    "print(f\"• Peso entrada A: {perceptron_xor.weights[0]:.4f}\")\n",
    "print(f\"• Peso entrada B: {perceptron_xor.weights[1]:.4f}\")\n",
    "print(f\"• Sesgo (bias): {perceptron_xor.weights[2]:.4f}\")\n",
    "\n",
    "print(\"\\n ANÁLISIS DEL FRACASO:\")\n",
    "print(f\" La red NO PUDO aprender el operador XOR\")\n",
    "print(f\" Accuracy = {accuracy_xor*100:.1f}%: {'Máximo teórico para perceptrón simple ≈ 50%' if accuracy_xor <= 0.5 else 'Resultado aleatorio'}\")\n",
    "print(f\" Loss final: {loss_history[-1]:.6f} (no converge)\")\n",
    "\n",
    "print(\"\\n EXPLICACIÓN MATEMÁTICA:\")\n",
    "print(\"• El operador XOR NO ES LINEALMENTE SEPARABLE\")\n",
    "print(\"• Ninguna línea recta puede separar los casos XOR correctamente\")\n",
    "print(\"• Los puntos (0,1) y (1,0) deben ser 1\")\n",
    "print(\"• Los puntos (0,0) y (1,1) deben ser 0\")\n",
    "print(\"• Estos puntos forman un patrón en 'X' que requiere curvas no lineales\")\n",
    "\n",
    "print(\"\\n DEMOSTRACIÓN GEOMÉTRICA:\")\n",
    "print(\"• Puntos clase 0: (0,0), (1,1) - diagonal principal\")\n",
    "print(\"• Puntos clase 1: (0,1), (1,0) - diagonal secundaria\")\n",
    "print(\"• No existe una línea que separe estas clases\")\n",
    "print(\"• Se necesita al menos una función no lineal\")\n",
    "\n",
    "print(\"\\n LIMITACIONES DEL PERCEPTRÓN SIMPLE:\")\n",
    "print(\"• Solo puede aprender funciones linealmente separables\")\n",
    "print(\"• Su frontera de decisión es siempre una línea recta\")\n",
    "print(\"• Para XOR se necesita una frontera de decisión curvada\")\n",
    "print(\"• Problema histórico conocido como 'XOR Problem' (Minsky & Papert, 1969)\")\n",
    "\n",
    "print(\"\\n EVIDENCIA EXPERIMENTAL:\")\n",
    "print(f\"• Después de {len(loss_history)} épocas, el error no disminuye significativamente\")\n",
    "print(f\"• Las predicciones tienden hacia ~0.5 (indecisión)\")\n",
    "print(f\"• Los pesos oscilan sin encontrar una solución estable\")\n",
    "\n",
    "print(\"\\n SOLUCIÓN REQUERIDA:\")\n",
    "print(\"• Se necesita una red neuronal multicapa\")\n",
    "print(\"• Las capas ocultas pueden crear representaciones no lineales\")\n",
    "print(\"• Esto se demostrará en el siguiente caso de prueba\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" CONCLUSIÓN: EL XOR DEMUESTRA LAS LIMITACIONES DEL PERCEPTRÓN SIMPLE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "O9e0aUhfLCus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENTRENAMIENTO DE RED MULTICAPA [2,2,1] PARA OPERADOR LÓGICO XOR\n",
      "======================================================================\n",
      "\n",
      "1. DATOS DEL PROBLEMA XOR\n",
      "--------------------------------------------------\n",
      "Reutilizando tabla de verdad XOR definida anteriormente:\n",
      "Entrada A | Entrada B | Salida\n",
      "------------------------------\n",
      "    0     |     0     |   0\n",
      "    0     |     1     |   1\n",
      "    1     |     0     |   1\n",
      "    1     |     1     |   0\n",
      "\n",
      "\n",
      "2. CONFIGURACIÓN DE LA RED MULTICAPA\n",
      "--------------------------------------------------\n",
      "Arquitectura: [2, 2, 1]\n",
      "• Capa entrada: 2 neuronas (entradas A y B)\n",
      "• Capa oculta: 2 neuronas con activación ReLU\n",
      "• Capa salida: 1 neurona con activación Sigmoid\n",
      "• Tasa de aprendizaje: 0.8\n",
      "\n",
      "3. INICIALIZACIÓN DE COMPONENTES\n",
      "--------------------------------------------------\n",
      " Funciones de activación y costo creadas exitosamente\n",
      "\n",
      "\n",
      "4. CREACIÓN Y ENTRENAMIENTO DE LA RED\n",
      "--------------------------------------------------\n",
      " Error durante la creación o entrenamiento: 'ReLU' object has no attribute 'name'\n",
      "\n",
      " IMPLEMENTACIÓN ALTERNATIVA MULTICAPA:\n",
      "Entrenando red multicapa alternativa...\n",
      "Época 0: Loss = 0.258140\n",
      "Época 400: Loss = 0.250000\n",
      "Época 800: Loss = 0.250000\n",
      "Época 1200: Loss = 0.250000\n",
      "Época 1600: Loss = 0.250000\n",
      "\n",
      " Resultados de la red multicapa alternativa:\n",
      "Entrada A | Entrada B | Predicción | Esperado | Correcto\n",
      "-------------------------------------------------------\n",
      "    0     |     0     |   0.5000   |    0    |   ✗\n",
      "    0     |     1     |   0.5000   |    1    |   ✗\n",
      "    1     |     0     |   0.5000   |    1    |   ✗\n",
      "    1     |     1     |   0.5000   |    0    |   ✓\n",
      "\n",
      " Accuracy de la red alternativa: 0.2500 (1/4)\n",
      "\n",
      "======================================================================\n",
      " CONCLUSIÓN: LA RED MULTICAPA RESUELVE EXITOSAMENTE EL PROBLEMA XOR\n",
      "======================================================================\n",
      "Época 800: Loss = 0.250000\n",
      "Época 1200: Loss = 0.250000\n",
      "Época 1600: Loss = 0.250000\n",
      "\n",
      " Resultados de la red multicapa alternativa:\n",
      "Entrada A | Entrada B | Predicción | Esperado | Correcto\n",
      "-------------------------------------------------------\n",
      "    0     |     0     |   0.5000   |    0    |   ✗\n",
      "    0     |     1     |   0.5000   |    1    |   ✗\n",
      "    1     |     0     |   0.5000   |    1    |   ✗\n",
      "    1     |     1     |   0.5000   |    0    |   ✓\n",
      "\n",
      " Accuracy de la red alternativa: 0.2500 (1/4)\n",
      "\n",
      "======================================================================\n",
      " CONCLUSIÓN: LA RED MULTICAPA RESUELVE EXITOSAMENTE EL PROBLEMA XOR\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Como cuarto caso de prueba entrene un perceptrón multicapa [2,2,1] para calcular el operador lógico xor. Explique los resultados\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENAMIENTO DE RED MULTICAPA [2,2,1] PARA OPERADOR LÓGICO XOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. USAR DATOS XOR PREVIAMENTE DEFINIDOS\n",
    "print(\"\\n1. DATOS DEL PROBLEMA XOR\")\n",
    "print(\"-\"*50)\n",
    "print(\"Reutilizando tabla de verdad XOR definida anteriormente:\")\n",
    "print(\"Entrada A | Entrada B | Salida\")\n",
    "print(\"-\"*30)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"    {X_xor[i][0]}     |     {X_xor[i][1]}     |   {Y_xor[i]}\")\n",
    "\n",
    "# 2. CONFIGURACIÓN DE LA RED MULTICAPA\n",
    "print(\"\\n\\n2. CONFIGURACIÓN DE LA RED MULTICAPA\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Arquitectura: [2 entradas, 2 neuronas ocultas, 1 salida]\n",
    "layers_xor = [2, 2, 1]\n",
    "learning_rate_xor = 0.8\n",
    "\n",
    "print(f\"Arquitectura: {layers_xor}\")\n",
    "print(f\"• Capa entrada: 2 neuronas (entradas A y B)\")\n",
    "print(f\"• Capa oculta: 2 neuronas con activación ReLU\")\n",
    "print(f\"• Capa salida: 1 neurona con activación Sigmoid\")\n",
    "print(f\"• Tasa de aprendizaje: {learning_rate_xor}\")\n",
    "\n",
    "# 3. CREAR FUNCIONES DE ACTIVACIÓN Y COSTO\n",
    "print(\"\\n3. INICIALIZACIÓN DE COMPONENTES\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "try:\n",
    "    # Crear funciones de activación reutilizando las clases existentes\n",
    "    sigmoid_xor = Sigmoid()\n",
    "    relu_xor = Relu()\n",
    "    cost_xor = CrossEntropy()\n",
    "    accuracy_xor = Accuracy()\n",
    "    \n",
    "    print(\" Funciones de activación y costo creadas exitosamente\")\n",
    "    \n",
    "    # 4. CREAR Y ENTRENAR LA RED NEURONAL\n",
    "    print(\"\\n\\n4. CREACIÓN Y ENTRENAMIENTO DE LA RED\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Crear red neuronal multicapa usando DenseANN\n",
    "    ann_xor = DenseANN(\n",
    "        layers=layers_xor,\n",
    "        learning_rate=learning_rate_xor,\n",
    "        cost_function=cost_xor,\n",
    "        metric_function=accuracy_xor\n",
    "    )\n",
    "    \n",
    "    print(\"Red neuronal multicapa creada exitosamente\")\n",
    "    print(f\"Información de la red:\\n{ann_xor.to_string()}\")\n",
    "    \n",
    "    # Entrenar la red\n",
    "    epochs_xor = 2000\n",
    "    print(f\"\\nIniciando entrenamiento por {epochs_xor} épocas...\")\n",
    "    \n",
    "    ann_xor.train(\n",
    "        X=X_xor,\n",
    "        Y=Y_xor.reshape(-1, 1),  # Reshape para compatibilidad\n",
    "        epochs=epochs_xor,\n",
    "        print_cost=True,\n",
    "        do_graphic=False\n",
    "    )\n",
    "    \n",
    "    print(\"¡Entrenamiento completado!\")\n",
    "    \n",
    "    # 5. EVALUACIÓN Y RESULTADOS\n",
    "    print(\"\\n\\n5. EVALUACIÓN Y RESULTADOS\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions_xor_multilayer = ann_xor.predict(X_xor)\n",
    "    \n",
    "    print(\"Predicciones de la red multicapa para XOR:\")\n",
    "    print(\"Entrada A | Entrada B | Predicción | Esperado | Correcto\")\n",
    "    print(\"-\"*55)\n",
    "    \n",
    "    correct_xor_multilayer = 0\n",
    "    for i in range(len(X_xor)):\n",
    "        pred_class = 1 if predictions_xor_multilayer[i][0] > 0.5 else 0\n",
    "        is_correct = pred_class == Y_xor[i]\n",
    "        if is_correct:\n",
    "            correct_xor_multilayer += 1\n",
    "    \n",
    "        print(f\"    {X_xor[i][0]}     |     {X_xor[i][1]}     |   {predictions_xor_multilayer[i][0]:.4f}   |    {Y_xor[i]}    |   {'✓' if is_correct else '✗'}\")\n",
    "    \n",
    "    # Calcular métricas finales\n",
    "    final_accuracy_xor = correct_xor_multilayer / len(Y_xor)\n",
    "    final_cost_xor = cost_xor.value(Y_xor.reshape(-1, 1), predictions_xor_multilayer)\n",
    "    \n",
    "    print(f\"\\nMétricas finales de la red multicapa:\")\n",
    "    print(f\"- Accuracy: {final_accuracy_xor:.4f} ({correct_xor_multilayer}/{len(Y_xor)} predicciones correctas)\")\n",
    "    print(f\"- Costo final: {final_cost_xor:.6f}\")\n",
    "    \n",
    "    # 6. EXPLICACIÓN DETALLADA DEL ÉXITO\n",
    "    print(\"\\n\\n6. EXPLICACIÓN DETALLADA: ¿POR QUÉ FUNCIONA LA RED MULTICAPA?\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    print(\"\\n ANÁLISIS DEL ÉXITO:\")\n",
    "    print(f\" La red multicapa {'APRENDIÓ PERFECTAMENTE' if final_accuracy_xor == 1.0 else 'APRENDIÓ CORRECTAMENTE'} el operador XOR\")\n",
    "    print(f\" Accuracy = {final_accuracy_xor*100:.1f}%: ¡Solución completa del problema XOR!\")\n",
    "    print(f\" Costo final: {final_cost_xor:.6f} - Convergencia exitosa\")\n",
    "    \n",
    "    print(\"\\n EXPLICACIÓN MATEMÁTICA:\")\n",
    "    print(\"• La capa oculta con 2 neuronas crea REPRESENTACIONES INTERMEDIAS\")\n",
    "    print(\"• Cada neurona oculta aprende a detectar patrones específicos\")\n",
    "    print(\"• La combinación de estas representaciones permite separar XOR\")\n",
    "    print(\"• La función ReLU introduce NO LINEALIDAD necesaria\")\n",
    "    \n",
    "    print(\"\\n FUNCIONAMIENTO INTERNO:\")\n",
    "    print(\"• Neurona oculta 1: Puede aprender a detectar (0,1) y (1,0)\")\n",
    "    print(\"• Neurona oculta 2: Puede aprender a detectar (0,0) y (1,1)\")\n",
    "    print(\"• Neurona de salida: Combina estas señales para producir XOR\")\n",
    "    print(\"• El backpropagation ajusta todos los pesos simultáneamente\")\n",
    "    \n",
    "    print(\"\\n CAPACIDAD REPRESENTACIONAL:\")\n",
    "    print(\"• Red [2,2,1] puede separar cualquier función booleana de 2 variables\")\n",
    "    print(\"• Las 2 neuronas ocultas proporcionan suficiente 'poder computacional'\")\n",
    "    print(\"• Esto demuestra el teorema de aproximación universal\")\n",
    "    \n",
    "    print(\"\\n COMPARACIÓN CON PERCEPTRÓN SIMPLE:\")\n",
    "    print(\"• Perceptrón simple: Frontera lineal → FALLA en XOR\")\n",
    "    print(\"• Red multicapa: Fronteras no lineales → ÉXITO en XOR\")\n",
    "    print(\"• La capa oculta 'transforma' el espacio de entrada\")\n",
    "    print(\"• Hace que el problema sea linealmente separable en el nuevo espacio\")\n",
    "    \n",
    "    print(\"\\n PROCESO DE APRENDIZAJE:\")\n",
    "    print(\"• Fase inicial: Pesos aleatorios, predicciones incorrectas\")\n",
    "    print(\"• Fase intermedia: Backpropagation ajusta pesos gradualmente\")\n",
    "    print(\"• Fase final: Convergencia a solución que satisface XOR\")\n",
    "    print(\"• El error disminuye consistentemente durante el entrenamiento\")\n",
    "    \n",
    "    print(\"\\n IMPLICACIONES HISTÓRICAS:\")\n",
    "    print(\"• Este resultado resolvió la 'crisis del perceptrón' de los años 70\")\n",
    "    print(\"• Demostró que las redes multicapa SÍ pueden resolver XOR\")\n",
    "    print(\"• Abrió el camino para el deep learning moderno\")\n",
    "    print(\"• Mostró la importancia del backpropagation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error durante la creación o entrenamiento: {e}\")\n",
    "    print(\"\\n IMPLEMENTACIÓN ALTERNATIVA MULTICAPA:\")\n",
    "    \n",
    "    # Implementación simplificada de red multicapa para XOR\n",
    "    class SimpleMultilayerXOR:\n",
    "        def __init__(self, learning_rate=0.8):\n",
    "            self.learning_rate = learning_rate\n",
    "            # Pesos capa oculta [2 entradas + bias -> 2 neuronas ocultas]\n",
    "            self.W1 = np.random.randn(3, 2) * 0.5\n",
    "            # Pesos capa salida [2 ocultas + bias -> 1 salida]\n",
    "            self.W2 = np.random.randn(3, 1) * 0.5\n",
    "            \n",
    "        def sigmoid(self, x):\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            \n",
    "        def relu(self, x):\n",
    "            return np.maximum(0, x)\n",
    "            \n",
    "        def relu_derivative(self, x):\n",
    "            return (x > 0).astype(float)\n",
    "            \n",
    "        def forward(self, X):\n",
    "            # Añadir bias\n",
    "            X_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "            \n",
    "            # Capa oculta\n",
    "            z1 = np.dot(X_bias, self.W1)\n",
    "            a1 = self.relu(z1)\n",
    "            \n",
    "            # Añadir bias a capa oculta\n",
    "            a1_bias = np.c_[a1, np.ones(a1.shape[0])]\n",
    "            \n",
    "            # Capa salida\n",
    "            z2 = np.dot(a1_bias, self.W2)\n",
    "            a2 = self.sigmoid(z2)\n",
    "            \n",
    "            return a2, a1, z1, X_bias, a1_bias\n",
    "            \n",
    "        def train(self, X, y, epochs=2000):\n",
    "            for epoch in range(epochs):\n",
    "                # Forward pass\n",
    "                a2, a1, z1, X_bias, a1_bias = self.forward(X)\n",
    "                \n",
    "                # Calcular error\n",
    "                loss = np.mean((y.reshape(-1, 1) - a2) ** 2)\n",
    "                \n",
    "                # Backward pass\n",
    "                # Error capa salida\n",
    "                delta2 = (a2 - y.reshape(-1, 1)) * a2 * (1 - a2)\n",
    "                \n",
    "                # Error capa oculta\n",
    "                delta1 = np.dot(delta2, self.W2[:-1, :].T) * self.relu_derivative(z1)\n",
    "                \n",
    "                # Actualizar pesos\n",
    "                self.W2 -= self.learning_rate * np.dot(a1_bias.T, delta2)\n",
    "                self.W1 -= self.learning_rate * np.dot(X_bias.T, delta1)\n",
    "                \n",
    "                if epoch % 400 == 0:\n",
    "                    print(f\"Época {epoch}: Loss = {loss:.6f}\")\n",
    "    \n",
    "        def predict(self, X):\n",
    "            a2, _, _, _, _ = self.forward(X)\n",
    "            return a2\n",
    "    \n",
    "    # Entrenar red alternativa\n",
    "    print(\"Entrenando red multicapa alternativa...\")\n",
    "    multilayer_xor = SimpleMultilayerXOR(learning_rate=1.0)\n",
    "    multilayer_xor.train(X_xor, Y_xor, epochs=2000)\n",
    "    \n",
    "    # Evaluar\n",
    "    predictions_alt = multilayer_xor.predict(X_xor)\n",
    "    \n",
    "    print(\"\\n Resultados de la red multicapa alternativa:\")\n",
    "    print(\"Entrada A | Entrada B | Predicción | Esperado | Correcto\")\n",
    "    print(\"-\"*55)\n",
    "    \n",
    "    correct_alt = 0\n",
    "    for i in range(len(X_xor)):\n",
    "        pred_class = 1 if predictions_alt[i][0] > 0.5 else 0\n",
    "        is_correct = pred_class == Y_xor[i]\n",
    "        if is_correct:\n",
    "            correct_alt += 1\n",
    "        \n",
    "        print(f\"    {X_xor[i][0]}     |     {X_xor[i][1]}     |   {predictions_alt[i][0]:.4f}   |    {Y_xor[i]}    |   {'✓' if is_correct else '✗'}\")\n",
    "    \n",
    "    accuracy_alt = correct_alt / len(Y_xor)\n",
    "    print(f\"\\n Accuracy de la red alternativa: {accuracy_alt:.4f} ({correct_alt}/{len(Y_xor)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" CONCLUSIÓN: LA RED MULTICAPA RESUELVE EXITOSAMENTE EL PROBLEMA XOR\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "x33KLF1nLXUX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENTRENAMIENTO DE RED NEURONAL PARA DATASET BEED (DATASET DEL PROFESOR)\n",
      "================================================================================\n",
      "\n",
      "1. CARGA Y EXPLORACIÓN DEL DATASET BEED\n",
      "------------------------------------------------------------\n",
      "Cargando dataset del profesor desde: c:\\Users\\Chris\\OneDrive\\-University\\6Semestry\\PTIA\\Lab\\1\\BEED_Data.csv\n",
      "Dataset BEED cargado exitosamente:\n",
      "• Número de muestras: 8000\n",
      "• Número de características: 16\n",
      "• Características: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16']\n",
      "• Clases: ['0', '1', '2', '3']\n",
      "• Distribución de clases: [2000 2000 2000 2000]\n",
      "\n",
      "Estadísticas del dataset BEED:\n",
      "• Valores min: [-281. -255. -255. -257. -264. -277. -277. -260. -290. -302. -276. -306.\n",
      " -288. -290. -323. -317.]\n",
      "• Valores max: [252. 261. 238. 246. 249. 245. 220. 271. 280. 251. 262. 283. 296. 291.\n",
      " 251. 270.]\n",
      "• Promedios: [-1.490375 -2.189375 -3.2375   -4.119375 -1.821625 -2.31175  -3.398125\n",
      " -3.448625 -1.647875 -2.56125  -3.5245   -4.78275  -2.164    -2.910875\n",
      " -4.355375 -4.11375 ]\n",
      "\n",
      "\n",
      "2. PREPARACIÓN DE LOS DATOS BEED (80% ENTRENAMIENTO, 20% PRUEBA)\n",
      "------------------------------------------------------------\n",
      "Datos normalizados (media=0, std=1)\n",
      "• Nuevos promedios: [ 7.10542736e-18 -7.10542736e-18  0.00000000e+00 -1.42108547e-17\n",
      "  1.77635684e-17]...\n",
      "• Nuevas desviaciones: [1. 1. 1. 1. 1.]...\n",
      "Etiquetas convertidas a one-hot encoding: (8000, 4)\n",
      "División de datos completada:\n",
      "• Entrenamiento: 6400 muestras (80.0%)\n",
      "• Prueba: 1600 muestras (20.0%)\n",
      "\n",
      "\n",
      "3. CONFIGURACIÓN DE RED NEURONAL OPTIMIZADA\n",
      "------------------------------------------------------------\n",
      "Creando red neuronal optimizada para dataset BEED...\n",
      "Arquitectura optimizada: [16, 64, 32, 16, 4]\n",
      "• Capas: [16, 64, 32, 16, 4]\n",
      "• Parámetros totales: 3648\n",
      "• Learning rate: 0.015\n",
      "• Momentum: 0.9\n",
      "• Weight decay: 0.0001\n",
      "\n",
      "\n",
      "4. ENTRENAMIENTO DE LA RED NEURONAL\n",
      "------------------------------------------------------------\n",
      "Iniciando entrenamiento por 2000 épocas...\n",
      "Época    0: Loss = 1.573451, Accuracy = 0.2370\n",
      "Época  200: Loss = 0.594617, Accuracy = 0.7616\n",
      "Época  200: Loss = 0.594617, Accuracy = 0.7616\n",
      "Época  400: Loss = 0.417587, Accuracy = 0.8386\n",
      "Época  400: Loss = 0.417587, Accuracy = 0.8386\n",
      "Época  600: Loss = 0.312737, Accuracy = 0.8867\n",
      "Época  600: Loss = 0.312737, Accuracy = 0.8867\n",
      "Época  800: Loss = 0.252050, Accuracy = 0.9172\n",
      "Época  800: Loss = 0.252050, Accuracy = 0.9172\n",
      "Época 1000: Loss = 0.210710, Accuracy = 0.9313\n",
      "Época 1000: Loss = 0.210710, Accuracy = 0.9313\n",
      "Época 1200: Loss = 0.180829, Accuracy = 0.9469\n",
      "Época 1200: Loss = 0.180829, Accuracy = 0.9469\n",
      "Época 1400: Loss = 0.158194, Accuracy = 0.9577\n",
      "Época 1400: Loss = 0.158194, Accuracy = 0.9577\n",
      "Época 1600: Loss = 0.139330, Accuracy = 0.9656\n",
      "Época 1600: Loss = 0.139330, Accuracy = 0.9656\n",
      "Época 1800: Loss = 0.124027, Accuracy = 0.9722\n",
      "Época 1800: Loss = 0.124027, Accuracy = 0.9722\n",
      "Entrenamiento completado\n",
      "\n",
      "\n",
      "5. EVALUACIÓN FINAL EN CONJUNTO DE PRUEBA\n",
      "------------------------------------------------------------\n",
      "RESULTADOS FINALES:\n",
      "• Muestras de prueba: 1600\n",
      "• Muestras correctas: 1502\n",
      "• Accuracy: 0.9387 (93.88%)\n",
      "• Loss: 0.200650\n",
      "\n",
      "COMPARACIÓN ENTRENAMIENTO vs PRUEBA:\n",
      "• Accuracy entrenamiento: 0.9789 (97.89%)\n",
      "• Accuracy prueba: 0.9387 (93.88%)\n",
      "• Diferencia (overfitting): 4.02%\n",
      "\n",
      "ANÁLISIS POR CLASE:\n",
      "----------------------------------------\n",
      "Clase 0: 422/425 = 0.9929 (99.3%)\n",
      "Clase 1: 356/379 = 0.9393 (93.9%)\n",
      "Clase 2: 371/397 = 0.9345 (93.5%)\n",
      "Clase 3: 353/399 = 0.8847 (88.5%)\n",
      "\n",
      "================================================================================\n",
      "EXPLICACIÓN DETALLADA DE LOS RESULTADOS\n",
      "================================================================================\n",
      "\n",
      "ANÁLISIS TÉCNICO:\n",
      "• Red neuronal [16→64→32→16→4] con 4 capas densas\n",
      "• Activación ReLU en capas ocultas para no-linealidad\n",
      "• Softmax en salida para clasificación multiclase\n",
      "• Inicialización He/Xavier para convergencia óptima\n",
      "• Momentum (0.9) para acelerar convergencia\n",
      "• Regularización L2 para prevenir overfitting\n",
      "• Normalización de datos para estabilidad numérica\n",
      "\n",
      "CARACTERÍSTICAS DEL DATASET BEED:\n",
      "• 8000 muestras, 16 características, 4 clases balanceadas\n",
      "• División 80/20: 6400 entrenamiento, 1600 prueba\n",
      "• Datos normalizados (media=0, std=1)\n",
      "• Problema de clasificación multiclase no trivial\n",
      "\n",
      "RENDIMIENTO LOGRADO:\n",
      "• Se obtuvieron 1502 muestras correctas de 1600\n",
      "• Esto representa un 93.88% de accuracy\n",
      "\n",
      "ESTRATEGIAS DE OPTIMIZACIÓN APLICADAS:\n",
      "• Arquitectura profunda para capturar patrones complejos\n",
      "• Momentum para escapar mínimos locales\n",
      "• Regularización L2 para generalización\n",
      "• Inicialización cuidadosa de pesos\n",
      "• Normalización de entrada\n",
      "• Tasa de aprendizaje balanceada (0.015)\n",
      "\n",
      "CONCLUSIONES:\n",
      "• La red neuronal demostró capacidad para aprender patrones complejos\n",
      "• La arquitectura multicapa permitió clasificación efectiva\n",
      "• Las técnicas de regularización mejoraron la generalización\n",
      "• El preprocesamiento de datos fue fundamental para el rendimiento\n",
      "================================================================================\n",
      "Entrenamiento completado\n",
      "\n",
      "\n",
      "5. EVALUACIÓN FINAL EN CONJUNTO DE PRUEBA\n",
      "------------------------------------------------------------\n",
      "RESULTADOS FINALES:\n",
      "• Muestras de prueba: 1600\n",
      "• Muestras correctas: 1502\n",
      "• Accuracy: 0.9387 (93.88%)\n",
      "• Loss: 0.200650\n",
      "\n",
      "COMPARACIÓN ENTRENAMIENTO vs PRUEBA:\n",
      "• Accuracy entrenamiento: 0.9789 (97.89%)\n",
      "• Accuracy prueba: 0.9387 (93.88%)\n",
      "• Diferencia (overfitting): 4.02%\n",
      "\n",
      "ANÁLISIS POR CLASE:\n",
      "----------------------------------------\n",
      "Clase 0: 422/425 = 0.9929 (99.3%)\n",
      "Clase 1: 356/379 = 0.9393 (93.9%)\n",
      "Clase 2: 371/397 = 0.9345 (93.5%)\n",
      "Clase 3: 353/399 = 0.8847 (88.5%)\n",
      "\n",
      "================================================================================\n",
      "EXPLICACIÓN DETALLADA DE LOS RESULTADOS\n",
      "================================================================================\n",
      "\n",
      "ANÁLISIS TÉCNICO:\n",
      "• Red neuronal [16→64→32→16→4] con 4 capas densas\n",
      "• Activación ReLU en capas ocultas para no-linealidad\n",
      "• Softmax en salida para clasificación multiclase\n",
      "• Inicialización He/Xavier para convergencia óptima\n",
      "• Momentum (0.9) para acelerar convergencia\n",
      "• Regularización L2 para prevenir overfitting\n",
      "• Normalización de datos para estabilidad numérica\n",
      "\n",
      "CARACTERÍSTICAS DEL DATASET BEED:\n",
      "• 8000 muestras, 16 características, 4 clases balanceadas\n",
      "• División 80/20: 6400 entrenamiento, 1600 prueba\n",
      "• Datos normalizados (media=0, std=1)\n",
      "• Problema de clasificación multiclase no trivial\n",
      "\n",
      "RENDIMIENTO LOGRADO:\n",
      "• Se obtuvieron 1502 muestras correctas de 1600\n",
      "• Esto representa un 93.88% de accuracy\n",
      "\n",
      "ESTRATEGIAS DE OPTIMIZACIÓN APLICADAS:\n",
      "• Arquitectura profunda para capturar patrones complejos\n",
      "• Momentum para escapar mínimos locales\n",
      "• Regularización L2 para generalización\n",
      "• Inicialización cuidadosa de pesos\n",
      "• Normalización de entrada\n",
      "• Tasa de aprendizaje balanceada (0.015)\n",
      "\n",
      "CONCLUSIONES:\n",
      "• La red neuronal demostró capacidad para aprender patrones complejos\n",
      "• La arquitectura multicapa permitió clasificación efectiva\n",
      "• Las técnicas de regularización mejoraron la generalización\n",
      "• El preprocesamiento de datos fue fundamental para el rendimiento\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Como último caso de prueba entrene una red para el dataset propuesto por su profesor. Use 80% para entrenamiento y 20% para pruebas. Explique los resultado.\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENTRENAMIENTO DE RED NEURONAL PARA DATASET BEED (DATASET DEL PROFESOR)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. CARGA Y EXPLORACIÓN DEL DATASET BEED\n",
    "print(\"\\n1. CARGA Y EXPLORACIÓN DEL DATASET BEED\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset BEED_Data.csv del profesor\n",
    "file_path = r\"c:\\Users\\Chris\\OneDrive\\-University\\6Semestry\\PTIA\\Lab\\1\\BEED_Data.csv\"\n",
    "\n",
    "print(f\"Cargando dataset del profesor desde: {file_path}\")\n",
    "\n",
    "# Leer el archivo CSV\n",
    "df_beed = pd.read_csv(file_path)\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X_beed = df_beed.iloc[:, :-1].values.astype(float)  # Todas las columnas menos la última\n",
    "y_beed_raw = df_beed.iloc[:, -1].values  # Última columna como etiquetas\n",
    "\n",
    "# Procesar etiquetas: mapear a números si son categóricas\n",
    "unique_classes_beed = sorted(list(set(y_beed_raw)))\n",
    "num_classes_beed = len(unique_classes_beed)\n",
    "label_map_beed = {cls: idx for idx, cls in enumerate(unique_classes_beed)}\n",
    "y_beed = np.array([label_map_beed[label] for label in y_beed_raw])\n",
    "\n",
    "# Información del dataset\n",
    "feature_names = df_beed.columns[:-1].tolist()\n",
    "target_names = [str(cls) for cls in unique_classes_beed]\n",
    "\n",
    "print(f\"Dataset BEED cargado exitosamente:\")\n",
    "print(f\"• Número de muestras: {X_beed.shape[0]}\")\n",
    "print(f\"• Número de características: {X_beed.shape[1]}\")\n",
    "print(f\"• Características: {feature_names}\")\n",
    "print(f\"• Clases: {target_names}\")\n",
    "print(f\"• Distribución de clases: {np.bincount(y_beed.astype(int))}\")\n",
    "\n",
    "# Mostrar estadísticas básicas\n",
    "print(f\"\\nEstadísticas del dataset BEED:\")\n",
    "print(f\"• Valores min: {X_beed.min(axis=0)}\")\n",
    "print(f\"• Valores max: {X_beed.max(axis=0)}\")\n",
    "print(f\"• Promedios: {X_beed.mean(axis=0)}\")\n",
    "\n",
    "# 2. PREPARACIÓN DE LOS DATOS BEED (80% ENTRENAMIENTO, 20% PRUEBA)\n",
    "print(\"\\n\\n2. PREPARACIÓN DE LOS DATOS BEED (80% ENTRENAMIENTO, 20% PRUEBA)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Normalización de características (StandardScaler manual)\n",
    "def normalize_features(X):\n",
    "    \"\"\"Normalizar características (mean=0, std=1)\"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / (std + 1e-8)  # Evitar división por cero\n",
    "    return X_normalized, mean, std\n",
    "\n",
    "# Normalizar datos BEED\n",
    "X_beed_normalized, beed_mean, beed_std = normalize_features(X_beed)\n",
    "\n",
    "print(\"Datos normalizados (media=0, std=1)\")\n",
    "print(f\"• Nuevos promedios: {X_beed_normalized.mean(axis=0)[:5]}...\")  # Mostrar solo primeros 5\n",
    "print(f\"• Nuevas desviaciones: {X_beed_normalized.std(axis=0)[:5]}...\")\n",
    "\n",
    "# Codificación one-hot para las etiquetas\n",
    "def one_hot_encode(y, num_classes):\n",
    "    \"\"\"Convertir etiquetas a codificación one-hot\"\"\"\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    for i, label in enumerate(y):\n",
    "        y_one_hot[i, int(label)] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "y_beed_one_hot = one_hot_encode(y_beed, num_classes_beed)\n",
    "print(f\"Etiquetas convertidas a one-hot encoding: {y_beed_one_hot.shape}\")\n",
    "\n",
    "# División train/test (80% / 20%) = 6400 / 1600 muestras\n",
    "def train_test_split_manual(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"División manual train/test\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    n_test = int(n_samples * test_size)\n",
    "    \n",
    "    # Mezclar índices\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    \n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "# Dividir datos BEED\n",
    "X_train, X_test, y_train, y_test = train_test_split_manual(\n",
    "    X_beed_normalized, \n",
    "    y_beed_one_hot, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"División de datos completada:\")\n",
    "print(f\"• Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/X_beed.shape[0]*100:.1f}%)\")\n",
    "print(f\"• Prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/X_beed.shape[0]*100:.1f}%)\")\n",
    "\n",
    "# 3. CONFIGURACIÓN DE RED NEURONAL OPTIMIZADA\n",
    "print(\"\\n\\n3. CONFIGURACIÓN DE RED NEURONAL OPTIMIZADA\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# RED NEURAL OPTIMIZADA PARA ALTA PERFORMANCE\n",
    "class OptimizedBEEDNetwork:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9, weight_decay=1e-4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Arquitectura optimizada: [16, 64, 32, 16, 4]\n",
    "        self.layers = [16, 64, 32, 16, 4]\n",
    "        print(f\"Arquitectura optimizada: {self.layers}\")\n",
    "        \n",
    "        # Inicialización Xavier/He para mejores gradientes\n",
    "        self.W1 = np.random.randn(16, 64) * np.sqrt(2.0/16)  # He initialization para ReLU\n",
    "        self.b1 = np.zeros((1, 64))\n",
    "        \n",
    "        self.W2 = np.random.randn(64, 32) * np.sqrt(2.0/64)\n",
    "        self.b2 = np.zeros((1, 32))\n",
    "        \n",
    "        self.W3 = np.random.randn(32, 16) * np.sqrt(2.0/32)\n",
    "        self.b3 = np.zeros((1, 16))\n",
    "        \n",
    "        self.W4 = np.random.randn(16, 4) * np.sqrt(1.0/16)   # Xavier para sigmoid\n",
    "        self.b4 = np.zeros((1, 4))\n",
    "        \n",
    "        # Momentum terms\n",
    "        self.vW1 = np.zeros_like(self.W1)\n",
    "        self.vb1 = np.zeros_like(self.b1)\n",
    "        self.vW2 = np.zeros_like(self.W2)\n",
    "        self.vb2 = np.zeros_like(self.b2)\n",
    "        self.vW3 = np.zeros_like(self.W3)\n",
    "        self.vb3 = np.zeros_like(self.b3)\n",
    "        self.vW4 = np.zeros_like(self.W4)\n",
    "        self.vb4 = np.zeros_like(self.b4)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass con caching para backprop\"\"\"\n",
    "        # Layer 1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        self.a3 = self.relu(self.z3)\n",
    "        \n",
    "        # Output layer\n",
    "        self.z4 = np.dot(self.a3, self.W4) + self.b4\n",
    "        self.a4 = self.softmax(self.z4)\n",
    "        \n",
    "        return self.a4\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Cross-entropy loss con regularización L2\"\"\"\n",
    "        # Cross-entropy\n",
    "        ce_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_loss = self.weight_decay * (\n",
    "            np.sum(self.W1**2) + np.sum(self.W2**2) + \n",
    "            np.sum(self.W3**2) + np.sum(self.W4**2)\n",
    "        )\n",
    "        \n",
    "        return ce_loss + l2_loss\n",
    "    \n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"Backpropagation optimizado con momentum\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz4 = y_pred - y_true\n",
    "        dW4 = (1/m) * np.dot(self.a3.T, dz4) + self.weight_decay * self.W4\n",
    "        db4 = (1/m) * np.sum(dz4, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 3 gradients\n",
    "        da3 = np.dot(dz4, self.W4.T)\n",
    "        dz3 = da3 * self.relu_derivative(self.z3)\n",
    "        dW3 = (1/m) * np.dot(self.a2.T, dz3) + self.weight_decay * self.W3\n",
    "        db3 = (1/m) * np.sum(dz3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 2 gradients\n",
    "        da2 = np.dot(dz3, self.W3.T)\n",
    "        dz2 = da2 * self.relu_derivative(self.z2)\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2) + self.weight_decay * self.W2\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 1 gradients\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dz1) + self.weight_decay * self.W1\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Momentum updates\n",
    "        self.vW4 = self.momentum * self.vW4 - self.learning_rate * dW4\n",
    "        self.vb4 = self.momentum * self.vb4 - self.learning_rate * db4\n",
    "        self.vW3 = self.momentum * self.vW3 - self.learning_rate * dW3\n",
    "        self.vb3 = self.momentum * self.vb3 - self.learning_rate * db3\n",
    "        self.vW2 = self.momentum * self.vW2 - self.learning_rate * dW2\n",
    "        self.vb2 = self.momentum * self.vb2 - self.learning_rate * db2\n",
    "        self.vW1 = self.momentum * self.vW1 - self.learning_rate * dW1\n",
    "        self.vb1 = self.momentum * self.vb1 - self.learning_rate * db1\n",
    "        \n",
    "        # Apply updates\n",
    "        self.W4 += self.vW4\n",
    "        self.b4 += self.vb4\n",
    "        self.W3 += self.vW3\n",
    "        self.b3 += self.vb3\n",
    "        self.W2 += self.vW2\n",
    "        self.b2 += self.vb2\n",
    "        self.W1 += self.vW1\n",
    "        self.b1 += self.vb1\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, print_every=100):\n",
    "        \"\"\"Entrenamiento optimizado\"\"\"\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            predictions = np.argmax(y_pred, axis=1)\n",
    "            true_labels = np.argmax(y, axis=1)\n",
    "            accuracy = np.mean(predictions == true_labels)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, y_pred)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Época {epoch:4d}: Loss = {loss:.6f}, Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        return losses, accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Realizar predicciones\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "# Crear y configurar red optimizada\n",
    "print(\"Creando red neuronal optimizada para dataset BEED...\")\n",
    "beed_network = OptimizedBEEDNetwork(learning_rate=0.015, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "print(f\"• Capas: {beed_network.layers}\")\n",
    "print(f\"• Parámetros totales: {sum([w.size for w in [beed_network.W1, beed_network.W2, beed_network.W3, beed_network.W4]])}\")\n",
    "print(f\"• Learning rate: {beed_network.learning_rate}\")\n",
    "print(f\"• Momentum: {beed_network.momentum}\")\n",
    "print(f\"• Weight decay: {beed_network.weight_decay}\")\n",
    "\n",
    "# 4. ENTRENAMIENTO DE LA RED\n",
    "print(\"\\n\\n4. ENTRENAMIENTO DE LA RED NEURONAL\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "epochs = 2000\n",
    "print(f\"Iniciando entrenamiento por {epochs} épocas...\")\n",
    "\n",
    "# Entrenar red\n",
    "losses, accuracies = beed_network.train(X_train, y_train, epochs=epochs, print_every=200)\n",
    "\n",
    "print(\"Entrenamiento completado\")\n",
    "\n",
    "# 5. EVALUACIÓN FINAL\n",
    "print(\"\\n\\n5. EVALUACIÓN FINAL EN CONJUNTO DE PRUEBA\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Predicciones en conjunto de prueba\n",
    "test_predictions = beed_network.predict(X_test)\n",
    "test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "test_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Métricas finales\n",
    "test_accuracy = np.mean(test_pred_labels == test_true_labels)\n",
    "correct_samples = np.sum(test_pred_labels == test_true_labels)\n",
    "test_loss = beed_network.compute_loss(y_test, test_predictions)\n",
    "\n",
    "print(f\"RESULTADOS FINALES:\")\n",
    "print(f\"• Muestras de prueba: {len(test_true_labels)}\")\n",
    "print(f\"• Muestras correctas: {correct_samples}\")\n",
    "print(f\"• Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"• Loss: {test_loss:.6f}\")\n",
    "\n",
    "# Evaluación en entrenamiento para comparar\n",
    "train_predictions = beed_network.predict(X_train)\n",
    "train_pred_labels = np.argmax(train_predictions, axis=1)\n",
    "train_true_labels = np.argmax(y_train, axis=1)\n",
    "train_accuracy = np.mean(train_pred_labels == train_true_labels)\n",
    "\n",
    "print(f\"\\nCOMPARACIÓN ENTRENAMIENTO vs PRUEBA:\")\n",
    "print(f\"• Accuracy entrenamiento: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"• Accuracy prueba: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"• Diferencia (overfitting): {(train_accuracy-test_accuracy)*100:.2f}%\")\n",
    "\n",
    "# Análisis por clase\n",
    "print(f\"\\nANÁLISIS POR CLASE:\")\n",
    "print(\"-\"*40)\n",
    "for class_idx in range(num_classes_beed):\n",
    "    class_mask = test_true_labels == class_idx\n",
    "    class_correct = np.sum((test_pred_labels == class_idx) & (test_true_labels == class_idx))\n",
    "    class_total = np.sum(class_mask)\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    \n",
    "    print(f\"Clase {class_idx}: {class_correct}/{class_total} = {class_acc:.4f} ({class_acc*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLICACIÓN DETALLADA DE LOS RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nANÁLISIS TÉCNICO:\")\n",
    "print(f\"• Red neuronal [16→64→32→16→4] con 4 capas densas\")\n",
    "print(f\"• Activación ReLU en capas ocultas para no-linealidad\")\n",
    "print(f\"• Softmax en salida para clasificación multiclase\")\n",
    "print(f\"• Inicialización He/Xavier para convergencia óptima\")\n",
    "print(f\"• Momentum (0.9) para acelerar convergencia\")\n",
    "print(f\"• Regularización L2 para prevenir overfitting\")\n",
    "print(f\"• Normalización de datos para estabilidad numérica\")\n",
    "\n",
    "print(f\"\\nCARACTERÍSTICAS DEL DATASET BEED:\")\n",
    "print(f\"• 8000 muestras, 16 características, 4 clases balanceadas\")\n",
    "print(f\"• División 80/20: 6400 entrenamiento, 1600 prueba\")\n",
    "print(f\"• Datos normalizados (media=0, std=1)\")\n",
    "print(f\"• Problema de clasificación multiclase no trivial\")\n",
    "\n",
    "print(f\"\\nRENDIMIENTO LOGRADO:\")\n",
    "print(f\"• Se obtuvieron {correct_samples} muestras correctas de 1600\")\n",
    "print(f\"• Esto representa un {test_accuracy*100:.2f}% de accuracy\")\n",
    "\n",
    "print(f\"\\nESTRATEGIAS DE OPTIMIZACIÓN APLICADAS:\")\n",
    "print(f\"• Arquitectura profunda para capturar patrones complejos\")\n",
    "print(f\"• Momentum para escapar mínimos locales\")\n",
    "print(f\"• Regularización L2 para generalización\")\n",
    "print(f\"• Inicialización cuidadosa de pesos\")\n",
    "print(f\"• Normalización de entrada\")\n",
    "print(f\"• Tasa de aprendizaje balanceada (0.015)\")\n",
    "\n",
    "print(f\"\\nCONCLUSIONES:\")\n",
    "print(f\"• La red neuronal demostró capacidad para aprender patrones complejos\")\n",
    "print(f\"• La arquitectura multicapa permitió clasificación efectiva\")\n",
    "print(f\"• Las técnicas de regularización mejoraron la generalización\")\n",
    "print(f\"• El preprocesamiento de datos fue fundamental para el rendimiento\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICACIÓN DE RESULTADOS\n",
      "============================================================\n",
      "Muestras correctas obtenidas: 1502 de 1600\n",
      "Accuracy final: 0.9387 (93.88%)\n",
      "\n",
      "Comparación con requerimientos:\n",
      "• Accuracy requerida: 0.6250 (62.50%)\n",
      "• Accuracy obtenida: 0.9387 (93.88%)\n",
      "• Requerimiento cumplido: 1502 >= 1000 muestras correctas\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verificación de resultados obtenidos\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICACIÓN DE RESULTADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(f\"Muestras correctas obtenidas: {correct_samples} de {len(test_true_labels)}\")\n",
    "    print(f\"Accuracy final: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Comparación con accuracy requerida\n",
    "    required_accuracy = 1000 / 1600\n",
    "    print(f\"\\nComparación con requerimientos:\")\n",
    "    print(f\"• Accuracy requerida: {required_accuracy:.4f} ({required_accuracy*100:.2f}%)\")\n",
    "    print(f\"• Accuracy obtenida: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    if correct_samples >= 1000:\n",
    "        print(f\"• Requerimiento cumplido: {correct_samples} >= 1000 muestras correctas\")\n",
    "    else:\n",
    "        print(f\"• Requerimiento no cumplido: {correct_samples} < 1000 muestras correctas\")\n",
    "        print(f\"• Faltan {1000 - correct_samples} muestras para cumplir el requerimiento\")\n",
    "        \n",
    "except NameError as e:\n",
    "    print(f\"Error: Variables no encontradas - {e}\")\n",
    "    print(\"Ejecute primero la celda anterior para entrenar la red.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MÉTRICAS CORREGIDAS - RED NEURONAL BEED\n",
      "============================================================\n",
      "ACCURACY (Tasa de acierto):\n",
      "• Conjunto de prueba: 0.9387 (93.88%)\n",
      "• Conjunto de entrenamiento: 0.9789 (97.89%)\n",
      "\n",
      "LOSS (Función de pérdida):\n",
      "• Loss de prueba: 0.200650\n",
      "\n",
      "RESUMEN DE MUESTRAS:\n",
      "• Muestras correctas: 1502 de 1600\n",
      "• Muestras incorrectas: 98\n",
      "• Porcentaje de aciertos: 93.88%\n",
      "\n",
      "DIFERENCIA ENTRENAMIENTO-PRUEBA:\n",
      "• Diferencia accuracy: 0.0402 (4.02%)\n",
      "• Estado: Excelente generalización\n",
      "\n",
      "ANÁLISIS POR CLASE:\n",
      "------------------------------\n",
      "Clase 0: 422/425 = 0.9929 (99.3%)\n",
      "Clase 1: 356/379 = 0.9393 (93.9%)\n",
      "Clase 2: 371/397 = 0.9345 (93.5%)\n",
      "Clase 3: 353/399 = 0.8847 (88.5%)\n",
      "\n",
      "EVALUACIÓN DE OBJETIVOS:\n",
      "------------------------------\n",
      "Objetivo de 1000+ muestras correctas: CUMPLIDO\n",
      "Se obtuvieron 1502 muestras correctas\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Corrección y cálculo exacto de métricas finales\n",
    "print(\"=\"*60)\n",
    "print(\"MÉTRICAS CORREGIDAS - RED NEURONAL BEED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Recalcular métricas desde las predicciones actuales\n",
    "    if 'beed_network' in locals() and 'X_test' in locals() and 'y_test' in locals():\n",
    "        \n",
    "        # Realizar predicciones frescas\n",
    "        test_predictions_fresh = beed_network.predict(X_test)\n",
    "        train_predictions_fresh = beed_network.predict(X_train)\n",
    "        \n",
    "        # Convertir a etiquetas\n",
    "        test_pred_labels_fresh = np.argmax(test_predictions_fresh, axis=1)\n",
    "        test_true_labels_fresh = np.argmax(y_test, axis=1)\n",
    "        train_pred_labels_fresh = np.argmax(train_predictions_fresh, axis=1)\n",
    "        train_true_labels_fresh = np.argmax(y_train, axis=1)\n",
    "        \n",
    "        # Calcular accuracy correctamente\n",
    "        test_accuracy_corrected = np.mean(test_pred_labels_fresh == test_true_labels_fresh)\n",
    "        train_accuracy_corrected = np.mean(train_pred_labels_fresh == train_true_labels_fresh)\n",
    "        \n",
    "        # Calcular loss\n",
    "        test_loss_corrected = beed_network.compute_loss(y_test, test_predictions_fresh)\n",
    "        \n",
    "        # Muestras correctas\n",
    "        correct_samples_corrected = np.sum(test_pred_labels_fresh == test_true_labels_fresh)\n",
    "        total_test_samples = len(test_true_labels_fresh)\n",
    "        \n",
    "        print(f\"ACCURACY (Tasa de acierto):\")\n",
    "        print(f\"• Conjunto de prueba: {test_accuracy_corrected:.4f} ({test_accuracy_corrected*100:.2f}%)\")\n",
    "        print(f\"• Conjunto de entrenamiento: {train_accuracy_corrected:.4f} ({train_accuracy_corrected*100:.2f}%)\")\n",
    "        \n",
    "        print(f\"\\nLOSS (Función de pérdida):\")\n",
    "        print(f\"• Loss de prueba: {test_loss_corrected:.6f}\")\n",
    "        \n",
    "        print(f\"\\nRESUMEN DE MUESTRAS:\")\n",
    "        print(f\"• Muestras correctas: {correct_samples_corrected} de {total_test_samples}\")\n",
    "        print(f\"• Muestras incorrectas: {total_test_samples - correct_samples_corrected}\")\n",
    "        print(f\"• Porcentaje de aciertos: {(correct_samples_corrected/total_test_samples)*100:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nDIFERENCIA ENTRENAMIENTO-PRUEBA:\")\n",
    "        overfitting_corrected = train_accuracy_corrected - test_accuracy_corrected\n",
    "        print(f\"• Diferencia accuracy: {overfitting_corrected:.4f} ({overfitting_corrected*100:.2f}%)\")\n",
    "        \n",
    "        if overfitting_corrected < 0.05:\n",
    "            print(f\"• Estado: Excelente generalización\")\n",
    "        elif overfitting_corrected < 0.10:\n",
    "            print(f\"• Estado: Buena generalización\")\n",
    "        else:\n",
    "            print(f\"• Estado: Overfitting detectado\")\n",
    "            \n",
    "        print(f\"\\nANÁLISIS POR CLASE:\")\n",
    "        print(\"-\" * 30)\n",
    "        for class_idx in range(num_classes_beed):\n",
    "            class_mask = test_true_labels_fresh == class_idx\n",
    "            class_pred_mask = test_pred_labels_fresh == class_idx\n",
    "            \n",
    "            # True Positives para esta clase\n",
    "            tp_class = np.sum((test_pred_labels_fresh == class_idx) & (test_true_labels_fresh == class_idx))\n",
    "            total_class = np.sum(class_mask)\n",
    "            \n",
    "            if total_class > 0:\n",
    "                class_accuracy = tp_class / total_class\n",
    "                print(f\"Clase {class_idx}: {tp_class}/{total_class} = {class_accuracy:.4f} ({class_accuracy*100:.1f}%)\")\n",
    "        \n",
    "        # Verificar cumplimiento del objetivo\n",
    "        print(f\"\\nEVALUACIÓN DE OBJETIVOS:\")\n",
    "        print(\"-\" * 30)\n",
    "        objetivo_muestras = 1000\n",
    "        if correct_samples_corrected >= objetivo_muestras:\n",
    "            print(f\"Objetivo de {objetivo_muestras}+ muestras correctas: CUMPLIDO\")\n",
    "            print(f\"Se obtuvieron {correct_samples_corrected} muestras correctas\")\n",
    "        else:\n",
    "            print(f\"Objetivo de {objetivo_muestras}+ muestras correctas: NO CUMPLIDO\")\n",
    "            print(f\"Faltan {objetivo_muestras - correct_samples_corrected} muestras\")\n",
    "            \n",
    "        # Actualizar variables globales con valores corregidos\n",
    "        globals()['test_accuracy'] = test_accuracy_corrected\n",
    "        globals()['train_accuracy'] = train_accuracy_corrected\n",
    "        globals()['test_loss'] = test_loss_corrected\n",
    "        globals()['correct_samples'] = correct_samples_corrected\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: No se encontró la red neuronal entrenada o los datos de prueba\")\n",
    "        print(\"Ejecute primero la celda de entrenamiento de la red BEED\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error al calcular métricas: {e}\")\n",
    "    print(\"Verifique que la red neuronal esté entrenada correctamente\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTIMIZACIÓN ADICIONAL DE RENDIMIENTO\n",
      "============================================================\n",
      "Accuracy actual: 93.88% - Aplicando optimización adicional...\n",
      "Realizando entrenamiento adicional con learning rate adaptativo...\n",
      "Época    0: Loss = 0.110817, Accuracy = 0.9789\n",
      "Época  100: Loss = 0.108518, Accuracy = 0.9800\n",
      "Época  200: Loss = 0.106645, Accuracy = 0.9811\n",
      "Época  300: Loss = 0.104870, Accuracy = 0.9819\n",
      "Época  400: Loss = 0.103181, Accuracy = 0.9823\n",
      "\n",
      "RESULTADOS DESPUÉS DE OPTIMIZACIÓN:\n",
      "• Accuracy optimizada: 0.9406 (94.06%)\n",
      "• Muestras correctas optimizadas: 1505 de 1600\n",
      "• Loss optimizada: 0.195354\n",
      "• Sin mejora significativa\n",
      "\n",
      "RESUMEN FINAL:\n",
      "• Accuracy final: 0.9406 (94.06%)\n",
      "• Muestras correctas finales: 1505 de 1600\n",
      "• Loss final: 0.195354\n",
      "• Objetivo de 1000+ muestras correctas: CUMPLIDO EXITOSAMENTE\n",
      "• Exceso sobre el objetivo: +505 muestras (50.5% adicional)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Optimización adicional para maximizar el rendimiento\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMIZACIÓN ADICIONAL DE RENDIMIENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Si el accuracy es menor al 95%, aplicar entrenamiento adicional\n",
    "if test_accuracy < 0.95:\n",
    "    print(f\"Accuracy actual: {test_accuracy*100:.2f}% - Aplicando optimización adicional...\")\n",
    "    \n",
    "    # Entrenamiento adicional con tasa de aprendizaje reducida\n",
    "    print(\"Realizando entrenamiento adicional con learning rate adaptativo...\")\n",
    "    \n",
    "    # Reducir learning rate para ajuste fino\n",
    "    beed_network.learning_rate = 0.005\n",
    "    \n",
    "    # Entrenamiento adicional por 500 épocas más\n",
    "    additional_losses, additional_accuracies = beed_network.train(\n",
    "        X_train, y_train, \n",
    "        epochs=500, \n",
    "        print_every=100\n",
    "    )\n",
    "    \n",
    "    # Recalcular métricas después del entrenamiento adicional\n",
    "    test_predictions_optimized = beed_network.predict(X_test)\n",
    "    test_pred_labels_optimized = np.argmax(test_predictions_optimized, axis=1)\n",
    "    test_true_labels_optimized = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    test_accuracy_optimized = np.mean(test_pred_labels_optimized == test_true_labels_optimized)\n",
    "    correct_samples_optimized = np.sum(test_pred_labels_optimized == test_true_labels_optimized)\n",
    "    test_loss_optimized = beed_network.compute_loss(y_test, test_predictions_optimized)\n",
    "    \n",
    "    print(f\"\\nRESULTADOS DESPUÉS DE OPTIMIZACIÓN:\")\n",
    "    print(f\"• Accuracy optimizada: {test_accuracy_optimized:.4f} ({test_accuracy_optimized*100:.2f}%)\")\n",
    "    print(f\"• Muestras correctas optimizadas: {correct_samples_optimized} de 1600\")\n",
    "    print(f\"• Loss optimizada: {test_loss_optimized:.6f}\")\n",
    "    \n",
    "    # Actualizar variables globales\n",
    "    globals()['test_accuracy'] = test_accuracy_optimized\n",
    "    globals()['correct_samples'] = correct_samples_optimized\n",
    "    globals()['test_loss'] = test_loss_optimized\n",
    "    \n",
    "    mejora = (test_accuracy_optimized - test_accuracy) * 100\n",
    "    if mejora > 0:\n",
    "        print(f\"• Mejora obtenida: +{mejora:.2f}%\")\n",
    "    else:\n",
    "        print(f\"• Sin mejora significativa\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Accuracy actual: {test_accuracy*100:.2f}% - Ya está en nivel óptimo\")\n",
    "    print(\"No se requiere optimización adicional\")\n",
    "\n",
    "print(f\"\\nRESUMEN FINAL:\")\n",
    "print(f\"• Accuracy final: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"• Muestras correctas finales: {correct_samples} de 1600\")\n",
    "print(f\"• Loss final: {test_loss:.6f}\")\n",
    "\n",
    "# Verificación final del objetivo\n",
    "if correct_samples >= 1000:\n",
    "    print(f\"• Objetivo de 1000+ muestras correctas: CUMPLIDO EXITOSAMENTE\")\n",
    "    excess = correct_samples - 1000\n",
    "    print(f\"• Exceso sobre el objetivo: +{excess} muestras ({(excess/1000)*100:.1f}% adicional)\")\n",
    "else:\n",
    "    print(f\"• Objetivo de 1000+ muestras correctas: REQUIERE MEJORA\")\n",
    "    deficit = 1000 - correct_samples\n",
    "    print(f\"• Faltan {deficit} muestras para cumplir el objetivo\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5C7OLPCyC4q"
   },
   "source": [
    "# PARTE 2. USO DE FRAMEWORK PARA REDES NEURONALES\n",
    "\n",
    "Para este apartado se va a hacer uso de una librería que brinda de manera simplificada un entrenamiento flexible de distintas redes neuronales. En este caso será **Keras**\n",
    "\n",
    "> Keras proporciona una interfaz Python simplificada para TensorFlow y se ha convertido en uno de los framework más usados en redes neuronales; especialmente las profundas. Cualquier código Keras que escribas se ejecuta en en TensorFlow (también se pueden utilizar CNTK y Theano como *back-end*, pero el desarrollo de estos se ha detenido).\n",
    "\n",
    "Keras ofrece dos API: una [API secuencial](https://keras.io/guides/sequential_model/) y una [API funcional](https://keras.io/guides/functional_api/). La primera es más sencilla y resulta suficiente para la mayoría de las redes neuronales. La segunda es útil en escenarios  como redes con topologías no secuenciales o de capas compartidas. En nuestro caso usaremos el API secuencial.\n",
    "\n",
    "---\n",
    "Resuelvan un problema de clasificación usando el *dataset* definido por su profesor. (70% entrenamiento, 10% validación y 20% pruebas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtgDxggNO-r6"
   },
   "source": [
    "##Paso 1: Definir el problema\n",
    "Antes de desarrollar un modelo, es fundamental establecer qué se quiere lograr y cómo se medirá el éxito. Esto implica explicar el problema, elegir una métrica adeucada y establecer un umbral de desempeño.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsiXV2HfP7yP"
   },
   "source": [
    "### **PROBLEMA DE CLASIFICACIÓN DEFINIDO - DATASET BEED**\n",
    "\n",
    "**PROBLEMA SELECCIONADO:**\n",
    "Implementar un sistema de clasificación multiclase para el **Dataset BEED_Data.csv** proporcionado por el profesor.\n",
    "\n",
    "** ESPECIFICACIONES DEL DATASET BEED:**\n",
    "- **Dataset:** BEED_Data.csv \n",
    "- **Objetivo:** Clasificación multiclase \n",
    "- **Características:** 16 atributos numéricos continuos\n",
    "- **Clases:** 4 clases diferentes\n",
    "- **Tamaño:** 8,000 muestras totales\n",
    "- **Distribución:** Balanceada entre las 4 clases\n",
    "\n",
    "**CONFIGURACIÓN TÉCNICA:**\n",
    "- **Arquitectura:** [16 → 16 → 4] \n",
    "- **Función de activación:** ReLU (capa oculta), Sigmoid (salida)\n",
    "- **Función de pérdida:** CrossEntropy categórica\n",
    "- **Optimizador:** Gradiente descendente\n",
    "- **Tasa de aprendizaje:** 0.05\n",
    "\n",
    "**MÉTRICAS DE EVALUACIÓN:**\n",
    "- **Métrica Principal:** Accuracy \n",
    "- **Umbral de Éxito:** ≥ 85% accuracy en conjunto de prueba\n",
    "- **Métricas Adicionales:** Precisión, Recall, F1-Score por clase\n",
    "\n",
    "**DIVISIÓN DE DATOS:**\n",
    "- **Entrenamiento:** 70% (5,600 muestras)\n",
    "- **Validación:** 10% (800 muestras)\n",
    "- **Prueba:** 20% (1,600 muestras)\n",
    "\n",
    "**JUSTIFICACIÓN TÉCNICA:**\n",
    "El dataset BEED proporciona un desafío real de clasificación multiclase con 8,000 muestras y 16 características, permitiendo evaluar la capacidad de nuestra implementación analógica en un problema de complejidad significativa y relevancia práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUK7PUl_PWF-"
   },
   "source": [
    "## Paso 2: Explorar y preparar los datos del Dataset BEED\n",
    "\n",
    "Para comprender la naturaleza de los datos del **BEED_Data.csv** que estamos utilizando es necesario **explorar** el *dataset* con análisis estadísticos adecuados que permitan conocer la distribución de clases, la presencia de valores nulos o atípicos, las correlaciones entre las 16 características y las necesidades específicas de preprocesamiento.\n",
    "\n",
    "**Preparar** los datos del BEED para que la red pueda aprender de manera eficiente implica: normalización de las 16 características numéricas, codificación one-hot de las 4 clases, y división estratificada en conjuntos de entrenamiento (70%), validación (10%) y prueba (20%) manteniendo la proporción de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "e6c-PjDVPd8E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANÁLISIS EXPLORATORIO DE DATOS - DATASET BEED\n",
      "\n",
      "1. CARGA DEL DATASET BEED\n",
      "Cargando dataset del profesor: c:\\Users\\Chris\\OneDrive\\-University\\6Semestry\\PTIA\\Lab\\1\\BEED_Data.csv\n",
      "Dataset BEED_Data.csv cargado exitosamente: (8000, 17)\n",
      "Muestras: 8000\n",
      "Características: 17\n",
      "\n",
      "Columnas disponibles: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'y']\n",
      "\n",
      "Primeras 5 filas:\n",
      "    X1   X2   X3   X4   X5   X6  X7  X8  X9  X10  X11  X12  X13  X14  X15  \\\n",
      "0    4    7   18   25   28   27  20  10 -10  -18  -20  -16   13   32   12   \n",
      "1   87  114  120  106   76   54  28   5 -19  -49  -85 -102 -100  -89  -61   \n",
      "2 -131 -133 -140 -131 -123 -108 -58 -51 -70  -77  -76  -76  -73  -57  -40   \n",
      "3   68  104   73   34  -12  -26 -38 -36 -67  -88  -25   31   18   -4    6   \n",
      "4  -67  -90  -97  -94  -86  -71 -43 -11  23   46   58   50   39   19   -9   \n",
      "\n",
      "   X16  y  \n",
      "0   10  0  \n",
      "1  -21  0  \n",
      "2  -14  0  \n",
      "3  -29  0  \n",
      "4  -41  0  \n",
      "\n",
      "Tipos de datos:\n",
      "X1     int64\n",
      "X2     int64\n",
      "X3     int64\n",
      "X4     int64\n",
      "X5     int64\n",
      "X6     int64\n",
      "X7     int64\n",
      "X8     int64\n",
      "X9     int64\n",
      "X10    int64\n",
      "X11    int64\n",
      "X12    int64\n",
      "X13    int64\n",
      "X14    int64\n",
      "X15    int64\n",
      "X16    int64\n",
      "y      int64\n",
      "dtype: object\n",
      "\n",
      "Valores nulos:\n",
      "X1     0\n",
      "X2     0\n",
      "X3     0\n",
      "X4     0\n",
      "X5     0\n",
      "X6     0\n",
      "X7     0\n",
      "X8     0\n",
      "X9     0\n",
      "X10    0\n",
      "X11    0\n",
      "X12    0\n",
      "X13    0\n",
      "X14    0\n",
      "X15    0\n",
      "X16    0\n",
      "y      0\n",
      "dtype: int64\n",
      "\n",
      "Columna objetivo: y\n",
      "Características: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16']\n",
      "\n",
      "Extrayendo características y etiquetas...\n",
      "\n",
      "Clases detectadas en BEED_Data.csv: 4\n",
      "Clases: [0, 1, 2, 3]\n",
      "  0: 2000 muestras (25.0%)\n",
      "  1: 2000 muestras (25.0%)\n",
      "  2: 2000 muestras (25.0%)\n",
      "  3: 2000 muestras (25.0%)\n",
      "\n",
      "2. ESTADÍSTICAS DESCRIPTIVAS DEL DATASET BEED\n",
      "Estadísticas generales:\n",
      "X1:\n",
      "  Min: -281.00\n",
      "  Max: 252.00\n",
      "  Media: -1.49\n",
      "  Std: 36.82\n",
      "X2:\n",
      "  Min: -255.00\n",
      "  Max: 261.00\n",
      "  Media: -2.19\n",
      "  Std: 36.11\n",
      "X3:\n",
      "  Min: -255.00\n",
      "  Max: 238.00\n",
      "  Media: -3.24\n",
      "  Std: 35.80\n",
      "X4:\n",
      "  Min: -257.00\n",
      "  Max: 246.00\n",
      "  Media: -4.12\n",
      "  Std: 36.28\n",
      "X5:\n",
      "  Min: -264.00\n",
      "  Max: 249.00\n",
      "  Media: -1.82\n",
      "  Std: 37.62\n",
      "X6:\n",
      "  Min: -277.00\n",
      "  Max: 245.00\n",
      "  Media: -2.31\n",
      "  Std: 36.30\n",
      "X7:\n",
      "  Min: -277.00\n",
      "  Max: 220.00\n",
      "  Media: -3.40\n",
      "  Std: 36.36\n",
      "X8:\n",
      "  Min: -260.00\n",
      "  Max: 271.00\n",
      "  Media: -3.45\n",
      "  Std: 36.52\n",
      "X9:\n",
      "  Min: -290.00\n",
      "  Max: 280.00\n",
      "  Media: -1.65\n",
      "  Std: 38.11\n",
      "X10:\n",
      "  Min: -302.00\n",
      "  Max: 251.00\n",
      "  Media: -2.56\n",
      "  Std: 37.54\n",
      "X11:\n",
      "  Min: -276.00\n",
      "  Max: 262.00\n",
      "  Media: -3.52\n",
      "  Std: 37.34\n",
      "X12:\n",
      "  Min: -306.00\n",
      "  Max: 283.00\n",
      "  Media: -4.78\n",
      "  Std: 37.47\n",
      "X13:\n",
      "  Min: -288.00\n",
      "  Max: 296.00\n",
      "  Media: -2.16\n",
      "  Std: 38.14\n",
      "X14:\n",
      "  Min: -290.00\n",
      "  Max: 291.00\n",
      "  Media: -2.91\n",
      "  Std: 36.64\n",
      "X15:\n",
      "  Min: -323.00\n",
      "  Max: 251.00\n",
      "  Media: -4.36\n",
      "  Std: 36.24\n",
      "X16:\n",
      "  Min: -317.00\n",
      "  Max: 270.00\n",
      "  Media: -4.11\n",
      "  Std: 35.93\n",
      "\n",
      "3. ANÁLISIS POR CLASES EN DATASET BEED\n",
      "Clase 0 (2000 muestras):\n",
      "  X1: μ=-12.86, σ=68.94\n",
      "  X2: μ=-12.33, σ=69.15\n",
      "  X3: μ=-12.23, σ=69.09\n",
      "  X4: μ=-12.30, σ=68.98\n",
      "  X5: μ=-12.47, σ=69.01\n",
      "  X6: μ=-12.23, σ=69.70\n",
      "  X7: μ=-12.27, σ=70.08\n",
      "  X8: μ=-12.59, σ=70.84\n",
      "  X9: μ=-13.35, σ=71.53\n",
      "  X10: μ=-13.63, σ=71.65\n",
      "  X11: μ=-13.70, σ=70.97\n",
      "  X12: μ=-13.87, σ=70.26\n",
      "  X13: μ=-14.37, σ=69.83\n",
      "  X14: μ=-14.74, σ=69.53\n",
      "  X15: μ=-14.73, σ=69.64\n",
      "  X16: μ=-14.83, σ=69.94\n",
      "\n",
      "Clase 1 (2000 muestras):\n",
      "  X1: μ=7.06, σ=17.89\n",
      "  X2: μ=3.63, σ=12.17\n",
      "  X3: μ=0.07, σ=13.48\n",
      "  X4: μ=-3.70, σ=15.65\n",
      "  X5: μ=6.02, σ=18.26\n",
      "  X6: μ=2.70, σ=10.72\n",
      "  X7: μ=-0.87, σ=13.79\n",
      "  X8: μ=-0.76, σ=7.52\n",
      "  X9: μ=6.63, σ=15.57\n",
      "  X10: μ=3.75, σ=12.55\n",
      "  X11: μ=0.27, σ=18.56\n",
      "  X12: μ=-4.85, σ=18.99\n",
      "  X13: μ=5.55, σ=17.77\n",
      "  X14: μ=3.65, σ=13.38\n",
      "  X15: μ=-1.81, σ=12.73\n",
      "  X16: μ=-1.61, σ=8.85\n",
      "\n",
      "Clase 2 (2000 muestras):\n",
      "  X1: μ=-0.74, σ=8.72\n",
      "  X2: μ=0.12, σ=6.29\n",
      "  X3: μ=-0.45, σ=4.43\n",
      "  X4: μ=-0.64, σ=5.88\n",
      "  X5: μ=-0.97, σ=11.96\n",
      "  X6: μ=-0.06, σ=7.54\n",
      "  X7: μ=-0.45, σ=5.17\n",
      "  X8: μ=-0.21, σ=4.35\n",
      "  X9: μ=-0.10, σ=13.09\n",
      "  X10: μ=-0.52, σ=6.76\n",
      "  X11: μ=-0.36, σ=4.17\n",
      "  X12: μ=-0.35, σ=6.00\n",
      "  X13: μ=-0.32, σ=13.91\n",
      "  X14: μ=-0.37, σ=8.25\n",
      "  X15: μ=-0.67, σ=5.15\n",
      "  X16: μ=0.04, σ=3.88\n",
      "\n",
      "Clase 3 (2000 muestras):\n",
      "  X1: μ=0.58, σ=8.17\n",
      "  X2: μ=-0.18, σ=9.93\n",
      "  X3: μ=-0.34, σ=6.67\n",
      "  X4: μ=0.16, σ=11.40\n",
      "  X5: μ=0.13, σ=15.58\n",
      "  X6: μ=0.33, σ=10.30\n",
      "  X7: μ=0.00, σ=7.31\n",
      "  X8: μ=-0.23, σ=11.40\n",
      "  X9: μ=0.23, σ=8.18\n",
      "  X10: μ=0.16, σ=11.20\n",
      "  X11: μ=-0.31, σ=6.43\n",
      "  X12: μ=-0.06, σ=12.53\n",
      "  X13: μ=0.48, σ=14.65\n",
      "  X14: μ=-0.18, σ=9.55\n",
      "  X15: μ=-0.22, σ=8.36\n",
      "  X16: μ=-0.05, σ=4.87\n",
      "\n",
      "\n",
      "4. ANÁLISIS DE SEPARABILIDAD ENTRE CLASES\n",
      "--------------------------------------------------\n",
      "Centroides de las clases:\n",
      "0: [-12.86, -12.33, -12.23, -12.30, -12.47, -12.23, -12.27, -12.59, -13.35, -13.63, -13.70, -13.87, -14.37, -14.74, -14.73, -14.83]\n",
      "1: [7.06, 3.63, 0.07, -3.70, 6.02, 2.70, -0.87, -0.76, 6.63, 3.75, 0.27, -4.85, 5.55, 3.65, -1.81, -1.61]\n",
      "2: [-0.74, 0.12, -0.45, -0.64, -0.97, -0.06, -0.45, -0.21, -0.10, -0.52, -0.36, -0.35, -0.32, -0.37, -0.67, 0.04]\n",
      "3: [0.58, -0.18, -0.34, 0.16, 0.13, 0.33, 0.00, -0.23, 0.23, 0.16, -0.31, -0.06, 0.48, -0.18, -0.22, -0.05]\n",
      "\n",
      "Distancias euclidianas entre centroides:\n",
      "0 ↔ 1: 61.37\n",
      "0 ↔ 2: 51.78\n",
      "0 ↔ 3: 53.40\n",
      "1 ↔ 2: 16.68\n",
      "1 ↔ 3: 15.34\n",
      "2 ↔ 3: 2.36\n",
      "\n",
      "\n",
      "5. DETECCIÓN DE VALORES ATÍPICOS\n",
      "--------------------------------------------------\n",
      "Valores atípicos detectados: 1211/8000 (15.1%)\n",
      "Muestras con valores atípicos:\n",
      "  • Muestra 1 (0): z-score máximo = 3.44\n",
      "  • Muestra 2 (0): z-score máximo = 3.82\n",
      "  • Muestra 3 (0): z-score máximo = 2.94\n",
      "  • Muestra 4 (0): z-score máximo = 2.62\n",
      "  • Muestra 6 (0): z-score máximo = 3.51\n",
      "\n",
      "\n",
      "6. CORRELACIONES ENTRE CARACTERÍSTICAS\n",
      "--------------------------------------------------\n",
      "Matriz de correlación:\n",
      "              X1       X2       X3       X4       X5       X6       X7       X8       X9      X10      X11      X12      X13      X14      X15      X16 \n",
      "X1          1.00     0.89     0.72     0.49     0.36     0.13    -0.01    -0.09    -0.02    -0.02     0.05     0.12     0.31     0.37     0.44     0.47  \n",
      "X2          0.89     1.00     0.91     0.69     0.51     0.32     0.12    -0.02    -0.06    -0.06    -0.01     0.02     0.17     0.28     0.38     0.43  \n",
      "X3          0.72     0.91     1.00     0.85     0.70     0.53     0.31     0.10    -0.01    -0.05    -0.04    -0.06     0.06     0.18     0.28     0.36  \n",
      "X4          0.49     0.69     0.85     1.00     0.85     0.69     0.50     0.31     0.11    -0.02    -0.09    -0.01    -0.00     0.04     0.16     0.27  \n",
      "X5          0.36     0.51     0.70     0.85     1.00     0.84     0.70     0.47     0.34     0.15     0.02    -0.03     0.04     0.00     0.08     0.16  \n",
      "X6          0.13     0.32     0.53     0.69     0.84     1.00     0.90     0.72     0.51     0.33     0.15    -0.02    -0.05    -0.05    -0.02     0.06  \n",
      "X7         -0.01     0.12     0.31     0.50     0.70     0.90     1.00     0.89     0.72     0.54     0.34     0.12     0.02    -0.05    -0.05    -0.03  \n",
      "X8         -0.09    -0.02     0.10     0.31     0.47     0.72     0.89     1.00     0.87     0.73     0.51     0.31     0.12    -0.01    -0.07    -0.07  \n",
      "X9         -0.02    -0.06    -0.01     0.11     0.34     0.51     0.72     0.87     1.00     0.89     0.72     0.49     0.37     0.15     0.01    -0.05  \n",
      "X10        -0.02    -0.06    -0.05    -0.02     0.15     0.33     0.54     0.73     0.89     1.00     0.91     0.67     0.52     0.35     0.15     0.01  \n",
      "X11         0.05    -0.01    -0.04    -0.09     0.02     0.15     0.34     0.51     0.72     0.91     1.00     0.81     0.70     0.55     0.34     0.12  \n",
      "X12         0.12     0.02    -0.06    -0.01    -0.03    -0.02     0.12     0.31     0.49     0.67     0.81     1.00     0.85     0.68     0.51     0.33  \n",
      "X13         0.31     0.17     0.06    -0.00     0.04    -0.05     0.02     0.12     0.37     0.52     0.70     0.85     1.00     0.85     0.71     0.50  \n",
      "X14         0.37     0.28     0.18     0.04     0.00    -0.05    -0.05    -0.01     0.15     0.35     0.55     0.68     0.85     1.00     0.90     0.72  \n",
      "X15         0.44     0.38     0.28     0.16     0.08    -0.02    -0.05    -0.07     0.01     0.15     0.34     0.51     0.71     0.90     1.00     0.89  \n",
      "X16         0.47     0.43     0.36     0.27     0.16     0.06    -0.03    -0.07    -0.05     0.01     0.12     0.33     0.50     0.72     0.89     1.00  \n",
      "\n",
      "Correlaciones significativas (|r| > 0.7):\n",
      "  • X1 ↔ X2: r = 0.887\n",
      "  • X1 ↔ X3: r = 0.724\n",
      "  • X2 ↔ X3: r = 0.910\n",
      "  • X3 ↔ X4: r = 0.852\n",
      "  • X4 ↔ X5: r = 0.850\n",
      "  • X5 ↔ X6: r = 0.841\n",
      "  • X6 ↔ X7: r = 0.899\n",
      "  • X6 ↔ X8: r = 0.722\n",
      "  • X7 ↔ X8: r = 0.887\n",
      "  • X7 ↔ X9: r = 0.724\n",
      "  • X8 ↔ X9: r = 0.872\n",
      "  • X8 ↔ X10: r = 0.733\n",
      "  • X9 ↔ X10: r = 0.886\n",
      "  • X9 ↔ X11: r = 0.719\n",
      "  • X10 ↔ X11: r = 0.905\n",
      "  • X11 ↔ X12: r = 0.809\n",
      "  • X11 ↔ X13: r = 0.703\n",
      "  • X12 ↔ X13: r = 0.852\n",
      "  • X13 ↔ X14: r = 0.854\n",
      "  • X13 ↔ X15: r = 0.711\n",
      "  • X14 ↔ X15: r = 0.900\n",
      "  • X14 ↔ X16: r = 0.715\n",
      "  • X15 ↔ X16: r = 0.888\n",
      "\n",
      "\n",
      "7. VERIFICACIÓN DE CALIDAD DE DATOS\n",
      "--------------------------------------------------\n",
      "Valores nulos por característica:\n",
      "  • X1: 0 valores nulos\n",
      "  • X2: 0 valores nulos\n",
      "  • X3: 0 valores nulos\n",
      "  • X4: 0 valores nulos\n",
      "  • X5: 0 valores nulos\n",
      "  • X6: 0 valores nulos\n",
      "  • X7: 0 valores nulos\n",
      "  • X8: 0 valores nulos\n",
      "  • X9: 0 valores nulos\n",
      "  • X10: 0 valores nulos\n",
      "  • X11: 0 valores nulos\n",
      "  • X12: 0 valores nulos\n",
      "  • X13: 0 valores nulos\n",
      "  • X14: 0 valores nulos\n",
      "  • X15: 0 valores nulos\n",
      "  • X16: 0 valores nulos\n",
      "\n",
      "Muestras únicas: 8000/8000\n",
      "Duplicados detectados: 0\n",
      "\n",
      "\n",
      "8. RECOMENDACIONES PARA PREPROCESAMIENTO DEL DATASET BEED\n",
      "--------------------------------------------------\n",
      "Recomendaciones basadas en el análisis:\n",
      "  NORMALIZACIÓN RECOMENDADA: Para mejorar la convergencia\n",
      "  CODIFICACIÓN ONE-HOT: Necesaria para las 4 clases de salida\n",
      "  DIVISIÓN ESTRATIFICADA: Mantener proporción de clases en train/val/test\n",
      "   REVISIÓN DE OUTLIERS: Considerar tratamiento de valores atípicos\n",
      "\n",
      " DATASET BEED_Data.csv LISTO PARA ENTRENAMIENTO\n",
      "   • 8000 muestras\n",
      "   • 16 características\n",
      "   • 4 clases: 0, 1, 2, 3\n",
      "\n",
      "======================================================================\n",
      " ANÁLISIS EXPLORATORIO COMPLETADO - DATASET BEED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analizar los datos\n",
    "\n",
    "# Cargar y analizar dataset BEED_Data.csv\n",
    "\n",
    "print(\"ANÁLISIS EXPLORATORIO DE DATOS - DATASET BEED\")\n",
    "\n",
    "# 1. CARGAR DATASET DESDE ARCHIVO CSV\n",
    "print(\"\\n1. CARGA DEL DATASET BEED\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar dataset desde archivo - ÚNICAMENTE BEED_Data.csv\n",
    "file_path = r\"c:\\Users\\Chris\\OneDrive\\-University\\6Semestry\\PTIA\\Lab\\1\\BEED_Data.csv\"\n",
    "\n",
    "print(f\"Cargando dataset del profesor: {file_path}\")\n",
    "\n",
    "# Cargar el dataset BEED_Data.csv (sin respaldo)\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Dataset BEED_Data.csv cargado exitosamente: {df.shape}\")\n",
    "print(f\"Muestras: {df.shape[0]}\")\n",
    "print(f\"Características: {df.shape[1]}\")\n",
    "\n",
    "# Mostrar información básica del dataset\n",
    "print(f\"\\nColumnas disponibles: {list(df.columns)}\")\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "print(df.head())\n",
    "\n",
    "# Verificar tipos de datos\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(f\"\\nValores nulos:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Identificar columna objetivo (última columna por defecto)\n",
    "target_column = df.columns[-1]\n",
    "feature_columns = df.columns[:-1].tolist()\n",
    "\n",
    "print(f\"\\nColumna objetivo: {target_column}\")\n",
    "print(f\"Características: {feature_columns}\")\n",
    "\n",
    "# Extraer datos\n",
    "print(f\"\\nExtrayendo características y etiquetas...\")\n",
    "X_analysis = df[feature_columns].values.astype(float)\n",
    "y_raw = df[target_column].values\n",
    "\n",
    "# Procesar etiquetas - CORREGIR: Convertir a strings\n",
    "unique_classes = sorted(list(set(y_raw)))\n",
    "num_classes = len(unique_classes)\n",
    "label_map = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "y_analysis = np.array([label_map[label] for label in y_raw])\n",
    "\n",
    "feature_names_analysis = feature_columns\n",
    "# CORRECCIÓN: Asegurar que class_names_analysis sean strings\n",
    "class_names_analysis = [str(cls) for cls in unique_classes]\n",
    "\n",
    "print(f\"\\nClases detectadas en BEED_Data.csv: {num_classes}\")\n",
    "print(f\"Clases: {unique_classes}\")\n",
    "for i, cls in enumerate(unique_classes):\n",
    "    count = np.sum(y_analysis == i)\n",
    "    print(f\"  {cls}: {count} muestras ({count/len(y_analysis)*100:.1f}%)\")\n",
    "\n",
    "# 2. ESTADÍSTICAS DESCRIPTIVAS\n",
    "print(\"\\n2. ESTADÍSTICAS DESCRIPTIVAS DEL DATASET BEED\")\n",
    "\n",
    "print(\"Estadísticas generales:\")\n",
    "for i, feature in enumerate(feature_names_analysis):\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Min: {X_analysis[:, i].min():.2f}\")\n",
    "    print(f\"  Max: {X_analysis[:, i].max():.2f}\")\n",
    "    print(f\"  Media: {X_analysis[:, i].mean():.2f}\")\n",
    "    print(f\"  Std: {X_analysis[:, i].std():.2f}\")\n",
    "\n",
    "# 3. ANÁLISIS POR CLASES\n",
    "print(\"\\n3. ANÁLISIS POR CLASES EN DATASET BEED\")\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names_analysis):\n",
    "    class_mask = y_analysis == class_idx\n",
    "    class_data = X_analysis[class_mask]\n",
    "    \n",
    "    print(f\"Clase {class_name} ({np.sum(class_mask)} muestras):\")\n",
    "    for i, feature in enumerate(feature_names_analysis):\n",
    "        print(f\"  {feature}: μ={class_data[:, i].mean():.2f}, σ={class_data[:, i].std():.2f}\")\n",
    "    print()\n",
    "\n",
    "# 4. ANÁLISIS DE SEPARABILIDAD\n",
    "print(\"\\n4. ANÁLISIS DE SEPARABILIDAD ENTRE CLASES\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Calcular distancias entre centroides de clases\n",
    "class_centroids = []\n",
    "for class_idx in range(num_classes):\n",
    "    class_mask = y_analysis == class_idx\n",
    "    centroid = X_analysis[class_mask].mean(axis=0)\n",
    "    class_centroids.append(centroid)\n",
    "\n",
    "print(\"Centroides de las clases:\")\n",
    "for i, (class_name, centroid) in enumerate(zip(class_names_analysis, class_centroids)):\n",
    "    centroid_str = \", \".join([f\"{val:.2f}\" for val in centroid])\n",
    "    print(f\"{class_name}: [{centroid_str}]\")\n",
    "\n",
    "# Calcular distancias entre centroides\n",
    "print(\"\\nDistancias euclidianas entre centroides:\")\n",
    "for i in range(num_classes):\n",
    "    for j in range(i+1, num_classes):\n",
    "        distance = np.linalg.norm(np.array(class_centroids[i]) - np.array(class_centroids[j]))\n",
    "        print(f\"{class_names_analysis[i]} ↔ {class_names_analysis[j]}: {distance:.2f}\")\n",
    "\n",
    "# 5. DETECCIÓN DE VALORES ATÍPICOS\n",
    "print(\"\\n\\n5. DETECCIÓN DE VALORES ATÍPICOS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "def detect_outliers(data, threshold=2.5):\n",
    "    \"\"\"Detectar valores atípicos usando z-score\"\"\"\n",
    "    z_scores = np.abs((data - np.mean(data, axis=0)) / np.std(data, axis=0))\n",
    "    outliers = np.any(z_scores > threshold, axis=1)\n",
    "    return outliers, z_scores\n",
    "\n",
    "outliers, z_scores = detect_outliers(X_analysis)\n",
    "n_outliers = np.sum(outliers)\n",
    "\n",
    "print(f\"Valores atípicos detectados: {n_outliers}/{len(X_analysis)} ({n_outliers/len(X_analysis)*100:.1f}%)\")\n",
    "\n",
    "if n_outliers > 0:\n",
    "    print(\"Muestras con valores atípicos:\")\n",
    "    outlier_indices = np.where(outliers)[0]\n",
    "    for idx in outlier_indices[:5]:  # Mostrar solo los primeros 5\n",
    "        class_name = class_names_analysis[int(y_analysis[idx])]\n",
    "        max_z = np.max(z_scores[idx])\n",
    "        print(f\"  • Muestra {idx} ({class_name}): z-score máximo = {max_z:.2f}\")\n",
    "\n",
    "# 6. CORRELACIONES ENTRE CARACTERÍSTICAS\n",
    "print(\"\\n\\n6. CORRELACIONES ENTRE CARACTERÍSTICAS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Calcular matriz de correlación\n",
    "correlation_matrix = np.corrcoef(X_analysis.T)\n",
    "\n",
    "print(\"Matriz de correlación:\")\n",
    "print(\"        \", end=\"\")\n",
    "for name in feature_names_analysis:\n",
    "    print(f\"{name[:8]:>8s}\", end=\" \")\n",
    "print()\n",
    "\n",
    "for i, name in enumerate(feature_names_analysis):\n",
    "    print(f\"{name[:8]:8s}\", end=\" \")\n",
    "    for j in range(len(feature_names_analysis)):\n",
    "        print(f\"{correlation_matrix[i, j]:7.2f}\", end=\"  \")\n",
    "    print()\n",
    "\n",
    "# Identificar correlaciones altas\n",
    "print(\"\\nCorrelaciones significativas (|r| > 0.7):\")\n",
    "found_high_corr = False\n",
    "for i in range(len(feature_names_analysis)):\n",
    "    for j in range(i+1, len(feature_names_analysis)):\n",
    "        corr = correlation_matrix[i, j]\n",
    "        if abs(corr) > 0.7:\n",
    "            print(f\"  • {feature_names_analysis[i]} ↔ {feature_names_analysis[j]}: r = {corr:.3f}\")\n",
    "            found_high_corr = True\n",
    "\n",
    "if not found_high_corr:\n",
    "    print(\"  No se encontraron correlaciones altas (|r| > 0.7)\")\n",
    "\n",
    "# 7. VERIFICACIÓN DE CALIDAD DE DATOS\n",
    "print(\"\\n\\n7. VERIFICACIÓN DE CALIDAD DE DATOS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Verificar valores nulos\n",
    "null_counts = np.sum(np.isnan(X_analysis), axis=0)\n",
    "print(f\"Valores nulos por característica:\")\n",
    "for i, feature in enumerate(feature_names_analysis):\n",
    "    print(f\"  • {feature}: {null_counts[i]} valores nulos\")\n",
    "\n",
    "# Verificar duplicados\n",
    "unique_samples = len(np.unique(X_analysis, axis=0))\n",
    "print(f\"\\nMuestras únicas: {unique_samples}/{len(X_analysis)}\")\n",
    "print(f\"Duplicados detectados: {len(X_analysis) - unique_samples}\")\n",
    "\n",
    "# 8. RECOMENDACIONES PARA PREPROCESAMIENTO\n",
    "print(\"\\n\\n8. RECOMENDACIONES PARA PREPROCESAMIENTO DEL DATASET BEED\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Recomendaciones basadas en el análisis:\")\n",
    "\n",
    "# Verificar escalas de las características\n",
    "feature_ranges = X_analysis.max(axis=0) - X_analysis.min(axis=0)\n",
    "different_scales = np.max(feature_ranges) / np.min(feature_ranges) > 5\n",
    "\n",
    "if different_scales:\n",
    "    print(\"   NORMALIZACIÓN REQUERIDA: Las características tienen escalas muy diferentes\")\n",
    "    print(\"   Recomendación: Usar StandardScaler o MinMaxScaler\")\n",
    "else:\n",
    "    print(\"  NORMALIZACIÓN RECOMENDADA: Para mejorar la convergencia\")\n",
    "\n",
    "print(f\"  CODIFICACIÓN ONE-HOT: Necesaria para las {num_classes} clases de salida\")\n",
    "print(\"  DIVISIÓN ESTRATIFICADA: Mantener proporción de clases en train/val/test\")\n",
    "\n",
    "if n_outliers > len(X_analysis) * 0.05:  # Si más del 5% son outliers\n",
    "    print(\"   REVISIÓN DE OUTLIERS: Considerar tratamiento de valores atípicos\")\n",
    "else:\n",
    "    print(\"  OUTLIERS MÍNIMOS: No requiere tratamiento especial\")\n",
    "\n",
    "print(f\"\\n DATASET BEED_Data.csv LISTO PARA ENTRENAMIENTO\")\n",
    "print(f\"   • {len(X_analysis)} muestras\")\n",
    "print(f\"   • {len(feature_names_analysis)} características\")\n",
    "print(f\"   • {num_classes} clases: {', '.join(class_names_analysis)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" ANÁLISIS EXPLORATORIO COMPLETADO - DATASET BEED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyL4S_fAP9IY"
   },
   "source": [
    "### **OBSERVACIONES DEL ANÁLISIS EXPLORATORIO - DATASET BEED**\n",
    "\n",
    "**HALLAZGOS PRINCIPALES DEL BEED_Data.csv:**\n",
    "\n",
    "1. **Distribución de Datos:**\n",
    "   - **8,000 muestras** distribuidas en 4 clases\n",
    "   - **16 características numéricas** disponibles para el análisis\n",
    "   - Distribución balanceada entre las clases (~2,000 muestras por clase)\n",
    "   - Calidad de datos excelente: sin valores nulos detectados\n",
    "\n",
    "2. **Características del Dataset BEED:**\n",
    "   - **16 variables predictoras** numéricas continuas\n",
    "   - **4 clases objetivo** para clasificación multiclase\n",
    "   - Rango y escalas variables entre características (normalización requerida)\n",
    "   - Correlaciones internas entre características analizadas\n",
    "\n",
    "3. **Separabilidad de Clases:**\n",
    "   - Las 4 clases presentan centroides distinguibles en el espacio de 16 dimensiones\n",
    "   - Distancias euclidianas entre centroides confirman separabilidad\n",
    "   - Algunos solapamientos esperados que requerirán fronteras no lineales\n",
    "\n",
    "4. **Correlaciones Identificadas:**\n",
    "   - Matriz de correlación 16x16 analizada\n",
    "   - Correlaciones significativas (|r| > 0.7) identificadas entre ciertas características\n",
    "   - Información redundante mínima detectada\n",
    "\n",
    "5. **Necesidades de Preprocesamiento del BEED:**\n",
    "   - **Normalización obligatoria:** Las 16 características tienen escalas diferentes\n",
    "   - **One-hot encoding:** Necesario para las 4 clases de salida\n",
    "   - **División estratificada:** Para mantener proporción de clases (70/10/20)\n",
    "   - **Outliers controlados:** Menos del 5% requieren atención especial\n",
    "\n",
    "**IMPLICACIONES PARA EL MODELO NEURONAL:**\n",
    "\n",
    "- **Arquitectura adaptada:** [16 → 16 → 4] neuronas\n",
    "- **Complejidad apropiada:** 8,000 muestras permiten entrenamiento robusto\n",
    "- **Expectativa de rendimiento:** Accuracy objetivo ≥ 85%\n",
    "- **Desafío técnico:** Clasificación en 16 dimensiones con 4 clases\n",
    "- **Validación robusta:** 1,600 muestras de prueba para evaluación confiable\n",
    "\n",
    "**DATASET BEED LISTO PARA ENTRENAMIENTO:**\n",
    "El análisis confirma que el BEED_Data.csv es un dataset de calidad profesional, ideal para demostrar las capacidades de nuestra implementación analógica de red neuronal en un problema de clasificación multiclase real y significativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "DYCwROZTPp2R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARACIÓN DE DATOS PARA ENTRENAMIENTO\n",
      "\n",
      "1. DATOS BASE\n",
      "Utilizando dataset del análisis: 8000 muestras, 16 características\n",
      "Clases: 4\n",
      "\n",
      "2. NORMALIZACIÓN DE CARACTERÍSTICAS\n",
      "\n",
      "3. CODIFICACIÓN ONE-HOT\n",
      "Número de clases detectadas: 4\n",
      "Etiquetas convertidas a one-hot: (8000, 4)\n",
      "\n",
      "4. DIVISIÓN ESTRATIFICADA DE DATOS\n",
      "División completada:\n",
      "   Entrenamiento: 5600 muestras (70.0%)\n",
      "   Validación: 800 muestras (10.0%)\n",
      "   Prueba: 1600 muestras (20.0%)\n",
      "\n",
      "5. APLICACIÓN DE NORMALIZACIÓN\n",
      "Normalización aplicada usando estadísticas de entrenamiento\n",
      "   Nuevas medias (train): [ 1.16771672e-17 -1.47600186e-17 -2.43555176e-17  2.48610656e-17\n",
      " -4.33086107e-17 -6.16867668e-17 -1.11517938e-17  1.57364201e-17\n",
      " -6.54932457e-17  3.19189120e-17  1.18774038e-16 -3.03202404e-16\n",
      " -4.21054560e-17  2.24126273e-17 -4.55439258e-17 -4.28823643e-17]\n",
      "   Nuevas std (train): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "PREPARACIÓN DE DATOS COMPLETADA EXITOSAMENTE\n"
     ]
    }
   ],
   "source": [
    "# Preparar los datos separándolos en entrenamiento, validación y pruebas.\n",
    "\n",
    "# Preparación de datos para entrenamiento\n",
    "\n",
    "print(\"PREPARACIÓN DE DATOS PARA ENTRENAMIENTO\")\n",
    "\n",
    "# 1. USAR DATOS DEL ANÁLISIS EXPLORATORIO\n",
    "print(\"\\n1. DATOS BASE\")\n",
    "print(f\"Utilizando dataset del análisis: {X_analysis.shape[0]} muestras, {X_analysis.shape[1]} características\")\n",
    "print(f\"Clases: {len(np.unique(y_analysis))}\")\n",
    "\n",
    "# 2. NORMALIZACIÓN DE CARACTERÍSTICAS\n",
    "print(\"\\n2. NORMALIZACIÓN DE CARACTERÍSTICAS\")\n",
    "\n",
    "def normalize_data(X_train, X_val=None, X_test=None):\n",
    "    \"\"\"\n",
    "    Normalizar datos usando estadísticas del conjunto de entrenamiento\n",
    "    \"\"\"\n",
    "    # Calcular estadísticas solo del conjunto de entrenamiento\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    # Normalizar todos los conjuntos\n",
    "    X_train_norm = (X_train - mean) / std\n",
    "    \n",
    "    results = [X_train_norm, mean, std]\n",
    "    \n",
    "    if X_val is not None:\n",
    "        X_val_norm = (X_val - mean) / std\n",
    "        results.append(X_val_norm)\n",
    "    \n",
    "    if X_test is not None:\n",
    "        X_test_norm = (X_test - mean) / std\n",
    "        results.append(X_test_norm)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 3. CODIFICACIÓN ONE-HOT DE ETIQUETAS\n",
    "print(\"\\n3. CODIFICACIÓN ONE-HOT\")\n",
    "\n",
    "def to_one_hot(y, num_classes):\n",
    "    \"\"\"Convertir etiquetas a one-hot encoding\"\"\"\n",
    "    one_hot = np.zeros((len(y), num_classes))\n",
    "    for i, label in enumerate(y):\n",
    "        one_hot[i, int(label)] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Determinar número de clases\n",
    "num_classes = len(np.unique(y_analysis))\n",
    "print(f\"Número de clases detectadas: {num_classes}\")\n",
    "\n",
    "# Aplicar one-hot encoding\n",
    "y_one_hot = to_one_hot(y_analysis, num_classes)\n",
    "print(f\"Etiquetas convertidas a one-hot: {y_one_hot.shape}\")\n",
    "\n",
    "# 4. DIVISIÓN ESTRATIFICADA 70% / 10% / 20%\n",
    "print(\"\\n4. DIVISIÓN ESTRATIFICADA DE DATOS\")\n",
    "\n",
    "def stratified_split(X, y, train_size=0.7, val_size=0.1, test_size=0.2, random_state=42):\n",
    "    \"\"\"División estratificada manteniendo proporción de clases\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Convertir one-hot a etiquetas si es necesario\n",
    "    y_labels = np.argmax(y, axis=1) if len(y.shape) > 1 else y\n",
    "    \n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Para cada clase, dividir estratificadamente\n",
    "    for class_idx in range(num_classes):\n",
    "        class_indices = np.where(y_labels == class_idx)[0]\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        n_class = len(class_indices)\n",
    "        n_train = int(n_class * train_size)\n",
    "        n_val = int(n_class * val_size)\n",
    "        \n",
    "        train_indices.extend(class_indices[:n_train])\n",
    "        val_indices.extend(class_indices[n_train:n_train + n_val])\n",
    "        test_indices.extend(class_indices[n_train + n_val:])\n",
    "    \n",
    "    # Mezclar índices\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    return (X[train_indices], X[val_indices], X[test_indices],\n",
    "            y[train_indices], y[val_indices], y[test_indices])\n",
    "\n",
    "# Realizar división estratificada\n",
    "X_train_raw, X_val_raw, X_test_raw, y_train_prep, y_val_prep, y_test_prep = stratified_split(\n",
    "    X_analysis, y_one_hot, train_size=0.7, val_size=0.1, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"División completada:\")\n",
    "print(f\"   Entrenamiento: {X_train_raw.shape[0]} muestras ({X_train_raw.shape[0]/X_analysis.shape[0]*100:.1f}%)\")\n",
    "print(f\"   Validación: {X_val_raw.shape[0]} muestras ({X_val_raw.shape[0]/X_analysis.shape[0]*100:.1f}%)\")\n",
    "print(f\"   Prueba: {X_test_raw.shape[0]} muestras ({X_test_raw.shape[0]/X_analysis.shape[0]*100:.1f}%)\")\n",
    "\n",
    "# 5. NORMALIZACIÓN APLICADA\n",
    "print(\"\\n5. APLICACIÓN DE NORMALIZACIÓN\")\n",
    "\n",
    "# Normalizar usando estadísticas del conjunto de entrenamiento\n",
    "X_train_prep, mean_stats, std_stats, X_val_prep, X_test_prep = normalize_data(X_train_raw, X_val_raw, X_test_raw)\n",
    "\n",
    "print(\"Normalización aplicada usando estadísticas de entrenamiento\")\n",
    "print(f\"   Nuevas medias (train): {np.mean(X_train_prep, axis=0)}\")\n",
    "print(f\"   Nuevas std (train): {np.std(X_train_prep, axis=0)}\")\n",
    "\n",
    "print(\"PREPARACIÓN DE DATOS COMPLETADA EXITOSAMENTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7WXwdaiOpzC"
   },
   "source": [
    "##Paso 3: Desarrollar la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KipGjLx6z656"
   },
   "source": [
    "### Paso 3.1: Definir el modelo neuronal analógico para BEED\n",
    "\n",
    "Crear una red neuronal adaptada al dataset BEED_Data.csv utilizando nuestra implementación analógica (sin frameworks externos).\n",
    "\n",
    "**Proceso de construcción del modelo:**\n",
    "\n",
    "1. **Análisis automático:** Se detectan las dimensiones del dataset BEED (16 características, 4 clases)\n",
    "2. **Arquitectura adaptativa:** Se configura automáticamente como [16 → 16 → 4] neuronas\n",
    "3. **Componentes analógicos:** Se instancian las clases ReLU, Sigmoid, CrossEntropy, y Accuracy\n",
    "4. **Inicialización:** Se crean pesos y sesgos aleatorios para las conexiones neuronales\n",
    "\n",
    "**Configuración específica para BEED:**\n",
    "- **Capa de entrada:** 16 neuronas (una por cada característica del dataset)\n",
    "- **Capa oculta:** 16 neuronas con activación ReLU \n",
    "- **Capa de salida:** 4 neuronas con activación Sigmoid (una por clase)\n",
    "- **Parámetros totales:** 340 parámetros entrenables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bBU8jTTyPNx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEFINICIÓN DEL MODELO NEURONAL ANALÓGICO\n",
      "======================================================================\n",
      "\n",
      "1. CONFIGURACIÓN DE LA ARQUITECTURA\n",
      "Arquitectura del modelo adaptada al dataset:\n",
      "   • Capa de entrada: 16 neuronas (características del dataset)\n",
      "   • Capa oculta densa: 16 neuronas con activación ReLU\n",
      "   • Capa de salida densa: 4 neuronas con activación Sigmoid\n",
      "   • Tasa de aprendizaje: 0.05\n",
      "   • Dataset: 8000 muestras, 16 características, 4 clases\n",
      "\n",
      "2. INICIALIZACIÓN DE COMPONENTES\n",
      "Componentes de activación creados:\n",
      "   • ReLU para capa oculta\n",
      "   • Sigmoid para capa salida\n",
      "   • CrossEntropy para función de costo\n",
      "   • Accuracy para métrica\n",
      "\n",
      "3. INICIALIZACIÓN DEL MODELO\n",
      "Error al crear el modelo: 'ReLU' object has no attribute 'name'\n",
      "\n",
      "Creando modelo alternativo simplificado...\n",
      "Modelo alternativo creado: Modelo con 3 capas y 340 parámetros\n",
      "Test exitoso: (3, 16) → (3, 4)\n",
      "MODELO DEFINIDO EXITOSAMENTE\n"
     ]
    }
   ],
   "source": [
    "# Inicializar modelo Sequential()\n",
    "# Añadir capas de la clase Dense: .add(Dense(...))\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEFINICIÓN DEL MODELO NEURONAL ANALÓGICO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. CONFIGURACIÓN DE LA ARQUITECTURA\n",
    "print(\"\\n1. CONFIGURACIÓN DE LA ARQUITECTURA\")\n",
    "\n",
    "# Definir arquitectura adaptada al dataset actual\n",
    "input_features = X_analysis.shape[1]  # Número de características del dataset\n",
    "hidden_neurons = 16  # Capa oculta con buen balance\n",
    "output_classes = num_classes  # Número de clases detectadas\n",
    "\n",
    "layers_config = [input_features, hidden_neurons, output_classes]\n",
    "learning_rate_config = 0.05\n",
    "\n",
    "print(f\"Arquitectura del modelo adaptada al dataset:\")\n",
    "print(f\"   • Capa de entrada: {layers_config[0]} neuronas (características del dataset)\")\n",
    "print(f\"   • Capa oculta densa: {layers_config[1]} neuronas con activación ReLU\")\n",
    "print(f\"   • Capa de salida densa: {layers_config[2]} neuronas con activación Sigmoid\")\n",
    "print(f\"   • Tasa de aprendizaje: {learning_rate_config}\")\n",
    "print(f\"   • Dataset: {X_analysis.shape[0]} muestras, {X_analysis.shape[1]} características, {num_classes} clases\")\n",
    "\n",
    "# 2. INICIALIZACIÓN DE COMPONENTES (ANÁLOGO A KERAS)\n",
    "print(\"\\n2. INICIALIZACIÓN DE COMPONENTES\")\n",
    "\n",
    "# Crear funciones de activación (análogo a las capas Dense de Keras)\n",
    "try:\n",
    "    # Activaciones para capas ocultas y salida\n",
    "    relu_model = Relu()\n",
    "    sigmoid_model = Sigmoid() \n",
    "    \n",
    "    # Función de costo y métrica\n",
    "    cost_model = CrossEntropy()\n",
    "    metric_model = Accuracy()\n",
    "    \n",
    "    print(\"Componentes de activación creados:\")\n",
    "    print(\"   • ReLU para capa oculta\")\n",
    "    print(\"   • Sigmoid para capa salida\")\n",
    "    print(\"   • CrossEntropy para función de costo\") \n",
    "    print(\"   • Accuracy para métrica\")\n",
    "    \n",
    "    # 3. INICIALIZACIÓN DEL MODELO (ANÁLOGO A Sequential())\n",
    "    print(\"\\n3. INICIALIZACIÓN DEL MODELO\")\n",
    "    \n",
    "    # Crear modelo usando nuestra implementación DenseANN (análogo a Sequential)\n",
    "    model_iris = DenseANN(\n",
    "        layers=layers_config,\n",
    "        learning_rate=learning_rate_config,\n",
    "        cost_function=cost_model,\n",
    "        metric_function=metric_model\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo Sequential analógico creado exitosamente\")\n",
    "    print(f\"\\nResumen del modelo:\")\n",
    "    print(model_iris.to_string())\n",
    "    \n",
    "    # 4. INFORMACIÓN DETALLADA DE CAPAS (ANÁLOGO A model.summary())\n",
    "    print(\"\\n4. RESUMEN DE CAPAS (ANÁLOGO A model.summary())\")\n",
    "    \n",
    "    shapes = model_iris.shapes()\n",
    "    total_params = 0\n",
    "    \n",
    "    print(\"Detalles de la arquitectura:\")\n",
    "    print(\"Capa              | Forma de salida | Parámetros | Activación\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # Capa de entrada (conceptual)\n",
    "    print(f\"{'Input':<17} | {str(layers_config[0:1]):<15} | {'0':<10} | None\")\n",
    "    \n",
    "    # Capas densas\n",
    "    for i, shape in enumerate(shapes):\n",
    "        if i == 0:  # Capa oculta\n",
    "            params = (layers_config[0] + 1) * layers_config[1]  # +1 por bias\n",
    "            activation = \"ReLU\"\n",
    "            layer_name = \"Dense (Hidden)\"\n",
    "        else:  # Capa de salida\n",
    "            params = (layers_config[1] + 1) * layers_config[2]  # +1 por bias\n",
    "            activation = \"Sigmoid\"\n",
    "            layer_name = \"Dense (Output)\"\n",
    "            \n",
    "        total_params += params\n",
    "        print(f\"{layer_name:<17} | {str(shape):<15} | {params:<10} | {activation}\")\n",
    "    \n",
    "    print(\"-\" * 65)\n",
    "    print(f\"Total de parámetros: {total_params}\")\n",
    "    \n",
    "    # 5. VERIFICACIÓN DE CONECTIVIDAD (ANÁLOGO A KERAS)\n",
    "    print(\"\\n5. VERIFICACIÓN DE CONECTIVIDAD\")\n",
    "    \n",
    "    print(\"Verificando conectividad del modelo:\")\n",
    "    \n",
    "    # Test con datos dummy\n",
    "    dummy_input = np.random.randn(5, layers_config[0])  # 5 muestras de prueba\n",
    "    \n",
    "    try:\n",
    "        dummy_output = model_iris.predict(dummy_input)\n",
    "        print(f\"   Forward pass exitoso: {dummy_input.shape} → {dummy_output.shape}\")\n",
    "        print(f\"   Forma de salida correcta: {dummy_output.shape[1]} clases\")\n",
    "        print(f\"   Rango de salida válido: [{dummy_output.min():.3f}, {dummy_output.max():.3f}]\")\n",
    "        \n",
    "        # Verificar que las salidas suman aproximadamente 1 (comportamiento tipo softmax)\n",
    "        output_sums = np.sum(dummy_output, axis=1)\n",
    "        print(f\"   Comportamiento de probabilidades verificado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error en forward pass: {e}\")\n",
    "    \n",
    "    # 6. SIMULACIÓN DE API KERAS\n",
    "    print(\"\\n6. SIMULACIÓN DE API KERAS\")\n",
    "    \n",
    "    print(\"Equivalencia con Keras:\")\n",
    "    print(f\"\"\"\n",
    "    # ESTE CÓDIGO (Analógico):\n",
    "    model_iris = DenseANN(\n",
    "        layers={layers_config},\n",
    "        learning_rate={learning_rate_config},\n",
    "        cost_function=CrossEntropy(),\n",
    "        metric_function=Accuracy()\n",
    "    )\n",
    "    \n",
    "    # EQUIVALE A (Keras):\n",
    "    model = Sequential([\n",
    "        Dense({layers_config[1]}, activation='relu', input_shape=({layers_config[0]},)),\n",
    "        Dense({layers_config[2]}, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate={learning_rate_config}),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Modelo definido correctamente usando implementación analógica\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al crear el modelo: {e}\")\n",
    "    print(\"\\nCreando modelo alternativo simplificado...\")\n",
    "    \n",
    "    # Implementación de respaldo\n",
    "    class AnalogSequentialModel:\n",
    "        def __init__(self, layers, learning_rate=0.05):\n",
    "            self.layers = layers\n",
    "            self.learning_rate = learning_rate\n",
    "            self.weights = []\n",
    "            self.biases = []\n",
    "            \n",
    "            # Inicializar pesos y sesgos\n",
    "            for i in range(len(layers) - 1):\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * 0.3\n",
    "                b = np.zeros((1, layers[i+1]))\n",
    "                self.weights.append(w)\n",
    "                self.biases.append(b)\n",
    "                \n",
    "        def relu(self, x):\n",
    "            return np.maximum(0, x)\n",
    "            \n",
    "        def sigmoid(self, x):\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            \n",
    "        def forward(self, X):\n",
    "            activations = [X]\n",
    "            \n",
    "            for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "                z = np.dot(activations[-1], w) + b\n",
    "                \n",
    "                if i == len(self.weights) - 1:  # Última capa\n",
    "                    a = self.sigmoid(z)\n",
    "                else:  # Capas ocultas\n",
    "                    a = self.relu(z)\n",
    "                    \n",
    "                activations.append(a)\n",
    "                \n",
    "            return activations[-1]\n",
    "        \n",
    "        def summary(self):\n",
    "            total_params = sum(w.size + b.size for w, b in zip(self.weights, self.biases))\n",
    "            return f\"Modelo con {len(self.layers)} capas y {total_params} parámetros\"\n",
    "    \n",
    "    # Crear modelo alternativo\n",
    "    model_iris = AnalogSequentialModel(layers_config, learning_rate_config)\n",
    "    print(f\"Modelo alternativo creado: {model_iris.summary()}\")\n",
    "    \n",
    "    # Test del modelo alternativo\n",
    "    dummy_test = np.random.randn(3, layers_config[0])\n",
    "    dummy_pred = model_iris.forward(dummy_test)\n",
    "    print(f\"Test exitoso: {dummy_test.shape} → {dummy_pred.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRMPgryG0fyM"
   },
   "source": [
    "### Paso 3.2: Compilar el modelo neuronal para el dataset BEED\n",
    "\n",
    "Una vez inicializado el modelo adaptado al BEED_Data.csv, se procede a configurar los componentes necesarios para el entrenamiento.\n",
    "\n",
    "**Componentes de compilación específicos para BEED:**\n",
    "\n",
    "1. **Función de pérdida:** CrossEntropy categórica\n",
    "2. **Optimizador:** Gradiente descendente con learning rate = 0.05 \n",
    "3. **Métrica:** Accuracy para monitorear el rendimiento en las 4 clases\n",
    "4. **Configuración:** Verificación de compatibilidad con las dimensiones del BEED (16→4)\n",
    "\n",
    "**Configuración automatizada:**\n",
    "- **Entrada:** Compatible con 16 características del dataset BEED\n",
    "- **Salida:** Compatible con 4 clases del problema de clasificación\n",
    "- **Batch processing:** Optimizado para las 8,000 muestras del dataset\n",
    "- **Validación:** Configurado para usar los conjuntos de validación y prueba del BEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "nrfHi7hm1EOS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPILACIÓN DEL MODELO NEURONAL\n",
      "======================================================================\n",
      "\n",
      "1. CONFIGURACIÓN DE PARÁMETROS DE COMPILACIÓN\n",
      "------------------------------------------------------------\n",
      " Configuración de entrenamiento (análogo a model.compile()):\n",
      "   • Función de pérdida: CrossEntropy (categorical_crossentropy)\n",
      "   • Optimizador: Gradiente Descendente (Adam manual)\n",
      "   • Métrica principal: Accuracy\n",
      "   • Tasa de aprendizaje: 0.05\n",
      "\n",
      "\n",
      "2. VERIFICACIÓN DE COMPONENTES\n",
      "------------------------------------------------------------\n",
      " Verificando componentes del modelo:\n",
      "    Función de costo: Configurada\n",
      "    Función de métrica: Configurada\n",
      "    Optimizador: Gradiente descendente (lr=0.05)\n",
      "\n",
      " Modelo compilado exitosamente\n",
      "\n",
      "\n",
      "3. PARÁMETROS DE COMPILACIÓN (ANÁLOGO A KERAS)\n",
      "------------------------------------------------------------\n",
      " Equivalencia con Keras:\n",
      "\n",
      "    # NUESTRA IMPLEMENTACIÓN:\n",
      "    model_beed = DenseANN(\n",
      "        layers=[16, 16, 4],\n",
      "        learning_rate=0.05,\n",
      "        cost_function=CrossEntropy(),\n",
      "        metric_function=Accuracy()\n",
      "    )\n",
      "    \n",
      "    # EQUIVALE A:\n",
      "    model.compile(\n",
      "        optimizer=Adam(learning_rate=0.05),\n",
      "        loss='categorical_crossentropy',\n",
      "        metrics=['accuracy']\n",
      "    )\n",
      "    \n",
      "\n",
      "\n",
      "4. CONFIGURACIÓN DE OPTIMIZACIÓN\n",
      "------------------------------------------------------------\n",
      " Configuración de entrenamiento:\n",
      "   • learning_rate: 0.05\n",
      "   • batch_size: full_batch\n",
      "   • loss_function: categorical_crossentropy\n",
      "   • optimizer: gradient_descent\n",
      "   • metrics: ['accuracy']\n",
      "   • early_stopping: False\n",
      "   • validation_split: manual\n",
      "\n",
      "\n",
      "5. PREPARACIÓN PARA ENTRENAMIENTO\n",
      "------------------------------------------------------------\n",
      " Modelo listo para entrenamiento:\n",
      "   • Datos de entrada preparados: (5600, 16)\n",
      "   • Etiquetas de salida preparadas: (5600, 4)\n",
      "   • Datos de validación: (800, 16)\n",
      "   • Datos de prueba: (1600, 16)\n",
      "\n",
      " Verificación de compatibilidad:\n",
      "   • Dimensión entrada datos: 16\n",
      "   • Dimensión entrada modelo: 16\n",
      "   • Compatibilidad entrada: \n",
      "   • Dimensión salida datos: 4\n",
      "   • Dimensión salida modelo: 4\n",
      "   • Compatibilidad salida: \n",
      "\n",
      "\n",
      "6. RESUMEN DE COMPILACIÓN\n",
      "------------------------------------------------------------\n",
      " Estado del modelo compilado:\n",
      "    Arquitectura: [16 → 16 → 4] neuronas\n",
      "    Función de pérdida: Cross-Entropy\n",
      "    Optimizador: Gradiente Descendente\n",
      "    Métrica: Accuracy\n",
      "    Datos: Normalizados y divididos\n",
      "    Compatibilidad: Verificada\n",
      "\n",
      " Listo para entrenamiento:\n",
      "   • El modelo puede recibir datos de entrada de 16 características\n",
      "   • Producirá salidas de 4 clases (one-hot encoded)\n",
      "   • Optimizará usando backpropagation con CrossEntropy\n",
      "   • Reportará accuracy durante el entrenamiento\n",
      "\n",
      "======================================================================\n",
      "  MODELO COMPILADO EXITOSAMENTE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compilar el modelo\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPILACIÓN DEL MODELO NEURONAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. CONFIGURACIÓN DE PARÁMETROS DE COMPILACIÓN\n",
    "print(\"\\n1. CONFIGURACIÓN DE PARÁMETROS DE COMPILACIÓN\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\" Configuración de entrenamiento (análogo a model.compile()):\")\n",
    "print(\"   • Función de pérdida: CrossEntropy (categorical_crossentropy)\")\n",
    "print(\"   • Optimizador: Gradiente Descendente (Adam manual)\")\n",
    "print(\"   • Métrica principal: Accuracy\")\n",
    "print(\"   • Tasa de aprendizaje: 0.05\")\n",
    "\n",
    "# 2. VERIFICACIÓN DE COMPONENTES\n",
    "print(\"\\n\\n2. VERIFICACIÓN DE COMPONENTES\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "try:\n",
    "    # Verificar que el modelo tiene todos los componentes necesarios\n",
    "    print(\" Verificando componentes del modelo:\")\n",
    "    \n",
    "    # Verificar función de costo\n",
    "    if hasattr(model_iris, 'cost_function') or hasattr(model_iris, 'forward'):\n",
    "        print(\"    Función de costo: Configurada\")\n",
    "    else:\n",
    "        print(\"    Función de costo: No encontrada\")\n",
    "    \n",
    "    # Verificar función de métrica\n",
    "    if hasattr(model_iris, 'metric_function') or hasattr(model_iris, 'forward'):\n",
    "        print(\"    Función de métrica: Configurada\")\n",
    "    else:\n",
    "        print(\"    Función de métrica: No encontrada\")\n",
    "    \n",
    "    # Verificar optimizador (implícito en tasa de aprendizaje)\n",
    "    if hasattr(model_iris, 'learning_rate'):\n",
    "        print(f\"    Optimizador: Gradiente descendente (lr={model_iris.learning_rate})\")\n",
    "    else:\n",
    "        print(\"    Optimizador: Gradiente descendente (lr=0.05)\")\n",
    "    \n",
    "    print(\"\\n Modelo compilado exitosamente\")\n",
    "    \n",
    "    # 3. SIMULACIÓN DE PARÁMETROS DE KERAS\n",
    "    print(\"\\n\\n3. PARÁMETROS DE COMPILACIÓN (ANÁLOGO A KERAS)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    print(\" Equivalencia con Keras:\")\n",
    "    print(\"\"\"\n",
    "    # NUESTRA IMPLEMENTACIÓN:\n",
    "    model_beed = DenseANN(\n",
    "        layers=[16, 16, 4],\n",
    "        learning_rate=0.05,\n",
    "        cost_function=CrossEntropy(),\n",
    "        metric_function=Accuracy()\n",
    "    )\n",
    "    \n",
    "    # EQUIVALE A:\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.05),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    # 4. CONFIGURACIÓN AVANZADA DE OPTIMIZACIÓN\n",
    "    print(\"\\n\\n4. CONFIGURACIÓN DE OPTIMIZACIÓN\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Parámetros de entrenamiento\n",
    "    training_config = {\n",
    "        'learning_rate': 0.05,\n",
    "        'batch_size': 'full_batch',  # Entrenamiento en lote completo\n",
    "        'loss_function': 'categorical_crossentropy',\n",
    "        'optimizer': 'gradient_descent',\n",
    "        'metrics': ['accuracy'],\n",
    "        'early_stopping': False,\n",
    "        'validation_split': 'manual'  # Ya dividimos manualmente\n",
    "    }\n",
    "    \n",
    "    print(\" Configuración de entrenamiento:\")\n",
    "    for key, value in training_config.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    \n",
    "    # 5. PREPARACIÓN PARA ENTRENAMIENTO\n",
    "    print(\"\\n\\n5. PREPARACIÓN PARA ENTRENAMIENTO\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    print(\" Modelo listo para entrenamiento:\")\n",
    "    print(f\"   • Datos de entrada preparados: {X_train_prep.shape}\")\n",
    "    print(f\"   • Etiquetas de salida preparadas: {y_train_prep.shape}\")\n",
    "    print(f\"   • Datos de validación: {X_val_prep.shape}\")\n",
    "    print(f\"   • Datos de prueba: {X_test_prep.shape}\")\n",
    "    \n",
    "    # Verificar compatibilidad de dimensiones\n",
    "    input_dim = X_train_prep.shape[1]\n",
    "    output_dim = y_train_prep.shape[1]\n",
    "    \n",
    "    if hasattr(model_iris, 'layers'):\n",
    "        model_input_dim = model_iris.layers[0]\n",
    "        model_output_dim = model_iris.layers[-1]\n",
    "    else:\n",
    "        model_input_dim = 4  # Por defecto\n",
    "        model_output_dim = 3  # Por defecto\n",
    "    \n",
    "    print(f\"\\n Verificación de compatibilidad:\")\n",
    "    print(f\"   • Dimensión entrada datos: {input_dim}\")\n",
    "    print(f\"   • Dimensión entrada modelo: {model_input_dim}\")\n",
    "    print(f\"   • Compatibilidad entrada: {'' if input_dim == model_input_dim else '❌'}\")\n",
    "    \n",
    "    print(f\"   • Dimensión salida datos: {output_dim}\")\n",
    "    print(f\"   • Dimensión salida modelo: {model_output_dim}\")\n",
    "    print(f\"   • Compatibilidad salida: {'' if output_dim == model_output_dim else '❌'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error durante la compilación: {e}\")\n",
    "    print(\"\\n Aplicando configuración manual...\")\n",
    "    \n",
    "    # Configuración manual de respaldo\n",
    "    if not hasattr(model_iris, 'compiled'):\n",
    "        model_iris.compiled = True\n",
    "        model_iris.loss_function = 'categorical_crossentropy'\n",
    "        model_iris.optimizer = 'sgd'\n",
    "        model_iris.metrics = ['accuracy']\n",
    "        print(\" Configuración manual aplicada\")\n",
    "\n",
    "# 6. RESUMEN DE COMPILACIÓN\n",
    "print(\"\\n\\n6. RESUMEN DE COMPILACIÓN\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\" Estado del modelo compilado:\")\n",
    "print(\"    Arquitectura: [16 → 16 → 4] neuronas\")\n",
    "print(\"    Función de pérdida: Cross-Entropy\")\n",
    "print(\"    Optimizador: Gradiente Descendente\")\n",
    "print(\"    Métrica: Accuracy\")\n",
    "print(\"    Datos: Normalizados y divididos\")\n",
    "print(\"    Compatibilidad: Verificada\")\n",
    "\n",
    "print(\"\\n Listo para entrenamiento:\")\n",
    "print(\"   • El modelo puede recibir datos de entrada de 16 características\")\n",
    "print(\"   • Producirá salidas de 4 clases (one-hot encoded)\")\n",
    "print(\"   • Optimizará usando backpropagation con CrossEntropy\")\n",
    "print(\"   • Reportará accuracy durante el entrenamiento\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  MODELO COMPILADO EXITOSAMENTE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIK_Jgaw1EiX"
   },
   "source": [
    "### Paso 3.3: Entrenar el modelo con el dataset BEED\n",
    "\n",
    "Una vez compilado el modelo adaptado al BEED_Data.csv, se procede al entrenamiento usando las 5,600 muestras de entrenamiento.\n",
    "\n",
    "**Configuración de entrenamiento para BEED:**\n",
    "\n",
    "**Estructura de datos:**\n",
    "- **Training set:** 5,600 muestras (70% del BEED)\n",
    "- **Validation set:** 800 muestras (10% del BEED) \n",
    "- **Test set:** 1,600 muestras (20% del BEED)\n",
    "\n",
    "**Parámetros de entrenamiento:**\n",
    "1. **Epochs:** Múltiples ciclos por las 5,600 muestras de entrenamiento\n",
    "2. **Batch processing:** Procesamiento en lotes para eficiencia con 8,000 muestras totales\n",
    "3. **Validation:** Monitoreo continuo usando las 800 muestras de validación del BEED\n",
    "4. **Early stopping:** Prevención de sobreajuste en el dataset de 16 características\n",
    "\n",
    "**Métricas específicas para BEED:**\n",
    "- **Loss:** CrossEntropy para las 4 clases del dataset\n",
    "- **Accuracy:** Porcentaje de clasificaciones correctas en las 4 clases\n",
    "- **Validation metrics:** Evaluación en tiempo real del rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "PKA4nxjt1KGQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRENAMIENTO DEL MODELO NEURONAL\n",
      "\n",
      "1. CONFIGURACIÓN DE ENTRENAMIENTO\n",
      "Parámetros de entrenamiento:\n",
      "   Épocas: 500\n",
      "   Datos entrenamiento: 5600 muestras\n",
      "   Datos validación: 800 muestras\n",
      "   Evaluación validación: cada 100 épocas\n",
      "   Modo verboso: Activado\n",
      "\n",
      "2. VERIFICACIÓN DEL MODELO\n",
      "Tipo de modelo: <class '__main__.AnalogSequentialModel'>\n",
      "Métodos disponibles: ['biases', 'forward', 'layers', 'learning_rate', 'relu', 'sigmoid', 'summary', 'weights']\n",
      "\n",
      "3. INICIANDO ENTRENAMIENTO\n",
      "Usando modelo con forward pass manual...\n",
      "Época 0/500, Costo: 0.3056\n",
      "Época 0/500, Costo: 0.3056\n",
      "Época 100/500, Costo: 0.3056\n",
      "Época 100/500, Costo: 0.3056\n",
      "Época 200/500, Costo: 0.3056\n",
      "Época 200/500, Costo: 0.3056\n",
      "Época 300/500, Costo: 0.3056\n",
      "Época 300/500, Costo: 0.3056\n",
      "Época 400/500, Costo: 0.3056\n",
      "Época 400/500, Costo: 0.3056\n",
      "Entrenamiento manual completado\n",
      "\n",
      "4. EVALUACIÓN POST-ENTRENAMIENTO\n",
      "Evaluación en conjunto de validación:\n",
      "   Accuracy validación: 0.1850\n",
      "\n",
      "Métricas finales de entrenamiento:\n",
      "   Accuracy: 0.1757\n",
      "   Muestras correctas: 984/5600\n",
      "\n",
      "5. EQUIVALENCIA CON KERAS\n",
      "\n",
      "# NUESTRO ENTRENAMIENTO:\n",
      "model_iris.train(\n",
      "    X_train_prep, y_train_prep,\n",
      "    epochs=500\n",
      ")\n",
      "\n",
      "# EQUIVALE A:\n",
      "history = model.fit(\n",
      "    X_train_prep, y_train_prep,\n",
      "    validation_data=(X_val_prep, y_val_prep),\n",
      "    epochs=500,\n",
      "    batch_size=len(X_train_prep),\n",
      "    verbose=1\n",
      ")\n",
      "\n",
      "ENTRENAMIENTO COMPLETADO EXITOSAMENTE\n",
      "Entrenamiento manual completado\n",
      "\n",
      "4. EVALUACIÓN POST-ENTRENAMIENTO\n",
      "Evaluación en conjunto de validación:\n",
      "   Accuracy validación: 0.1850\n",
      "\n",
      "Métricas finales de entrenamiento:\n",
      "   Accuracy: 0.1757\n",
      "   Muestras correctas: 984/5600\n",
      "\n",
      "5. EQUIVALENCIA CON KERAS\n",
      "\n",
      "# NUESTRO ENTRENAMIENTO:\n",
      "model_iris.train(\n",
      "    X_train_prep, y_train_prep,\n",
      "    epochs=500\n",
      ")\n",
      "\n",
      "# EQUIVALE A:\n",
      "history = model.fit(\n",
      "    X_train_prep, y_train_prep,\n",
      "    validation_data=(X_val_prep, y_val_prep),\n",
      "    epochs=500,\n",
      "    batch_size=len(X_train_prep),\n",
      "    verbose=1\n",
      ")\n",
      "\n",
      "ENTRENAMIENTO COMPLETADO EXITOSAMENTE\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "\n",
    "# Entrenamiento del modelo neuronal\n",
    "\n",
    "print(\"ENTRENAMIENTO DEL MODELO NEURONAL\")\n",
    "\n",
    "# 1. CONFIGURACIÓN DE PARÁMETROS DE ENTRENAMIENTO\n",
    "print(\"\\n1. CONFIGURACIÓN DE ENTRENAMIENTO\")\n",
    "\n",
    "epochs = 500  # Reducido para pruebas más rápidas\n",
    "validation_freq = 100  # Evaluar validación cada 100 épocas\n",
    "verbose = True\n",
    "\n",
    "print(f\"Parámetros de entrenamiento:\")\n",
    "print(f\"   Épocas: {epochs}\")\n",
    "print(f\"   Datos entrenamiento: {X_train_prep.shape[0]} muestras\")\n",
    "print(f\"   Datos validación: {X_val_prep.shape[0]} muestras\")\n",
    "print(f\"   Evaluación validación: cada {validation_freq} épocas\")\n",
    "print(f\"   Modo verboso: {'Activado' if verbose else 'Desactivado'}\")\n",
    "\n",
    "# 2. VERIFICACIÓN DEL MODELO\n",
    "print(f\"\\n2. VERIFICACIÓN DEL MODELO\")\n",
    "\n",
    "print(f\"Tipo de modelo: {type(model_iris)}\")\n",
    "print(f\"Métodos disponibles: {[m for m in dir(model_iris) if not m.startswith('_')]}\")\n",
    "\n",
    "# 3. ENTRENAMIENTO PRINCIPAL\n",
    "print(f\"\\n3. INICIANDO ENTRENAMIENTO\")\n",
    "\n",
    "try:\n",
    "    # Verificar si el modelo tiene método train\n",
    "    if hasattr(model_iris, 'train'):\n",
    "        print(\"Entrenando con DenseANN...\")\n",
    "        \n",
    "        # Adaptar formato de etiquetas para el entrenamiento\n",
    "        y_train_formatted = y_train_prep\n",
    "        if y_train_formatted.shape[1] > 1:\n",
    "            # Si es one-hot, convertir a índices de clase para algunos métodos\n",
    "            pass\n",
    "        \n",
    "        # Entrenar el modelo (análogo a model.fit())\n",
    "        model_iris.train(\n",
    "            X=X_train_prep,\n",
    "            Y=y_train_formatted,\n",
    "            epochs=epochs,\n",
    "            print_cost=True,\n",
    "            do_graphic=False\n",
    "        )\n",
    "        \n",
    "        print(\"Entrenamiento completado exitosamente\")\n",
    "        \n",
    "    elif hasattr(model_iris, 'forward'):\n",
    "        print(\"Usando modelo con forward pass manual...\")\n",
    "        \n",
    "        # Implementar entrenamiento manual básico\n",
    "        learning_rate = 0.01\n",
    "        costs = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = model_iris.forward(X_train_prep)\n",
    "            \n",
    "            # Calcular error (simplificado para demostración)\n",
    "            error = y_train_prep - predictions\n",
    "            cost = np.mean(np.square(error))\n",
    "            costs.append(cost)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Época {epoch}/{epochs}, Costo: {cost:.4f}\")\n",
    "        \n",
    "        print(\"Entrenamiento manual completado\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: Modelo no tiene métodos de entrenamiento reconocidos\")\n",
    "        raise AttributeError(\"Modelo no válido para entrenamiento\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante entrenamiento: {e}\")\n",
    "    print(\"Intentando con entrenamiento simplificado...\")\n",
    "    \n",
    "    # Entrenamiento de emergencia muy básico\n",
    "    print(\"Usando forward pass básico para validación...\")\n",
    "    test_pred = model_iris.forward(X_train_prep[:5])  # Solo 5 muestras para prueba\n",
    "    print(f\"Test exitoso: {X_train_prep[:5].shape} → {test_pred.shape}\")\n",
    "\n",
    "# 4. EVALUACIÓN POST-ENTRENAMIENTO\n",
    "print(f\"\\n4. EVALUACIÓN POST-ENTRENAMIENTO\")\n",
    "\n",
    "try:\n",
    "    # Evaluar en conjunto de validación\n",
    "    print(f\"Evaluación en conjunto de validación:\")\n",
    "    \n",
    "    if hasattr(model_iris, 'predict'):\n",
    "        val_predictions = model_iris.predict(X_val_prep)\n",
    "    elif hasattr(model_iris, 'forward'):\n",
    "        val_predictions = model_iris.forward(X_val_prep)\n",
    "    else:\n",
    "        raise AttributeError(\"No se puede obtener predicciones\")\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    if val_predictions.shape[1] == y_val_prep.shape[1]:\n",
    "        val_accuracy = np.mean(np.argmax(y_val_prep, axis=1) == np.argmax(val_predictions, axis=1))\n",
    "    else:\n",
    "        # Formato alternativo\n",
    "        val_accuracy = np.mean(np.round(val_predictions).flatten() == np.argmax(y_val_prep, axis=1))\n",
    "    \n",
    "    print(f\"   Accuracy validación: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluación final en entrenamiento\n",
    "    if hasattr(model_iris, 'predict'):\n",
    "        train_final_pred = model_iris.predict(X_train_prep)\n",
    "    else:\n",
    "        train_final_pred = model_iris.forward(X_train_prep)\n",
    "    \n",
    "    if train_final_pred.shape[1] == y_train_prep.shape[1]:\n",
    "        train_final_acc = np.mean(np.argmax(y_train_prep, axis=1) == np.argmax(train_final_pred, axis=1))\n",
    "    else:\n",
    "        train_final_acc = np.mean(np.round(train_final_pred).flatten() == np.argmax(y_train_prep, axis=1))\n",
    "    \n",
    "    print(f\"\\nMétricas finales de entrenamiento:\")\n",
    "    print(f\"   Accuracy: {train_final_acc:.4f}\")\n",
    "    print(f\"   Muestras correctas: {int(train_final_acc * len(y_train_prep))}/{len(y_train_prep)}\")\n",
    "    \n",
    "    # Análisis por clase (si es posible)\n",
    "    if hasattr(model_iris, 'predict') and train_final_pred.shape[1] == y_train_prep.shape[1]:\n",
    "        print(f\"\\nAccuracy por clase (entrenamiento):\")\n",
    "        train_labels = np.argmax(y_train_prep, axis=1)\n",
    "        train_pred_labels = np.argmax(train_final_pred, axis=1)\n",
    "        \n",
    "        for class_idx in range(num_classes):\n",
    "            class_name = f\"Clase {class_idx}\"\n",
    "            if class_idx < len(class_names_analysis):\n",
    "                class_name = class_names_analysis[class_idx]\n",
    "            \n",
    "            class_mask = train_labels == class_idx\n",
    "            if np.sum(class_mask) > 0:\n",
    "                class_acc = np.mean(train_pred_labels[class_mask] == class_idx)\n",
    "                print(f\"   {class_name}: {class_acc:.4f} ({np.sum(train_pred_labels[class_mask] == class_idx)}/{np.sum(class_mask)})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error en evaluación: {e}\")\n",
    "    print(\"Evaluación básica completada con limitaciones\")\n",
    "\n",
    "# 5. EQUIVALENCIA CON KERAS\n",
    "print(f\"\\n5. EQUIVALENCIA CON KERAS\")\n",
    "\n",
    "print(f\"\"\"\n",
    "# NUESTRO ENTRENAMIENTO:\n",
    "model_iris.train(\n",
    "    X_train_prep, y_train_prep,\n",
    "    epochs={epochs}\n",
    ")\n",
    "\n",
    "# EQUIVALE A:\n",
    "history = model.fit(\n",
    "    X_train_prep, y_train_prep,\n",
    "    validation_data=(X_val_prep, y_val_prep),\n",
    "    epochs={epochs},\n",
    "    batch_size=len(X_train_prep),\n",
    "    verbose=1\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"ENTRENAMIENTO COMPLETADO EXITOSAMENTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThTyBFL01Ka7"
   },
   "source": [
    "### Paso 3.4: Evaluar el modelo con el dataset BEED\n",
    "\n",
    "Una vez entrenada la red neuronal con las 5,600 muestras de entrenamiento del BEED_Data.csv, se evalúa su rendimiento usando el conjunto de prueba de 1,600 muestras.\n",
    "\n",
    "**Evaluación específica para el dataset BEED:**\n",
    "\n",
    "**Conjuntos de evaluación:**\n",
    "- **Test set:** 1,600 muestras nunca vistas durante el entrenamiento\n",
    "- **Validation set:** 800 muestras para análisis de rendimiento intermedio\n",
    "- **Métricas:** Accuracy, Precisión, Recall, F1-Score para las 4 clases del BEED\n",
    "\n",
    "**Análisis de rendimiento esperado:**\n",
    "- **Target accuracy:** ≥ 85% en el conjunto de prueba del BEED\n",
    "- **Análisis por clase:** Evaluación individual de las 4 clases del dataset\n",
    "- **Matriz de confusión:** Identificación de patrones de clasificación errónea\n",
    "- **Generalización:** Verificación de que el modelo funciona bien con datos nuevos del dominio BEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "mRakYXvl1Qwo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUACIÓN FINAL DEL MODELO NEURONAL\n",
      "\n",
      "1. EVALUACIÓN EN CONJUNTO DE PRUEBA\n",
      "Evaluando modelo en conjunto de prueba...\n",
      "   Muestras de prueba: 1600\n",
      "   Características: 16\n",
      "   Clases: 4\n",
      "\n",
      "RESULTADOS PRINCIPALES:\n",
      "   Test Accuracy: 0.1875\n",
      "   Muestras correctas: 300/1600\n",
      "   Test Loss: 0.6347\n",
      "\n",
      "2. MATRIZ DE CONFUSIÓN\n",
      "Matriz de Confusión (filas=real, columnas=predicho):\n",
      "     Clase        0Clase        1Clase        2Clase        3\n",
      "Clase 0          209       116        31        44\n",
      "Clase 1          273        40        15        72\n",
      "Clase 2          329         9        10        52\n",
      "Clase 3          324        14        21        41\n",
      "\n",
      "3. MÉTRICAS DETALLADAS POR CLASE\n",
      "\n",
      "Clase 0:\n",
      "   • Precision: 0.1841\n",
      "   • Recall (Sensitivity): 0.5225\n",
      "   • F1-Score: 0.2723\n",
      "   • Samples: 400\n",
      "\n",
      "Clase 1:\n",
      "   • Precision: 0.2235\n",
      "   • Recall (Sensitivity): 0.1000\n",
      "   • F1-Score: 0.1382\n",
      "   • Samples: 400\n",
      "\n",
      "Clase 2:\n",
      "   • Precision: 0.1299\n",
      "   • Recall (Sensitivity): 0.0250\n",
      "   • F1-Score: 0.0419\n",
      "   • Samples: 400\n",
      "\n",
      "Clase 3:\n",
      "   • Precision: 0.1962\n",
      "   • Recall (Sensitivity): 0.1025\n",
      "   • F1-Score: 0.1346\n",
      "   • Samples: 400\n",
      "\n",
      "4. COMPARACIÓN ENTRE CONJUNTOS\n",
      "Resumen final de rendimiento:\n",
      "   • Training Accuracy: 0.1757\n",
      "   • Validation Accuracy: 0.1850\n",
      "   • Test Accuracy: 0.1875\n",
      "\n",
      "Análisis de capacidad del modelo:\n",
      "   • Gap Entrenamiento-Validación: -0.0093\n",
      "   • Gap Validación-Prueba: -0.0025\n",
      "   • Gap Entrenamiento-Prueba: -0.0118\n",
      "    Posible underfitting detectado\n",
      "\n",
      "5. ANÁLISIS DE CONFIANZA\n",
      "Estadísticas de confianza:\n",
      "   • Confianza promedio: 0.6821\n",
      "   • Confianza mínima: 0.4619\n",
      "   • Confianza máxima: 0.9999\n",
      "\n",
      "Distribución por rangos:\n",
      "   • Alta confianza (≥0.9): 219 muestras (13.7%)\n",
      "   • Media confianza (0.7-0.9): 297 muestras (18.6%)\n",
      "   • Baja confianza (<0.7): 1084 muestras (67.8%)\n",
      "\n",
      "6. EQUIVALENCIA CON KERAS\n",
      "Equivalencia con Keras:\n",
      "\n",
      "# NUESTRA EVALUACIÓN:\n",
      "test_predictions = model_iris.forward(X_test_prep)\n",
      "test_accuracy = np.mean(test_labels_true == test_labels_pred)\n",
      "\n",
      "# EQUIVALE A:\n",
      "test_loss, test_accuracy = model.evaluate(\n",
      "    X_test_prep, y_test_prep,\n",
      "    verbose=0\n",
      ")\n",
      "\n",
      "\n",
      "7. RESUMEN FINAL\n",
      "RENDIMIENTO FINAL DEL MODELO:\n",
      "   Arquitectura: [16, 16, 4] (Dense layers)\n",
      "   Dataset: BEED_Data.csv\n",
      "   Algoritmo: Custom implementation (analogue)\n",
      "   Función de pérdida: Cross-Entropy\n",
      "   Test Accuracy final: 0.1875\n",
      "   Test Loss final: 0.6347\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "\n",
    "# Evaluación final del modelo neuronal\n",
    "\n",
    "print(\"EVALUACIÓN FINAL DEL MODELO NEURONAL\")\n",
    "\n",
    "# 1. EVALUACIÓN EN CONJUNTO DE PRUEBA\n",
    "print(\"\\n1. EVALUACIÓN EN CONJUNTO DE PRUEBA\")\n",
    "\n",
    "print(f\"Evaluando modelo en conjunto de prueba...\")\n",
    "print(f\"   Muestras de prueba: {X_test_prep.shape[0]}\")\n",
    "print(f\"   Características: {X_test_prep.shape[1]}\")\n",
    "print(f\"   Clases: {y_test_prep.shape[1]}\")\n",
    "\n",
    "# Realizar predicciones\n",
    "if hasattr(model_iris, 'predict'):\n",
    "    test_predictions = model_iris.predict(X_test_prep)\n",
    "elif hasattr(model_iris, 'forward'):\n",
    "    test_predictions = model_iris.forward(X_test_prep)\n",
    "else:\n",
    "    raise AttributeError(\"Modelo no tiene métodos de predicción\")\n",
    "\n",
    "# Calcular métricas principales\n",
    "test_labels_true = np.argmax(y_test_prep, axis=1)\n",
    "test_labels_pred = np.argmax(test_predictions, axis=1)\n",
    "test_accuracy = np.mean(test_labels_true == test_labels_pred)\n",
    "\n",
    "print(f\"\\nRESULTADOS PRINCIPALES:\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"   Muestras correctas: {np.sum(test_labels_true == test_labels_pred)}/{len(test_labels_true)}\")\n",
    "\n",
    "# Calcular loss de prueba\n",
    "test_loss = -np.mean(np.sum(y_test_prep * np.log(test_predictions + 1e-15), axis=1))\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# 2. MATRIZ DE CONFUSIÓN\n",
    "print(f\"\\n2. MATRIZ DE CONFUSIÓN\")\n",
    "\n",
    "# Crear matriz de confusión manualmente\n",
    "confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "for true_label, pred_label in zip(test_labels_true, test_labels_pred):\n",
    "    confusion_matrix[true_label, pred_label] += 1\n",
    "\n",
    "print(\"Matriz de Confusión (filas=real, columnas=predicho):\")\n",
    "\n",
    "# Cabecera\n",
    "print(\"     \", end=\"\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"Clase {i:>8}\", end=\"\")\n",
    "print()\n",
    "\n",
    "# Filas de la matriz\n",
    "for i in range(num_classes):\n",
    "    print(f\"Clase {i:<4}\", end=\"\")\n",
    "    for j in range(num_classes):\n",
    "        print(f\"{confusion_matrix[i, j]:>10}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# 3. MÉTRICAS POR CLASE\n",
    "print(f\"\\n3. MÉTRICAS DETALLADAS POR CLASE\")\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    # True Positives, False Positives, False Negatives\n",
    "    tp = confusion_matrix[class_idx, class_idx]\n",
    "    fp = np.sum(confusion_matrix[:, class_idx]) - tp\n",
    "    fn = np.sum(confusion_matrix[class_idx, :]) - tp\n",
    "    tn = np.sum(confusion_matrix) - tp - fp - fn\n",
    "    \n",
    "    # Calcular métricas\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nClase {class_idx}:\")\n",
    "    print(f\"   • Precision: {precision:.4f}\")\n",
    "    print(f\"   • Recall (Sensitivity): {recall:.4f}\")\n",
    "    print(f\"   • F1-Score: {f1_score:.4f}\")\n",
    "    print(f\"   • Samples: {np.sum(test_labels_true == class_idx)}\")\n",
    "\n",
    "# 4. COMPARACIÓN CON CONJUNTOS DE ENTRENAMIENTO Y VALIDACIÓN\n",
    "print(f\"\\n4. COMPARACIÓN ENTRE CONJUNTOS\")\n",
    "\n",
    "# Predicciones en entrenamiento y validación usando método compatible\n",
    "if hasattr(model_iris, 'predict'):\n",
    "    train_pred = model_iris.predict(X_train_prep)\n",
    "    val_pred = model_iris.predict(X_val_prep)\n",
    "else:\n",
    "    train_pred = model_iris.forward(X_train_prep)\n",
    "    val_pred = model_iris.forward(X_val_prep)\n",
    "\n",
    "# Calcular accuracies\n",
    "train_acc = np.mean(np.argmax(y_train_prep, axis=1) == np.argmax(train_pred, axis=1))\n",
    "val_acc = np.mean(np.argmax(y_val_prep, axis=1) == np.argmax(val_pred, axis=1))\n",
    "\n",
    "print(f\"Resumen final de rendimiento:\")\n",
    "print(f\"   • Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"   • Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"   • Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Análisis de overfitting/underfitting\n",
    "train_val_gap = train_acc - val_acc\n",
    "val_test_gap = val_acc - test_accuracy\n",
    "overfitting_gap = train_acc - test_accuracy\n",
    "\n",
    "print(f\"\\nAnálisis de capacidad del modelo:\")\n",
    "print(f\"   • Gap Entrenamiento-Validación: {train_val_gap:.4f}\")\n",
    "print(f\"   • Gap Validación-Prueba: {val_test_gap:.4f}\")\n",
    "print(f\"   • Gap Entrenamiento-Prueba: {overfitting_gap:.4f}\")\n",
    "\n",
    "if overfitting_gap > 0.1:\n",
    "    print(\"    Posible overfitting detectado\")\n",
    "elif test_accuracy < 0.6:\n",
    "    print(\"    Posible underfitting detectado\")\n",
    "else:\n",
    "    print(\"   Modelo con buen balance\")\n",
    "\n",
    "# 5. ANÁLISIS DE CONFIANZA\n",
    "print(f\"\\n5. ANÁLISIS DE CONFIANZA\")\n",
    "\n",
    "# Calcular confianza de las predicciones (valor máximo de probabilidad)\n",
    "confidence_scores = np.max(test_predictions, axis=1)\n",
    "\n",
    "print(f\"Estadísticas de confianza:\")\n",
    "print(f\"   • Confianza promedio: {np.mean(confidence_scores):.4f}\")\n",
    "print(f\"   • Confianza mínima: {np.min(confidence_scores):.4f}\")\n",
    "print(f\"   • Confianza máxima: {np.max(confidence_scores):.4f}\")\n",
    "\n",
    "# Análisis por rangos de confianza\n",
    "high_conf = np.sum(confidence_scores >= 0.9)\n",
    "med_conf = np.sum((confidence_scores >= 0.7) & (confidence_scores < 0.9))\n",
    "low_conf = np.sum(confidence_scores < 0.7)\n",
    "\n",
    "print(f\"\\nDistribución por rangos:\")\n",
    "print(f\"   • Alta confianza (≥0.9): {high_conf} muestras ({high_conf/len(confidence_scores)*100:.1f}%)\")\n",
    "print(f\"   • Media confianza (0.7-0.9): {med_conf} muestras ({med_conf/len(confidence_scores)*100:.1f}%)\")\n",
    "print(f\"   • Baja confianza (<0.7): {low_conf} muestras ({low_conf/len(confidence_scores)*100:.1f}%)\")\n",
    "\n",
    "# 6. EQUIVALENCIA CON KERAS\n",
    "print(f\"\\n6. EQUIVALENCIA CON KERAS\")\n",
    "\n",
    "print(\"Equivalencia con Keras:\")\n",
    "print(f\"\"\"\n",
    "# NUESTRA EVALUACIÓN:\n",
    "test_predictions = model_iris.forward(X_test_prep)\n",
    "test_accuracy = np.mean(test_labels_true == test_labels_pred)\n",
    "\n",
    "# EQUIVALE A:\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    X_test_prep, y_test_prep,\n",
    "    verbose=0\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# 7. RESUMEN FINAL\n",
    "print(f\"\\n7. RESUMEN FINAL\")\n",
    "\n",
    "print(f\"RENDIMIENTO FINAL DEL MODELO:\")\n",
    "print(f\"   Arquitectura: [{X_analysis.shape[1]}, 16, {num_classes}] (Dense layers)\")\n",
    "print(f\"   Dataset: BEED_Data.csv\")\n",
    "print(f\"   Algoritmo: Custom implementation (analogue)\")\n",
    "print(f\"   Función de pérdida: Cross-Entropy\")\n",
    "print(f\"   Test Accuracy final: {test_accuracy:.4f}\")\n",
    "print(f\"   Test Loss final: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8H5aoqsQKG5"
   },
   "source": [
    "## Paso 4: Redactar conclusiones del experimento con BEED_Data.csv\n",
    "\n",
    "**Resumen del experimento realizado:**\n",
    "\n",
    "Se implementó exitosamente una red neuronal analógica (sin frameworks externos) para clasificar el dataset BEED_Data.csv proporcionado por el profesor, logrando una implementación completamente desde cero que demuestra comprensión profunda de los fundamentos de las redes neuronales.\n",
    "\n",
    "**Resultados obtenidos con el dataset BEED:**\n",
    "- **Dataset procesado:** 8,000 muestras, 16 características, 4 clases\n",
    "- **Arquitectura implementada:** [16 → 16 → 4] neuronas\n",
    "- **Accuracy lograda:** [Completar con el resultado obtenido]\n",
    "- **Tiempo de entrenamiento:** [Completar con el tiempo medido]\n",
    "\n",
    "**Aspectos técnicos destacados:**\n",
    "- Implementación analógica completa de backpropagation\n",
    "- Normalización automática adaptada a las 16 características del BEED\n",
    "- Codificación one-hot para las 4 clases del problema\n",
    "- División estratificada manteniendo proporción de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a256172f"
   },
   "source": [
    "## RETROSPECTIVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8ee9bfb"
   },
   "source": [
    "**1.** ¿Cuál fue el tiempo total invertido en el laboratorio por cada uno de ustedes? (Horas/Hombre)\n",
    "    \n",
    "    Christian Alfonso Romero Martinez: 7 Horas\n",
    "    Manuel David Robayo Vega: 7 Horas\n",
    "\n",
    "**2.** ¿Cuál es el estado actual del laboratorio? ¿Por qué?\n",
    "    El laboratorio esta terminado, principalmente por el compromiso y la comunicacion del equipo para cumplir con la tarea\n",
    "\n",
    "**3.** ¿Cuál consideran fue el mayor logro? ¿Por qué?\n",
    "    El mayor logro tiene que ver con la clase DenseANN, puesto que esta clase es muy importante y evidenciamos falencias en ella segun desarrollabamos el laboratorio. por lo que debimos replantear esta clase y sus clases cascadas\n",
    "\n",
    "**4.** ¿Cuál consideran que fue el mayor problema técnico? ¿Qué hicieron para resolverlo?\n",
    "    Como se menciono anteriormente el mayor problema tecnico fue con la clase DenseANN. No podemos hablar de una solucion especifica, puesto que en un inicio resolvimos en modo apaga fuegos, y en la ultima modificacion hicimos un replanteo completo del codigo y nos soportamos en IA generativa para hacer la correccion del codigo\n",
    "\n",
    "**5.** ¿Qué hicieron bien como equipo? ¿Qué se comprometen a hacer para mejorar los resultados?\n",
    "    Como equipo resolvimos a tiempo, sin embargo podemos mejorar el tiempo de desarrollo, ya que se puedo hacer mas oportuno para tener retroalimentaciones oportunas del docente\n",
    "\n",
    "**6**.¿Qué referencias usaron? ¿Cuál fue la más útil? Incluya citas con los estándares adecuados.\n",
    "\n",
    "    Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press.\n",
    "\n",
    "Haykin, S. (2009). Neural networks and learning machines (3rd ed.). Pearson Education.\n",
    "\n",
    "LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444. https://doi.org/10.1038/nature14539\n",
    "\n",
    "Minsky, M., & Papert, S. (1969). Perceptrons: An introduction to computational geometry. MIT Press.\n",
    "\n",
    "Nielsen, M. A. (2015). Neural networks and deep learning. Determination Press. http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386-408. https://doi.org/10.1037/h0042519\n",
    "\n",
    "Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536. https://doi.org/10.1038/323533a0\n",
    "\n",
    "Documentación Técnica:\n",
    "\n",
    "Chollet, F. (2021). Deep learning with Python (2nd ed.). Manning Publications.\n",
    "\n",
    "Géron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow (2nd ed.). O'Reilly Media.\n",
    "\n",
    "Keras Team. (2023). Keras documentation. https://keras.io/\n",
    "\n",
    "NumPy Developers. (2023). NumPy documentation. https://numpy.org/doc/\n",
    "\n",
    "Herramientas de Inteligencia Artificial:\n",
    "\n",
    "**Importante** :La referencia de Claude la consideramos mas importante porque especifico y aclaro las dudas especificas para este laboratorio, ademas que nos dio plantillas y ejemplos de implementacion.\n",
    "\n",
    "Anthropic. (2024). Claude 3.5 Sonnet [Modelo de lenguaje de gran escala]. Consultado durante el desarrollo del proyecto de implementación de redes neuronales analógicas para clasificación multiclase con dataset BEED_Data.csv. https://www.anthropic.com/claude\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
